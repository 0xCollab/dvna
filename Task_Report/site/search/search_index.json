{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of contents The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Setting Up Pipeline Static Analysis Comparing SAST Tools Configuring Webhook Deploying the Report Dynamic Analysis Comparing DAST Tools Code Quality Analysis Generating Software Bill of Materials Final Pipeline Structure Shifting Local Setup to AWS Secrets Management Limitations Resources","title":"Introduction"},{"location":"#table-of-contents","text":"The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Setting Up Pipeline Static Analysis Comparing SAST Tools Configuring Webhook Deploying the Report Dynamic Analysis Comparing DAST Tools Code Quality Analysis Generating Software Bill of Materials Final Pipeline Structure Shifting Local Setup to AWS Secrets Management Limitations Resources","title":"Table of contents"},{"location":"code_quality_analysis/","text":"Source Code Quality Analysis Objective The aim of this section is to perform linting checks on the source code of DVNA and generate a code quality report to provide a solution to the 1st point of the problem statement under Task 3 . Source Code Linting Linting is a process in which a tool analyzes the source code of an application to identify the presence of programmatic and stylistic errors. Running a Linter (a tool which can perform linting analysis) can help a developer to adhere to standard coding conventions and best practices. Linting tools are language-specific and thus, the tool that can be used depends on the application being tested. These tools can also be used to generate reports about the code quality by either invoking a built-in functionality or writing a reporting wrapper around the tool to aid the ease of resolving identified issues. Linting tools for DVNA DVNA is a Nodejs application and hence, I used jshint as the linter. I primarily chose jshint as it is available as a command-line utility and hence, I could easily integrate it into my CI pipeline with Jenkins. I used the official documentation for using jshint that is available here . Along with jshint , I also found eslint and used it as well, again, by following the official documentation for the CLI interface for eslint which can be found here . Integrating Jshint with Jenkins Pipeline To start off, I installed Jshint with NPM with the following command: npm install -g jshint To try out jshint , I ran the a scan with the command below, as mentioned by the documentation: jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js Note : I wrote this one-liner: $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) to exclude the node_modules/ directory and also exclude any files which do not have a .js or .ejs extension. Since Jshint gave a non-zero status code, when it found issues, I had to write a bash script to run the scan in a sub-shell and prevent the build from failing. The contents of the script, jshint-script.sh , are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js > /{JENKINS HOME DIRECTORY}/reports/jshint-report echo $? > /dev/null Lastly, I added a stage in the pipeline to execute the script after making it executable with chmod +x . The stage structure I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } } Integrating Eslint with Jenkins Pipeline To install Eslint , I used NPM again to run the following command, following the official documentation : npm install -g eslint Now, Eslint requires the project being scanned to have a .eslintrc file which specifies what Eslint should expect in the project and a few other configurations. It can be made by running eslint --init in the project's root folder. When I ran this, it generated a file, .eslintrc.json . I took note that I'll need to place this file every time I'll run a scan on the project with Eslint so, it doesn't prompt for running initialization each time. The contents of .eslintrc.json are: { \"env\": { \"es6\": true, \"node\": true }, \"extends\": \"eslint:recommended\", \"globals\": { \"Atomics\": \"readonly\", \"SharedArrayBuffer\": \"readonly\" }, \"parserOptions\": { \"ecmaVersion\": 2018, \"sourceType\": \"module\" }, \"rules\": { } } After initializing Eslint, I ran the scan on DVNA with the following command, to scan all files within the current folder and its sub-folder with .ejs and .js extensions (because under the /views directory there were .ejs files) and lastly, write the report to a file in JSON format: eslint --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ Like Jshint, Eslint also gave a non-zero status code if it identified issues with linting. So, I wrapped the required command in a bash script, eslint-script.sh whose contents are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline eslint --no-color --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ echo $? > /dev/null Note : I added the --no-color flag to avoid color formatting (for Linux terminals) as otherwise, it would have appended additional syntactical text to provide formatting which made the report difficult to read. Finally, I added a stage in the pipeline to run the script after I made it executable with chmod +x . The stage that I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } } Code Quality Report This section contains a brief about the quality report that the tools used above generated based on the default linting rules they had. Jshint Jshint found a total of 247 linting issues with DVNA. The report mostly comprised of errors stating a 'Missing Semicolon' and a few other errors that included 'Missing Identifier', 'Expected an Assignment', etc. Some of the bugs were logical issues within the codebase of DVNA but the vast majority referred to stylistic issues. The complete report generated by Jshint can be found here . Eslint Eslint proved to be a better linting tool than Jshint as it found a total of 549 issues with DVNA. It identified a wider range of issues within DVNA's codebase. Along with stylistic issues, it also found logical errors such as 'Undefined Elements', 'Unused Variables', 'Empty Blocks', 'Using Prototype Builtins', 'Redeclaration of Elements', etc. The complete report generated by Eslint can be found here .","title":"Code Quality Analysis"},{"location":"code_quality_analysis/#source-code-quality-analysis","text":"","title":"Source Code Quality Analysis"},{"location":"code_quality_analysis/#objective","text":"The aim of this section is to perform linting checks on the source code of DVNA and generate a code quality report to provide a solution to the 1st point of the problem statement under Task 3 .","title":"Objective"},{"location":"code_quality_analysis/#source-code-linting","text":"Linting is a process in which a tool analyzes the source code of an application to identify the presence of programmatic and stylistic errors. Running a Linter (a tool which can perform linting analysis) can help a developer to adhere to standard coding conventions and best practices. Linting tools are language-specific and thus, the tool that can be used depends on the application being tested. These tools can also be used to generate reports about the code quality by either invoking a built-in functionality or writing a reporting wrapper around the tool to aid the ease of resolving identified issues.","title":"Source Code Linting"},{"location":"code_quality_analysis/#linting-tools-for-dvna","text":"DVNA is a Nodejs application and hence, I used jshint as the linter. I primarily chose jshint as it is available as a command-line utility and hence, I could easily integrate it into my CI pipeline with Jenkins. I used the official documentation for using jshint that is available here . Along with jshint , I also found eslint and used it as well, again, by following the official documentation for the CLI interface for eslint which can be found here .","title":"Linting tools for DVNA"},{"location":"code_quality_analysis/#integrating-jshint-with-jenkins-pipeline","text":"To start off, I installed Jshint with NPM with the following command: npm install -g jshint To try out jshint , I ran the a scan with the command below, as mentioned by the documentation: jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js Note : I wrote this one-liner: $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) to exclude the node_modules/ directory and also exclude any files which do not have a .js or .ejs extension. Since Jshint gave a non-zero status code, when it found issues, I had to write a bash script to run the scan in a sub-shell and prevent the build from failing. The contents of the script, jshint-script.sh , are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js > /{JENKINS HOME DIRECTORY}/reports/jshint-report echo $? > /dev/null Lastly, I added a stage in the pipeline to execute the script after making it executable with chmod +x . The stage structure I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } }","title":"Integrating Jshint with Jenkins Pipeline"},{"location":"code_quality_analysis/#integrating-eslint-with-jenkins-pipeline","text":"To install Eslint , I used NPM again to run the following command, following the official documentation : npm install -g eslint Now, Eslint requires the project being scanned to have a .eslintrc file which specifies what Eslint should expect in the project and a few other configurations. It can be made by running eslint --init in the project's root folder. When I ran this, it generated a file, .eslintrc.json . I took note that I'll need to place this file every time I'll run a scan on the project with Eslint so, it doesn't prompt for running initialization each time. The contents of .eslintrc.json are: { \"env\": { \"es6\": true, \"node\": true }, \"extends\": \"eslint:recommended\", \"globals\": { \"Atomics\": \"readonly\", \"SharedArrayBuffer\": \"readonly\" }, \"parserOptions\": { \"ecmaVersion\": 2018, \"sourceType\": \"module\" }, \"rules\": { } } After initializing Eslint, I ran the scan on DVNA with the following command, to scan all files within the current folder and its sub-folder with .ejs and .js extensions (because under the /views directory there were .ejs files) and lastly, write the report to a file in JSON format: eslint --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ Like Jshint, Eslint also gave a non-zero status code if it identified issues with linting. So, I wrapped the required command in a bash script, eslint-script.sh whose contents are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline eslint --no-color --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ echo $? > /dev/null Note : I added the --no-color flag to avoid color formatting (for Linux terminals) as otherwise, it would have appended additional syntactical text to provide formatting which made the report difficult to read. Finally, I added a stage in the pipeline to run the script after I made it executable with chmod +x . The stage that I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } }","title":"Integrating Eslint with Jenkins Pipeline"},{"location":"code_quality_analysis/#code-quality-report","text":"This section contains a brief about the quality report that the tools used above generated based on the default linting rules they had.","title":"Code Quality Report"},{"location":"code_quality_analysis/#jshint","text":"Jshint found a total of 247 linting issues with DVNA. The report mostly comprised of errors stating a 'Missing Semicolon' and a few other errors that included 'Missing Identifier', 'Expected an Assignment', etc. Some of the bugs were logical issues within the codebase of DVNA but the vast majority referred to stylistic issues. The complete report generated by Jshint can be found here .","title":"Jshint"},{"location":"code_quality_analysis/#eslint","text":"Eslint proved to be a better linting tool than Jshint as it found a total of 549 issues with DVNA. It identified a wider range of issues within DVNA's codebase. Along with stylistic issues, it also found logical errors such as 'Undefined Elements', 'Unused Variables', 'Empty Blocks', 'Using Prototype Builtins', 'Redeclaration of Elements', etc. The complete report generated by Eslint can be found here .","title":"Eslint"},{"location":"comparing_dast_tools/","text":"Comparing DAST Tools Objective The aim of this section is to compare the findings of the various DAST tools used in the previous section and rank them to provide a solution to the 2nd point of the problem statement under Task 2 . Vulnerability Reports ZAP and W3AF, both, found various vulnerabilities. Interestingly, most of the vulnerabilities identified by the tools are different from the other. The tools also were different in terms of ease of usage. I found W3AF more user-friendly than ZAP, mainly due to the difficulty I faced while configuring the tools to authenticate with DVNA as a user. W3AF's authentication mechanism was extremely simple whereas, for ZAP, the CLI was very complicated to use. After the initial hitches, both tools were quite close in terms of the number of discoveries but ZAP found more vulnerabilities than W3AF. After consolidating the discoveries that the tools made, I'll rank ZAP above W3AF even though it was more complicated to use because the results that ZAP produced were more relevant. Below is a table of comparison the various types of discoveries ZAP and W3AF made: Rank Tool Vulnerabilities Discovered Informational Discoveries 1 ZAP (Zed Attack Proxy) 9 1 2 W3AF (Web Application Attack & Audit Framework) 5 3 ZAP ZAP uses a set of rules built into the application and segregated into two halves - active scan rules and passive scan rules. These rules are derived from the OWASP Top 10 project. The active scan rules attempt to uncover security vulnerabilities by using known attacks against the target application. The passive scan rules observe the HTTP messages sent to the web application and detect malicious behavior. ZAP's baseline scan found a total of 10 issues with DVNA. The split-up of the issues found is listed in the table below: Sl. No. Description Severity 1. X-Frame-Options Header Not Set Medium 2. CSP Scanner: Wildcard Directive Medium 3. Cross-Domain JavaScript Source File Inclusion Low 4. Absence of Anti-CSRF Tokens Low 5. X-Content-Type-Options Header Missing Low 6. Cookie Without SameSite Attribute Low 7. Web Browser XSS Protection Not Enabled Low 8. Server Leaks Information via \"X-Powered-By\" HTTP Response Header Field(s) Low 9. Content Security Policy (CSP) Header Not Set Low 10. Information Disclosure - Suspicious Comments Informational The complete report generated by the ZAP baseline scan can be found here . W3AF Since, W3AF had multiple output options, I initially went with storing the output in an HTML report as I thought it would be easier to comprehend but the report only stated a single informational discovery, which was not right because on the console I saw various other discoveries. So, I also generated a Text Report, which is supposed to be the exact replica of the console output and as expected, the text report was more detailed and contained more discoveries. W3AF works by first crawling the application to find injectable fields (with an option to authenticate with the application to reach segments that lie behind the authentication layer). After it curates a list of URLs, it tries to find injectable fields in each of those URLs and based on the plugins enables (such as SQLi plugin, XSS plugin) it injects crafted string inputs to those fields to test vulnerability while also taking note of auxiliary observations about the application's behavior, state or content that W3AF deems worthy of the user taking a look at them. Below mentioned is the table of the consolidated discoveries that W3AF made along with what kind of discovery it was: Sl. No. Description Type 1. HTTP response returned without the recommended HTTP header X-Content-Type-Options. Vulnerability 2. An HTTP response matching the web backdoor signature \"cmd.jsp\" was found at: \"http://localhost:9090/cmd.jspx\". Vulnerability 3. An HTTP response matching the web backdoor signature \"cmd.jsp\" was found at: \"http://localhost:9090/cmd.jsp\". Vulnerability 4. The URL: \"http://localhost:9090/learn\" has a script tag with a source that points to a third party site. Vulnerability 5. A comment containing HTML code [if lt IE 9]> <script src=\"http://htm\" was found in: \"http://localhost:9090/learn\". Vulnerability 6. X-Powered-By header for the target HTTP server is \"Express\". Information 7. Total unique URLs found (after removing redundant entries) is 24. Information 8. Total unique fuzzable URLs found (after removing redundant entries) is 24. Information The Text report I used to analyze the observations of the tool can be found here . The initial HTML report (which I left and instead used the Text report) can be found here . The actual Text report contains a lot of debug output. So, I redacted the reported and only kept the information relevant to security issues in a separate file which can be found here . Conclusion In conclusion, I would say both the performance of both the tools was quite close to the other. The ranking I gave is my personal opinion based solely on the results they provided and their relevance to the solution of the problem statement. I did not consider additional factors, such as user experience, while ranking them.","title":"Comparing DAST Tools"},{"location":"comparing_dast_tools/#comparing-dast-tools","text":"","title":"Comparing DAST Tools"},{"location":"comparing_dast_tools/#objective","text":"The aim of this section is to compare the findings of the various DAST tools used in the previous section and rank them to provide a solution to the 2nd point of the problem statement under Task 2 .","title":"Objective"},{"location":"comparing_dast_tools/#vulnerability-reports","text":"ZAP and W3AF, both, found various vulnerabilities. Interestingly, most of the vulnerabilities identified by the tools are different from the other. The tools also were different in terms of ease of usage. I found W3AF more user-friendly than ZAP, mainly due to the difficulty I faced while configuring the tools to authenticate with DVNA as a user. W3AF's authentication mechanism was extremely simple whereas, for ZAP, the CLI was very complicated to use. After the initial hitches, both tools were quite close in terms of the number of discoveries but ZAP found more vulnerabilities than W3AF. After consolidating the discoveries that the tools made, I'll rank ZAP above W3AF even though it was more complicated to use because the results that ZAP produced were more relevant. Below is a table of comparison the various types of discoveries ZAP and W3AF made: Rank Tool Vulnerabilities Discovered Informational Discoveries 1 ZAP (Zed Attack Proxy) 9 1 2 W3AF (Web Application Attack & Audit Framework) 5 3","title":"Vulnerability Reports"},{"location":"comparing_dast_tools/#zap","text":"ZAP uses a set of rules built into the application and segregated into two halves - active scan rules and passive scan rules. These rules are derived from the OWASP Top 10 project. The active scan rules attempt to uncover security vulnerabilities by using known attacks against the target application. The passive scan rules observe the HTTP messages sent to the web application and detect malicious behavior. ZAP's baseline scan found a total of 10 issues with DVNA. The split-up of the issues found is listed in the table below: Sl. No. Description Severity 1. X-Frame-Options Header Not Set Medium 2. CSP Scanner: Wildcard Directive Medium 3. Cross-Domain JavaScript Source File Inclusion Low 4. Absence of Anti-CSRF Tokens Low 5. X-Content-Type-Options Header Missing Low 6. Cookie Without SameSite Attribute Low 7. Web Browser XSS Protection Not Enabled Low 8. Server Leaks Information via \"X-Powered-By\" HTTP Response Header Field(s) Low 9. Content Security Policy (CSP) Header Not Set Low 10. Information Disclosure - Suspicious Comments Informational The complete report generated by the ZAP baseline scan can be found here .","title":"ZAP"},{"location":"comparing_dast_tools/#w3af","text":"Since, W3AF had multiple output options, I initially went with storing the output in an HTML report as I thought it would be easier to comprehend but the report only stated a single informational discovery, which was not right because on the console I saw various other discoveries. So, I also generated a Text Report, which is supposed to be the exact replica of the console output and as expected, the text report was more detailed and contained more discoveries. W3AF works by first crawling the application to find injectable fields (with an option to authenticate with the application to reach segments that lie behind the authentication layer). After it curates a list of URLs, it tries to find injectable fields in each of those URLs and based on the plugins enables (such as SQLi plugin, XSS plugin) it injects crafted string inputs to those fields to test vulnerability while also taking note of auxiliary observations about the application's behavior, state or content that W3AF deems worthy of the user taking a look at them. Below mentioned is the table of the consolidated discoveries that W3AF made along with what kind of discovery it was: Sl. No. Description Type 1. HTTP response returned without the recommended HTTP header X-Content-Type-Options. Vulnerability 2. An HTTP response matching the web backdoor signature \"cmd.jsp\" was found at: \"http://localhost:9090/cmd.jspx\". Vulnerability 3. An HTTP response matching the web backdoor signature \"cmd.jsp\" was found at: \"http://localhost:9090/cmd.jsp\". Vulnerability 4. The URL: \"http://localhost:9090/learn\" has a script tag with a source that points to a third party site. Vulnerability 5. A comment containing HTML code [if lt IE 9]> <script src=\"http://htm\" was found in: \"http://localhost:9090/learn\". Vulnerability 6. X-Powered-By header for the target HTTP server is \"Express\". Information 7. Total unique URLs found (after removing redundant entries) is 24. Information 8. Total unique fuzzable URLs found (after removing redundant entries) is 24. Information The Text report I used to analyze the observations of the tool can be found here . The initial HTML report (which I left and instead used the Text report) can be found here . The actual Text report contains a lot of debug output. So, I redacted the reported and only kept the information relevant to security issues in a separate file which can be found here .","title":"W3AF"},{"location":"comparing_dast_tools/#conclusion","text":"In conclusion, I would say both the performance of both the tools was quite close to the other. The ranking I gave is my personal opinion based solely on the results they provided and their relevance to the solution of the problem statement. I did not consider additional factors, such as user experience, while ranking them.","title":"Conclusion"},{"location":"comparing_sast_tools/","text":"Comparing SAST Tools Objective The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement under Task 1 . Vulnerability Reports The different tools found various vulnerabilities. Some found more vulnerabilities than others. I went through all the reports generated to find relevant content found in the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found. A more detailed explanation of the various tools generated are listed in the upcoming sections in this chapter. Here is a short summarized table of the findings that the tools showcased. After reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as follows: Rank Tool No. of Dependency-based vulnerabilities No. of Web-based vulnerabilities 1 NodeJsScan 34 5 2 Auditjs 22 None 3 Snyk 8 None 4 Dependency Check 7 None 5 NPM Audit 5 None 6 Retire.js 3 None 7 SonarQube 0 None SonarQube SonarQube states here that it utilizes security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax-based bugs. This is probably due to the fact that SonarQube has very few security rules for JavaScript. It works better with Java. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here . NPM Audit According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here . NodeJsScan NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi-Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here . Retire.js Retire.js maintains a database of known vulnerabilities, which can be found listed here under 'Vulnerabilities' sub-heading, in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here . OWASP Dependency Check According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project-specific to the technology being used, and categorizes them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards-based vulnerability management data, to find matching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cyber-security vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. By default, Dependency Check assigns the lowest confidence to a module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here . Auditjs Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here . Snyk Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them get collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): import json file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below-listed modules, with the type of vulnerability, the number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitrary Code Execution 3 High node-serialize Arbitrary Code Execution 1 High typed-function Arbitrary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitrary Code Execution 2 Medium The full report generated by Snyk can be found here . Conclusion In conclusion, there were two tools that stood out, NodeJsScan and Auditjs, because of the number of vulnerabilities they found. NodeJsScan also went a step ahead and identified a few web-based vulnerabilities, which no other tool did. All the other tools I used were quite close with the vulnerabilities that they found, except for SonarQube which did not find any vulnerability.","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#comparing-sast-tools","text":"","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#objective","text":"The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement under Task 1 .","title":"Objective"},{"location":"comparing_sast_tools/#vulnerability-reports","text":"The different tools found various vulnerabilities. Some found more vulnerabilities than others. I went through all the reports generated to find relevant content found in the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found. A more detailed explanation of the various tools generated are listed in the upcoming sections in this chapter. Here is a short summarized table of the findings that the tools showcased. After reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as follows: Rank Tool No. of Dependency-based vulnerabilities No. of Web-based vulnerabilities 1 NodeJsScan 34 5 2 Auditjs 22 None 3 Snyk 8 None 4 Dependency Check 7 None 5 NPM Audit 5 None 6 Retire.js 3 None 7 SonarQube 0 None","title":"Vulnerability Reports"},{"location":"comparing_sast_tools/#sonarqube","text":"SonarQube states here that it utilizes security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax-based bugs. This is probably due to the fact that SonarQube has very few security rules for JavaScript. It works better with Java. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here .","title":"SonarQube"},{"location":"comparing_sast_tools/#npm-audit","text":"According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here .","title":"NPM Audit"},{"location":"comparing_sast_tools/#nodejsscan","text":"NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi-Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here .","title":"NodeJsScan"},{"location":"comparing_sast_tools/#retirejs","text":"Retire.js maintains a database of known vulnerabilities, which can be found listed here under 'Vulnerabilities' sub-heading, in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here .","title":"Retire.js"},{"location":"comparing_sast_tools/#owasp-dependency-check","text":"According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project-specific to the technology being used, and categorizes them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards-based vulnerability management data, to find matching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cyber-security vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. By default, Dependency Check assigns the lowest confidence to a module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here .","title":"OWASP Dependency Check"},{"location":"comparing_sast_tools/#auditjs","text":"Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here .","title":"Auditjs"},{"location":"comparing_sast_tools/#snyk","text":"Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them get collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): import json file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below-listed modules, with the type of vulnerability, the number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitrary Code Execution 3 High node-serialize Arbitrary Code Execution 1 High typed-function Arbitrary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitrary Code Execution 2 Medium The full report generated by Snyk can be found here .","title":"Snyk"},{"location":"comparing_sast_tools/#conclusion","text":"In conclusion, there were two tools that stood out, NodeJsScan and Auditjs, because of the number of vulnerabilities they found. NodeJsScan also went a step ahead and identified a few web-based vulnerabilities, which no other tool did. All the other tools I used were quite close with the vulnerabilities that they found, except for SonarQube which did not find any vulnerability.","title":"Conclusion"},{"location":"configuring_webhook/","text":"Configuring Trigger with Webhook Objective The aim of this section is to create and configure a webhook to automate builds based on defined events occurring on the project repository in reference to the second sub-segment of the 8th point in the problem statement under Task 1 . Webhooks Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered by the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurrence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well. Configuring Jenkins Pipeline for Webhook For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked. Configuring GitHub for Webhook Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub does not send any request. I tried triggering the selected events, after unchecking the option, and it did not send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL. Using ngrok to handle Webhook over Internet Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dynamically generated URL that can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start an HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Configuring Webhook"},{"location":"configuring_webhook/#configuring-trigger-with-webhook","text":"","title":"Configuring Trigger with Webhook"},{"location":"configuring_webhook/#objective","text":"The aim of this section is to create and configure a webhook to automate builds based on defined events occurring on the project repository in reference to the second sub-segment of the 8th point in the problem statement under Task 1 .","title":"Objective"},{"location":"configuring_webhook/#webhooks","text":"Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered by the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurrence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well.","title":"Webhooks"},{"location":"configuring_webhook/#configuring-jenkins-pipeline-for-webhook","text":"For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked.","title":"Configuring Jenkins Pipeline for Webhook"},{"location":"configuring_webhook/#configuring-github-for-webhook","text":"Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub does not send any request. I tried triggering the selected events, after unchecking the option, and it did not send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL.","title":"Configuring GitHub for Webhook"},{"location":"configuring_webhook/#using-ngrok-to-handle-webhook-over-internet","text":"Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dynamically generated URL that can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start an HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Using ngrok to handle Webhook over Internet"},{"location":"deploying_report/","text":"Deploying Report with MkDocs Objective The aim of this section is to create documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement under Task 1 . Format and Tools The report was written in Markdown as required by the problem statement. Markdown is a markup language that allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as required by the problem statement, to build a static site with the report. MkDocs is a static site generator that creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file. Installing MkDocs I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as the rest of the steps were not required in the context of the task/problem statement. Selecting a Theme MkDocs allows users to use various themes to customize the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section. Site Configuration To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: setting_up_pipeline.md - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Dynamic Analysis: 'dynamic_analysis.md' - Comparing DAST Tools: 'comparing_dast_tools.md' - Code Quality Analysis: 'code_quality_analysis.md' - Generating Software Bill of Materials: 'generating_sbom.md' - Final Pipeline Structure: 'final_pipeline.md' - Shifting Local Setup to AWS: 'moving_setup_to_aws.md' - Secrets Management: 'secrets_management.md' - Limitations: 'limitations.md' - Resources: 'resources.md' theme: material site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site. Deploying Static Site To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with, in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying the Report"},{"location":"deploying_report/#deploying-report-with-mkdocs","text":"","title":"Deploying Report with MkDocs"},{"location":"deploying_report/#objective","text":"The aim of this section is to create documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement under Task 1 .","title":"Objective"},{"location":"deploying_report/#format-and-tools","text":"The report was written in Markdown as required by the problem statement. Markdown is a markup language that allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as required by the problem statement, to build a static site with the report. MkDocs is a static site generator that creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file.","title":"Format and Tools"},{"location":"deploying_report/#installing-mkdocs","text":"I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as the rest of the steps were not required in the context of the task/problem statement.","title":"Installing MkDocs"},{"location":"deploying_report/#selecting-a-theme","text":"MkDocs allows users to use various themes to customize the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section.","title":"Selecting a Theme"},{"location":"deploying_report/#site-configuration","text":"To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: setting_up_pipeline.md - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Dynamic Analysis: 'dynamic_analysis.md' - Comparing DAST Tools: 'comparing_dast_tools.md' - Code Quality Analysis: 'code_quality_analysis.md' - Generating Software Bill of Materials: 'generating_sbom.md' - Final Pipeline Structure: 'final_pipeline.md' - Shifting Local Setup to AWS: 'moving_setup_to_aws.md' - Secrets Management: 'secrets_management.md' - Limitations: 'limitations.md' - Resources: 'resources.md' theme: material site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site.","title":"Site Configuration"},{"location":"deploying_report/#deploying-static-site","text":"To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with, in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying Static Site"},{"location":"dynamic_analysis/","text":"Dynamic Analysis Objective The aim of this section is to perform DAST for DVNA with OWASP ZAP , W3AF and generate a report to provide a solution to the 1st point of the problem statement under Task 2 . DAST DAST or Dynamic Application Security Testing is a black-box testing technique in which the DAST tool interacts with the application being tested in its running state to imitate an attacker. Unlike static analysis, in DAST one does not have access to the source code of the application and the tool is completely reliant on the interactivity the application provides. In dynamic analysis, tools are used to automate attacks on the application ranging from SQL Injection, Input Validation, Cross-Site Scripting, and so forth. OWASP ZAP ZAP or Zed Attack Proxy is an open-source tool used to perform dynamic application security testing designed specifically for web applications. ZAP has a desktop interface, APIs for it to be used in an automated fashion and also a CLI. It imitates an actual user where it interacts with the application to perform various attacks. ZAP comes with a plethora of options to use, for which further details can be found here . ZAP also comes as a Docker image which is more convenient to use especially if one is using the CLI interface. Docker is a tool designed to provide ease in shipping applications across platforms. It packages the application, along with all its dependencies and other required libraries into an image . Then containers (running instances of the image) can be used to run the application on any machine. It is similar to a virtual machine but it differs greatly from them based on the fact that it does not require a full-fledged operating system to run the application. Instead, it runs the application on the system's kernel itself by just bringing along the required libraries with it which could be missing on machines other than the one the application was built on. This allows Docker-based applications to be portable i.e. they can be run on any machine that can run Docker containers. It allows for various versions of the same tool/library running on a host as different containers do not care about what is happening inside another container. Further information on docker can be found in the official documentation . Configuring OWASP ZAP with Docker To use ZAP with docker, I needed to pull the image from docker hub and start off. I used this documentation from Mozilla as it had a lot of errors demonstrated along with their rectification steps for starting out with ZAP. This was missing from all the other sources I found. Note : While running ZAP scans below, I explicitly ran DVNA, without Jenkins, for it to be tested. To start off with ZAP, I pulled the docker image by running: docker pull owasp/zap2docker-stable Then I tried to run the tool with its CLI interface. The CLI threw an error which was because of an encoding inconsistency between Python2 and Python3. To rectify this issue, I had to explicitly specify the encoding to be used with the help of some environment variables. These environment variables can be seen in the command below along with the -e flag for Docker to inject these environment variables in the container: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli --help Next I ran a quick-scan to have a look at how ZAP outputs information. Since ZAP is running inside a Docker container, I could not use localhost or 127.0.0.1 as then the container would take it to be its own host. So, to overcome this issue I used the IP assigned to docker0 , the network interface created for Docker. This IP can be found with this one-liner: $(ip -f inet -o addr show docker0 | awk '{print $4}' | cut -d '/' -f 1) .Also, as ZAP also has an API that can be used to interact with it programmatically, I had to use --start-options '-config api.disablekey=true' as otherwise, ZAP tried (and failed) to connect to the proxy as the API key was not specified. Also, the -l option specifies the severity level at which ZAP should log a vulnerability to console (or to a report). The --self-contained flag tells ZAP to turn off the daemon once the scan is complete and the --spider option tells ZAP to crawl the target to find other links present on the target site. The complete command is as mentioned below: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli quick-scan --start-options '-config api.disablekey=true' --self-contained --spider -l Low http://172.17.0.1:9090 Now, I tried the active-scan option. For this I first needed to start the daemon for zap to be accessed by the CLI, run open-url to add the target URL to the configuration in ZAP-CLI (without this, active-scan option will not start a scan on the target) and then run the scan against DVNA. This step was not required previously as while running a quick-scan , the daemon is automatically started to run the scan and stopped after the scan is finished. To do run the daemon with Docker, I ran the following command (with the -u zap segment to execute things with the zap user instead of Docker's default root user and I also appended the --rm flag to delete the container, automatically, when I stop it): docker run --rm -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 --name zap -u zap -p 8090:8080 -d owasp/zap2docker-stable zap.sh -daemon -port 8080 -host 0.0.0.0 -config api.disablekey=true docker exec <CONTAINER NAME/ID> zap-cli open-url <TARGET URL> docker exec <CONTAINER NAME/ID> zap-cli active-scan <TARGET URL> Note : When I ran the scan initially, it dropped an error from Python saying that the 'Connection was Refused' while sending requests to the target through the ZAP proxy. It turned out to be an issue with the command written in the blog that I was following. It exposed the wrong port and hence rectifying the port options in the command to -p 8090:8080 and -port 8080 solved the issue and the scan worked. Another thing is that I chose 8090 for the host port as I already had Jenkins running on 8080 . Now to be able to scan DVNA from the CLI with ZAP-CLI, I required a context which basically was a configuration written in XML to define how to perform a scan. This context also had a set of credentials in it that were recorded with a ZEST script, which is a JSON-based form used to record interactions between a website and a user specially created for security tools focused on web applications. This context file along with the ZEST script would allow zap-cli to authenticate with DVNA to be able to scan the portion of the application that lies behind the login screen. I used the browser-based GUI to generate the context and the Zest script and exported them to the machine. But importing them created various complications as zap-cli was unable to identify the embedded Zest script in the context provided to it. Due to the above complications, I decided to use Zap's baseline scan instead. To start off ZAP baseline scan with the docker image, I ran the following command, where the -t flag specified the target and -l flag defined the alert/severity level, by following this documentation : docker run -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -l INFO Now, to save the report on the Jenkins machine, I needed to mount a volume with Docker. I used the -v flag (as mentioned in the docker documentation ) to mount the present working directory of the host to the /zap/wrk directory of the container and also added the -r flag to save scan output to a HTML report on the Jenkins machine: docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS Integrating ZAP with Jenkins After understanding how to use the baseline-scan and its various options, I started integrating the scan as part of DAST in the Jenkins pipeline. But before I could scan DVNA with ZAP Baseline, I needed to build the dependencies and start an instance of DVNA. To do the same, I also had to fetch code from the repository on GitHub (explicitly, because I didn't use a Jenkinsfile for this task). Note : I chose to build a new pipeline just for DAST, as combining SAST and DAST in a single pipeline would have taken too much time during each execution which felt unnecessary at the time of development. To start off, I added a stage to fetch the code from the GitHub repository as follows: stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } Next, I built the dependencies for DVNA, as I did while performing SAST, and started an instance of DVNA, as mentioned in the stage mentioned below: stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } Note : I kept all the required environment variables in the env.sh file which can be seen in the above stage, and used it to export those variables for DVNA to be able to connect with the database. While trying to export the variables, the shell that comes along with Jenkins kept throwing an error 'source not found' i.e. saying it didn't recognize the command source . This turned out to be because that by default the sh shell in Jenkins points to /bin/dash which doesn't have the command source . To rectify this, I changed the Jenkins shell to point to /bin/bash instead with this command - sudo ln -sf /bin/bash /bin/sh , which I found in this blog . This method, however, will change the symlink for sh to bash for every user on the system (which can be created by other applications, such as Jenkins itself). This might break the functioning of the other applications and hence, it is more advisable to specify the changed shell for the specific user on the system that needs it. In the context of Jenkins, the recommended way would be to use usermod -s /bin/bash jenkins to change the default shell only for Jenkins. Now, that DVNA was up and running, ran the baseline scan on it with docker and Zap. But I had to wrap the command in a shell script to evade the non-zero status code that ZAP gives on finding issues. So, I wrote the script, baseline-scan.sh , mentioned below and made it executable with chmod +x : cd /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS echo $? > /dev/null Note : One thing to take note of is that Jenkins would require to use sudo when running docker commands as it's not part of the docker user group on the system. This is the preferred way, that the required docker setup is on another VM and Jenkins SSHs into the other VM with access to run docker commands as a sudo user. But for the purpose of this task, I did not set up another VM, instead, I added the jenkins user to the docker user group with - sudo usermod -aG docker jenkins , for it to be able to perform an operation without using sudo . This, however, is not recommended. I added a stage in the Jenkins Pipeline, to execute the shell script and generate the DAST report. The stage's content is mentioned below: stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } Note : Running the pipeline threw an access error, saying the report could not be written in the directory. This was because the zap user in the docker container did not have write permission for Jenkins' workspace. So, for the sake of the task, I modified the permissions of the node-dast-pipeline/ directory with chmod 777 node-dast-pipeline/ . This is also not recommended. If the permissions need to be changed, they should be specific and exact in terms of the access they grant which should not be more than that is required. Lastly, I added a stage to stop the instance of DVNA that was running as it was no longer needed and moved the report generated from the workspace directory to the reports directory that I've been using for the tasks: stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } The complete pipeline script is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } } } On a different note, I also tried running a docker agent in the Jenkins pipeline to execute zap-baseline scan but it kept throwing an access error (different than the one mentioned above). This issue is not rectified yet. The stage that I wrote (but was not working) is as follows: stage ('Run ZAP for DAST') { agent { docker { image 'owasp/zap2docker-stable' args '-v /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline/:/zap/wrk/' } } steps { sh 'zap-baseline.py -t http://172.17.0.1:9090 -r zap_baseline_report.html -l PASS' } } W3AF W3AF or the Web Application Attack and Audit Framework is another DAST tool like OWASP ZAP. Like ZAP, W3AF has both, a web-based GUI as well as a command-line interface. It is able to identify 200+ distinct vulnerabilities including SQL Injections, Cross-Site Scripting and OS Command Injection. Configuring W3AF Referring to the official documentation for W3AF, I first cloned the project's repository and moved into the project directory: git clone https://github.com/andresriancho/w3af.git && cd w3af/ Since, I was ultimately going to run the tool with the command-line interface as part of the pipeline, I skipped using the GUI that was available and started the CLI as follows: ./w3af_console I ran the console version of the tool, even though I knew it would not work (initially) since it had missing dependencies because W3AF automatically generates a bash script under /tmp for the user to install the unmet dependencies. The script just contains the command the user would have ran on the terminal manually, it just tries to make the tool easier to use to improve user's experience while using the tool. After executing W3AF's console script to generate the dependency-installation script, I ran it to install the missing dependencies for W3AF: /tmp/w3af_dependency_install.sh Once all the dependencies were installed, I executed the console script again to start the CLI for W3AF. It uses a prompt-based system, prefixed with w3af>>> as the prompt, and there are various options to configure the tool before launching the scan with it. I followed along the documentation to understand the various options and configurations that W3AF supports. W3AF also supports automation by allowing the user to write a configuration script, which is nothing but the exact same commands that one would have ran in the prompt in that same sequence written in a file, and pass it to W3AF with the -s flag. So, I picked up a template script from the project's repository which had all the scan options enabled. I did, however, made some amendments to the script. I removed the segment for bruteforcing injectable fields, I added a block to add authentication for DVNA and lastly made some changes by replacing values in the script to fit DVNA's context. The finished script, w3af_scan_script.w3af , is mentioned below: # Traversing into the Plugins Menu plugins # Configuring output options output console,text_file output config text_file set output_file output-w3af.txt set verbose True back output config console set verbose False back # Configuring audit Options audit all audit # Configuring grep options grep all grep # Configuring crawl options crawl all, !bing_spider, !google_spider, !spider_man crawl # Configuring auth options auth detailed auth config detailed set username admin set password admin set username_field username set password_field password set data_format %u=%U&%p=%P set auth_url <AUTH URL> set method POST set check_url <VERIFICATION URL> set check_string 'Logout' back # Traversing from plugin menu to main menu back # Configuring target options target set target <TARGET URL> back # Launching the scan start # Exiting from W3AF after the scan exit W3AF behind-the-scenes takes the commands written in the script mentioned above and passes them to the prompt on its own, allowing users to not have to pass arguments through the CLI prompt. Lastly, to launch the tool and start the scan with W3AF, I ran the following command, passing the script mentioned above: ./w3af_console -s w3af_scan_script.w3af Configuring Jenkins for Distributed Builds After having so many tools configured on the Jenkins VM, it was getting tedious to manage all of them on the same server. So, to do away with this issue, I used to Master-Agent architecture that Jenkins supports to use a new VM to run DAST on DVNA with W3AF installed on a jenkins agent running under the new VM. Configuring Agent VM for Jenkins The first step to creating an agent VM for Jenkins is to create a working directory where all the files associated with the Jenkins project will be kept along with some additional files/executables that Jenkins will use to use the Agent. I chose to create a /var/jenkins directory as follows: mkdir /var/jenkins Once, the directory was created I had to change the ownership of the directory to the non-root user ( ayush ) to avoid having to use sudo with commands that run on the Agent. To change the ownership of the directory, I used the following command: sudo chown ayush: /var/jenkins/ Then for the non-root user to be able to write files in the directory via Jenkins Agent, I gave the user write permissions. This allowed Jenkins Master node to write an agent.jar file on the Agent VM which will be used to provision jobs to the agent. I used the following command to modify the write access to the directory: sudo chmod u+w /var/jenkins To create an agent with Jenkins, I followed the following steps: I navigated to Dashboard > Manage Jenkins > Manage Nodes > New Node . I gave a name to the node, Agent VM , in this case, selected the Permanent Agent option (as the VM being used was managed outside of Jenkins) and clicked on Okay . Then I gave a brief description of what the Agent will be used for. I left the number of executors to the default setting of 1 . Under Remote root directory , I provided /var/jenkins which I configured earlier while setting up the Agent VM. I added the label raf-vm so I could use this label to specify stage(s) to use this agent instead of the master node. I chose the Use this node as much as possible option but since it is being used for only one task, it does not make any difference. Under Launch method , I chose the Launch agent agents with SSH option. I added the IP of the Agent VM and provided the credentials required to log on to the VM. I left Host Key Verification Strategy to the default of Manually trusted key Verification Strategy . Under Availability , I left the default option of Keep this agent online as much as possible . Lastly, I clicked on Save . Now, I had to launch the agent so Jenkins can provision jobs to it. The following are the steps I performed to do so: I navigated to Dashboard > Manage Jenkins > Manage Nodes . I clicked on Agent VM (the name of the node I gave). I clicked on Launch agent button to bring the node online and connect it to the Jenkins master node. Note : When I launched the agent for the first time, it did not work as I did not have Java installed on the VM. So, I installed the required JRE with the command - sudo apt install default-jre . Relaunching the agent after installing Java worked and the agent was online and connected to the master node of Jenkins successfully. The only thing remaining was to install W3AF on the Agent VM. I followed the first three steps as mentioned under the 'Configuring W3AF' above to do the same. Integrating W3AF with Jenkins Now that the Agent VM was fully configured and integrated with Jenkins, I had to amend the pipeline script, that I was using previously, to accommodate the new tool being used as well as utilize the newly provisioned agent node. I started off by amending the script, that I wrote initially, to use the IP of the Jenkins VM as that was were DVNA would be running. So, I replaced all localhost entries with 10.0.2.19:9090 , which is where the app was running. I placed the script in the Agent VM itself to not have to copy it every time I had to perform DAST and also because of the fact that, once written, it will not be changed frequently. Next, I wrote a stage to perform the scan with W3AF, save the report on the Agent VM and then copy it over to the Jenkins VM. I also specified that Jenkins provisions this stage to the Agent VM by using the label I assigned to it while creating the node. Since the SSH keys I used were for a user other than the jenkins user, I could not have directly copied the report over to the reports directory inside Jenkins' home directory as this would have resulted in a Permission Denied error so, I first copied the file the non-jenkins user's home directory and will move it to the required location in the upcoming stage. The contents of the stage are below: stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } Lastly, I copied the report from the home directory of the non-jenkins user to the reports directory under Jenkins' home directory where all the other reports were kept from all the other scans. The stage I wrote to do the same is as follows: stage ('Copy Report to Jenkins Home') { steps { sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } The complete pipeline script, after adding stages for W3AF and removing redundant steps, is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } } }","title":"Dynamic Analysis"},{"location":"dynamic_analysis/#dynamic-analysis","text":"","title":"Dynamic Analysis"},{"location":"dynamic_analysis/#objective","text":"The aim of this section is to perform DAST for DVNA with OWASP ZAP , W3AF and generate a report to provide a solution to the 1st point of the problem statement under Task 2 .","title":"Objective"},{"location":"dynamic_analysis/#dast","text":"DAST or Dynamic Application Security Testing is a black-box testing technique in which the DAST tool interacts with the application being tested in its running state to imitate an attacker. Unlike static analysis, in DAST one does not have access to the source code of the application and the tool is completely reliant on the interactivity the application provides. In dynamic analysis, tools are used to automate attacks on the application ranging from SQL Injection, Input Validation, Cross-Site Scripting, and so forth.","title":"DAST"},{"location":"dynamic_analysis/#owasp-zap","text":"ZAP or Zed Attack Proxy is an open-source tool used to perform dynamic application security testing designed specifically for web applications. ZAP has a desktop interface, APIs for it to be used in an automated fashion and also a CLI. It imitates an actual user where it interacts with the application to perform various attacks. ZAP comes with a plethora of options to use, for which further details can be found here . ZAP also comes as a Docker image which is more convenient to use especially if one is using the CLI interface. Docker is a tool designed to provide ease in shipping applications across platforms. It packages the application, along with all its dependencies and other required libraries into an image . Then containers (running instances of the image) can be used to run the application on any machine. It is similar to a virtual machine but it differs greatly from them based on the fact that it does not require a full-fledged operating system to run the application. Instead, it runs the application on the system's kernel itself by just bringing along the required libraries with it which could be missing on machines other than the one the application was built on. This allows Docker-based applications to be portable i.e. they can be run on any machine that can run Docker containers. It allows for various versions of the same tool/library running on a host as different containers do not care about what is happening inside another container. Further information on docker can be found in the official documentation .","title":"OWASP ZAP"},{"location":"dynamic_analysis/#configuring-owasp-zap-with-docker","text":"To use ZAP with docker, I needed to pull the image from docker hub and start off. I used this documentation from Mozilla as it had a lot of errors demonstrated along with their rectification steps for starting out with ZAP. This was missing from all the other sources I found. Note : While running ZAP scans below, I explicitly ran DVNA, without Jenkins, for it to be tested. To start off with ZAP, I pulled the docker image by running: docker pull owasp/zap2docker-stable Then I tried to run the tool with its CLI interface. The CLI threw an error which was because of an encoding inconsistency between Python2 and Python3. To rectify this issue, I had to explicitly specify the encoding to be used with the help of some environment variables. These environment variables can be seen in the command below along with the -e flag for Docker to inject these environment variables in the container: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli --help Next I ran a quick-scan to have a look at how ZAP outputs information. Since ZAP is running inside a Docker container, I could not use localhost or 127.0.0.1 as then the container would take it to be its own host. So, to overcome this issue I used the IP assigned to docker0 , the network interface created for Docker. This IP can be found with this one-liner: $(ip -f inet -o addr show docker0 | awk '{print $4}' | cut -d '/' -f 1) .Also, as ZAP also has an API that can be used to interact with it programmatically, I had to use --start-options '-config api.disablekey=true' as otherwise, ZAP tried (and failed) to connect to the proxy as the API key was not specified. Also, the -l option specifies the severity level at which ZAP should log a vulnerability to console (or to a report). The --self-contained flag tells ZAP to turn off the daemon once the scan is complete and the --spider option tells ZAP to crawl the target to find other links present on the target site. The complete command is as mentioned below: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli quick-scan --start-options '-config api.disablekey=true' --self-contained --spider -l Low http://172.17.0.1:9090 Now, I tried the active-scan option. For this I first needed to start the daemon for zap to be accessed by the CLI, run open-url to add the target URL to the configuration in ZAP-CLI (without this, active-scan option will not start a scan on the target) and then run the scan against DVNA. This step was not required previously as while running a quick-scan , the daemon is automatically started to run the scan and stopped after the scan is finished. To do run the daemon with Docker, I ran the following command (with the -u zap segment to execute things with the zap user instead of Docker's default root user and I also appended the --rm flag to delete the container, automatically, when I stop it): docker run --rm -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 --name zap -u zap -p 8090:8080 -d owasp/zap2docker-stable zap.sh -daemon -port 8080 -host 0.0.0.0 -config api.disablekey=true docker exec <CONTAINER NAME/ID> zap-cli open-url <TARGET URL> docker exec <CONTAINER NAME/ID> zap-cli active-scan <TARGET URL> Note : When I ran the scan initially, it dropped an error from Python saying that the 'Connection was Refused' while sending requests to the target through the ZAP proxy. It turned out to be an issue with the command written in the blog that I was following. It exposed the wrong port and hence rectifying the port options in the command to -p 8090:8080 and -port 8080 solved the issue and the scan worked. Another thing is that I chose 8090 for the host port as I already had Jenkins running on 8080 . Now to be able to scan DVNA from the CLI with ZAP-CLI, I required a context which basically was a configuration written in XML to define how to perform a scan. This context also had a set of credentials in it that were recorded with a ZEST script, which is a JSON-based form used to record interactions between a website and a user specially created for security tools focused on web applications. This context file along with the ZEST script would allow zap-cli to authenticate with DVNA to be able to scan the portion of the application that lies behind the login screen. I used the browser-based GUI to generate the context and the Zest script and exported them to the machine. But importing them created various complications as zap-cli was unable to identify the embedded Zest script in the context provided to it. Due to the above complications, I decided to use Zap's baseline scan instead. To start off ZAP baseline scan with the docker image, I ran the following command, where the -t flag specified the target and -l flag defined the alert/severity level, by following this documentation : docker run -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -l INFO Now, to save the report on the Jenkins machine, I needed to mount a volume with Docker. I used the -v flag (as mentioned in the docker documentation ) to mount the present working directory of the host to the /zap/wrk directory of the container and also added the -r flag to save scan output to a HTML report on the Jenkins machine: docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS","title":"Configuring OWASP ZAP with Docker"},{"location":"dynamic_analysis/#integrating-zap-with-jenkins","text":"After understanding how to use the baseline-scan and its various options, I started integrating the scan as part of DAST in the Jenkins pipeline. But before I could scan DVNA with ZAP Baseline, I needed to build the dependencies and start an instance of DVNA. To do the same, I also had to fetch code from the repository on GitHub (explicitly, because I didn't use a Jenkinsfile for this task). Note : I chose to build a new pipeline just for DAST, as combining SAST and DAST in a single pipeline would have taken too much time during each execution which felt unnecessary at the time of development. To start off, I added a stage to fetch the code from the GitHub repository as follows: stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } Next, I built the dependencies for DVNA, as I did while performing SAST, and started an instance of DVNA, as mentioned in the stage mentioned below: stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } Note : I kept all the required environment variables in the env.sh file which can be seen in the above stage, and used it to export those variables for DVNA to be able to connect with the database. While trying to export the variables, the shell that comes along with Jenkins kept throwing an error 'source not found' i.e. saying it didn't recognize the command source . This turned out to be because that by default the sh shell in Jenkins points to /bin/dash which doesn't have the command source . To rectify this, I changed the Jenkins shell to point to /bin/bash instead with this command - sudo ln -sf /bin/bash /bin/sh , which I found in this blog . This method, however, will change the symlink for sh to bash for every user on the system (which can be created by other applications, such as Jenkins itself). This might break the functioning of the other applications and hence, it is more advisable to specify the changed shell for the specific user on the system that needs it. In the context of Jenkins, the recommended way would be to use usermod -s /bin/bash jenkins to change the default shell only for Jenkins. Now, that DVNA was up and running, ran the baseline scan on it with docker and Zap. But I had to wrap the command in a shell script to evade the non-zero status code that ZAP gives on finding issues. So, I wrote the script, baseline-scan.sh , mentioned below and made it executable with chmod +x : cd /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker-stable zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS echo $? > /dev/null Note : One thing to take note of is that Jenkins would require to use sudo when running docker commands as it's not part of the docker user group on the system. This is the preferred way, that the required docker setup is on another VM and Jenkins SSHs into the other VM with access to run docker commands as a sudo user. But for the purpose of this task, I did not set up another VM, instead, I added the jenkins user to the docker user group with - sudo usermod -aG docker jenkins , for it to be able to perform an operation without using sudo . This, however, is not recommended. I added a stage in the Jenkins Pipeline, to execute the shell script and generate the DAST report. The stage's content is mentioned below: stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } Note : Running the pipeline threw an access error, saying the report could not be written in the directory. This was because the zap user in the docker container did not have write permission for Jenkins' workspace. So, for the sake of the task, I modified the permissions of the node-dast-pipeline/ directory with chmod 777 node-dast-pipeline/ . This is also not recommended. If the permissions need to be changed, they should be specific and exact in terms of the access they grant which should not be more than that is required. Lastly, I added a stage to stop the instance of DVNA that was running as it was no longer needed and moved the report generated from the workspace directory to the reports directory that I've been using for the tasks: stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } The complete pipeline script is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } } } On a different note, I also tried running a docker agent in the Jenkins pipeline to execute zap-baseline scan but it kept throwing an access error (different than the one mentioned above). This issue is not rectified yet. The stage that I wrote (but was not working) is as follows: stage ('Run ZAP for DAST') { agent { docker { image 'owasp/zap2docker-stable' args '-v /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline/:/zap/wrk/' } } steps { sh 'zap-baseline.py -t http://172.17.0.1:9090 -r zap_baseline_report.html -l PASS' } }","title":"Integrating ZAP with Jenkins"},{"location":"dynamic_analysis/#w3af","text":"W3AF or the Web Application Attack and Audit Framework is another DAST tool like OWASP ZAP. Like ZAP, W3AF has both, a web-based GUI as well as a command-line interface. It is able to identify 200+ distinct vulnerabilities including SQL Injections, Cross-Site Scripting and OS Command Injection.","title":"W3AF"},{"location":"dynamic_analysis/#configuring-w3af","text":"Referring to the official documentation for W3AF, I first cloned the project's repository and moved into the project directory: git clone https://github.com/andresriancho/w3af.git && cd w3af/ Since, I was ultimately going to run the tool with the command-line interface as part of the pipeline, I skipped using the GUI that was available and started the CLI as follows: ./w3af_console I ran the console version of the tool, even though I knew it would not work (initially) since it had missing dependencies because W3AF automatically generates a bash script under /tmp for the user to install the unmet dependencies. The script just contains the command the user would have ran on the terminal manually, it just tries to make the tool easier to use to improve user's experience while using the tool. After executing W3AF's console script to generate the dependency-installation script, I ran it to install the missing dependencies for W3AF: /tmp/w3af_dependency_install.sh Once all the dependencies were installed, I executed the console script again to start the CLI for W3AF. It uses a prompt-based system, prefixed with w3af>>> as the prompt, and there are various options to configure the tool before launching the scan with it. I followed along the documentation to understand the various options and configurations that W3AF supports. W3AF also supports automation by allowing the user to write a configuration script, which is nothing but the exact same commands that one would have ran in the prompt in that same sequence written in a file, and pass it to W3AF with the -s flag. So, I picked up a template script from the project's repository which had all the scan options enabled. I did, however, made some amendments to the script. I removed the segment for bruteforcing injectable fields, I added a block to add authentication for DVNA and lastly made some changes by replacing values in the script to fit DVNA's context. The finished script, w3af_scan_script.w3af , is mentioned below: # Traversing into the Plugins Menu plugins # Configuring output options output console,text_file output config text_file set output_file output-w3af.txt set verbose True back output config console set verbose False back # Configuring audit Options audit all audit # Configuring grep options grep all grep # Configuring crawl options crawl all, !bing_spider, !google_spider, !spider_man crawl # Configuring auth options auth detailed auth config detailed set username admin set password admin set username_field username set password_field password set data_format %u=%U&%p=%P set auth_url <AUTH URL> set method POST set check_url <VERIFICATION URL> set check_string 'Logout' back # Traversing from plugin menu to main menu back # Configuring target options target set target <TARGET URL> back # Launching the scan start # Exiting from W3AF after the scan exit W3AF behind-the-scenes takes the commands written in the script mentioned above and passes them to the prompt on its own, allowing users to not have to pass arguments through the CLI prompt. Lastly, to launch the tool and start the scan with W3AF, I ran the following command, passing the script mentioned above: ./w3af_console -s w3af_scan_script.w3af","title":"Configuring W3AF"},{"location":"dynamic_analysis/#configuring-jenkins-for-distributed-builds","text":"After having so many tools configured on the Jenkins VM, it was getting tedious to manage all of them on the same server. So, to do away with this issue, I used to Master-Agent architecture that Jenkins supports to use a new VM to run DAST on DVNA with W3AF installed on a jenkins agent running under the new VM.","title":"Configuring Jenkins for Distributed Builds"},{"location":"dynamic_analysis/#configuring-agent-vm-for-jenkins","text":"The first step to creating an agent VM for Jenkins is to create a working directory where all the files associated with the Jenkins project will be kept along with some additional files/executables that Jenkins will use to use the Agent. I chose to create a /var/jenkins directory as follows: mkdir /var/jenkins Once, the directory was created I had to change the ownership of the directory to the non-root user ( ayush ) to avoid having to use sudo with commands that run on the Agent. To change the ownership of the directory, I used the following command: sudo chown ayush: /var/jenkins/ Then for the non-root user to be able to write files in the directory via Jenkins Agent, I gave the user write permissions. This allowed Jenkins Master node to write an agent.jar file on the Agent VM which will be used to provision jobs to the agent. I used the following command to modify the write access to the directory: sudo chmod u+w /var/jenkins To create an agent with Jenkins, I followed the following steps: I navigated to Dashboard > Manage Jenkins > Manage Nodes > New Node . I gave a name to the node, Agent VM , in this case, selected the Permanent Agent option (as the VM being used was managed outside of Jenkins) and clicked on Okay . Then I gave a brief description of what the Agent will be used for. I left the number of executors to the default setting of 1 . Under Remote root directory , I provided /var/jenkins which I configured earlier while setting up the Agent VM. I added the label raf-vm so I could use this label to specify stage(s) to use this agent instead of the master node. I chose the Use this node as much as possible option but since it is being used for only one task, it does not make any difference. Under Launch method , I chose the Launch agent agents with SSH option. I added the IP of the Agent VM and provided the credentials required to log on to the VM. I left Host Key Verification Strategy to the default of Manually trusted key Verification Strategy . Under Availability , I left the default option of Keep this agent online as much as possible . Lastly, I clicked on Save . Now, I had to launch the agent so Jenkins can provision jobs to it. The following are the steps I performed to do so: I navigated to Dashboard > Manage Jenkins > Manage Nodes . I clicked on Agent VM (the name of the node I gave). I clicked on Launch agent button to bring the node online and connect it to the Jenkins master node. Note : When I launched the agent for the first time, it did not work as I did not have Java installed on the VM. So, I installed the required JRE with the command - sudo apt install default-jre . Relaunching the agent after installing Java worked and the agent was online and connected to the master node of Jenkins successfully. The only thing remaining was to install W3AF on the Agent VM. I followed the first three steps as mentioned under the 'Configuring W3AF' above to do the same.","title":"Configuring Agent VM for Jenkins"},{"location":"dynamic_analysis/#integrating-w3af-with-jenkins","text":"Now that the Agent VM was fully configured and integrated with Jenkins, I had to amend the pipeline script, that I was using previously, to accommodate the new tool being used as well as utilize the newly provisioned agent node. I started off by amending the script, that I wrote initially, to use the IP of the Jenkins VM as that was were DVNA would be running. So, I replaced all localhost entries with 10.0.2.19:9090 , which is where the app was running. I placed the script in the Agent VM itself to not have to copy it every time I had to perform DAST and also because of the fact that, once written, it will not be changed frequently. Next, I wrote a stage to perform the scan with W3AF, save the report on the Agent VM and then copy it over to the Jenkins VM. I also specified that Jenkins provisions this stage to the Agent VM by using the label I assigned to it while creating the node. Since the SSH keys I used were for a user other than the jenkins user, I could not have directly copied the report over to the reports directory inside Jenkins' home directory as this would have resulted in a Permission Denied error so, I first copied the file the non-jenkins user's home directory and will move it to the required location in the upcoming stage. The contents of the stage are below: stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } Lastly, I copied the report from the home directory of the non-jenkins user to the reports directory under Jenkins' home directory where all the other reports were kept from all the other scans. The stage I wrote to do the same is as follows: stage ('Copy Report to Jenkins Home') { steps { sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } The complete pipeline script, after adding stages for W3AF and removing redundant steps, is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } } }","title":"Integrating W3AF with Jenkins"},{"location":"final_pipeline/","text":"Final Pipeline Structure Objective The aim of this section is to define the final structure achieved for the pipeline after the various testing stages integrated into the pipeline as part of the solutions for all the three tasks in the problem statement . Final Pipeline After completing various testing stages (SAST, DAST, Code Quality Analysis) for DVNA, the last task was to combine all the segments together as all three tasks were tested on separate pipelines. Combining them also meant removing redundant steps from the stages to create a lean pipeline. All the scripts used in this pipeline remain the same as what they were when mentioned in the previous sections in the report. Below is a diagrammatic representation of the flow of the entire pipeline: The final pipeline script that was the result of combining all three segregated pipelines and removing redundancies is mentioned below: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh 'npm install' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Building DVNA') { steps { sh ''' source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } } stage ('Lint Analysis with EsLint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } } stage ('Generating Software Bill of Materials') { steps { sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && cd dvna && pm2 start server.js\"' } } } }","title":"Final Pipeline Structure"},{"location":"final_pipeline/#final-pipeline-structure","text":"","title":"Final Pipeline Structure"},{"location":"final_pipeline/#objective","text":"The aim of this section is to define the final structure achieved for the pipeline after the various testing stages integrated into the pipeline as part of the solutions for all the three tasks in the problem statement .","title":"Objective"},{"location":"final_pipeline/#final-pipeline","text":"After completing various testing stages (SAST, DAST, Code Quality Analysis) for DVNA, the last task was to combine all the segments together as all three tasks were tested on separate pipelines. Combining them also meant removing redundant steps from the stages to create a lean pipeline. All the scripts used in this pipeline remain the same as what they were when mentioned in the previous sections in the report. Below is a diagrammatic representation of the flow of the entire pipeline: The final pipeline script that was the result of combining all three segregated pipelines and removing redundancies is mentioned below: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh 'npm install' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Building DVNA') { steps { sh ''' source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Run W3AF for DAST') { agent { label 'raf-vm' } steps { sh '/{PATH TO SCRIPT}/w3af/w3af_console -s /{PATH TO SCRIPT}/scripts/w3af_scan_script.w3af' sh 'scp -r /{PATH TO OUTPUT}/w3af/output-w3af.txt chaos@10.0.2.19:/{HOME DIRECTORY}/' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' sh 'cp /{HOME DIRECTORY}/output-w3af.txt /{JENKINS HOME DIRECTORY}/reports/w3af-report' } } stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } } stage ('Lint Analysis with EsLint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } } stage ('Generating Software Bill of Materials') { steps { sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && cd dvna && pm2 start server.js\"' } } } }","title":"Final Pipeline"},{"location":"generating_sbom/","text":"Generating Software Bill of Materials Objective The aim of this section is to generate a Software Bill of Materials for DVNA and provide a solution to the 2nd point of the problem statement under Task 3 . Software Bill of Materials A Bill of Materials is a list of components used to assemble/create a product. It gives out a specification about how each component was used in the making of the end product. A Software Bill of Materials (SBoM) refers to a list of all software components, open-source or commercial, that was utilized to build a software solution. A more detailed description of software bill of materials can be found here . CycloneDX DVNA, like most other applications, is built with dependencies. To generate the SBoM for DVNA, I found a tool called CycloneDX . According to its documentation , it is a tool that creates the SBoM which contains the aggregate of all the dependencies for the project. CycloneDX is available to be used an NPM package that can generate SBoMs for Nodejs applications but also comes in a variety of implementations to serve projects using different stacks such as Python, Maven, .NET, etc. For my use case, I stuck with the NPM package as DVNA only utilizes Nodejs. Generating SBoM for DVNA To start off, I installed CycleDX's Node module with NPM by following the official documentation, and using the command: npm install -g @cyclonedx/bom Then I ran CycloneDX, with the command mentioned below, in the root directory of the project to gauge the output and figure out the structure of the SBoM generated: cyclonedx-bom Note : I initially ran the scan before building the modules as I used the project directory from the Lint Analysis pipeline which did not require me to build DVNA. So, I had to run npm install before I ran CycloneDX again as it required the /node_modules directory to look through and identify the dependencies. Lastly, I added a stage in the pipeline to run CycloneDX and store the SBoM ( sbom.xml ) in the local reports folder that I have been using through the entirety of the tasks: stage ('Generating Software Bill of Materials') { steps { //Building the dependencies to generate SBoM sh 'npm install' sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } } Software Bill of Material for DVNA CycloneDX generated a comprehensive SBoM for DVNA. It was in XML format. For each dependency, CycloneDX reported - Name of the Module, the version being used, its description, its hash checksum, the license the module uses, the package URL and external references (if any). The full Software Bill of Material generated by CycloneDX can be found here .","title":"Generating Software Bill of Materials"},{"location":"generating_sbom/#generating-software-bill-of-materials","text":"","title":"Generating Software Bill of Materials"},{"location":"generating_sbom/#objective","text":"The aim of this section is to generate a Software Bill of Materials for DVNA and provide a solution to the 2nd point of the problem statement under Task 3 .","title":"Objective"},{"location":"generating_sbom/#software-bill-of-materials","text":"A Bill of Materials is a list of components used to assemble/create a product. It gives out a specification about how each component was used in the making of the end product. A Software Bill of Materials (SBoM) refers to a list of all software components, open-source or commercial, that was utilized to build a software solution. A more detailed description of software bill of materials can be found here .","title":"Software Bill of Materials"},{"location":"generating_sbom/#cyclonedx","text":"DVNA, like most other applications, is built with dependencies. To generate the SBoM for DVNA, I found a tool called CycloneDX . According to its documentation , it is a tool that creates the SBoM which contains the aggregate of all the dependencies for the project. CycloneDX is available to be used an NPM package that can generate SBoMs for Nodejs applications but also comes in a variety of implementations to serve projects using different stacks such as Python, Maven, .NET, etc. For my use case, I stuck with the NPM package as DVNA only utilizes Nodejs.","title":"CycloneDX"},{"location":"generating_sbom/#generating-sbom-for-dvna","text":"To start off, I installed CycleDX's Node module with NPM by following the official documentation, and using the command: npm install -g @cyclonedx/bom Then I ran CycloneDX, with the command mentioned below, in the root directory of the project to gauge the output and figure out the structure of the SBoM generated: cyclonedx-bom Note : I initially ran the scan before building the modules as I used the project directory from the Lint Analysis pipeline which did not require me to build DVNA. So, I had to run npm install before I ran CycloneDX again as it required the /node_modules directory to look through and identify the dependencies. Lastly, I added a stage in the pipeline to run CycloneDX and store the SBoM ( sbom.xml ) in the local reports folder that I have been using through the entirety of the tasks: stage ('Generating Software Bill of Materials') { steps { //Building the dependencies to generate SBoM sh 'npm install' sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } }","title":"Generating SBoM for DVNA"},{"location":"generating_sbom/#software-bill-of-material-for-dvna","text":"CycloneDX generated a comprehensive SBoM for DVNA. It was in XML format. For each dependency, CycloneDX reported - Name of the Module, the version being used, its description, its hash checksum, the license the module uses, the package URL and external references (if any). The full Software Bill of Material generated by CycloneDX can be found here .","title":"Software Bill of Material for DVNA"},{"location":"glossary/","text":"Glossary There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Agent VM The VM which has a Jenkins Agent installed to use the distributed build architecture. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM. EC2 Elastic Cloud Compute; A virtual machine on AWS Cloud. ECS Elastic Container Service; A container orchestration service on AWS Cloud. ECR Elastic Container Registry; A container registry on AWS Cloud. RDS Relational Database Service; A managed relational DB on AWS Cloud. IAM Identity and Access Management; A service to manage and access resources on AWS Cloud.","title":"Glossary"},{"location":"glossary/#glossary","text":"There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Agent VM The VM which has a Jenkins Agent installed to use the distributed build architecture. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM. EC2 Elastic Cloud Compute; A virtual machine on AWS Cloud. ECS Elastic Container Service; A container orchestration service on AWS Cloud. ECR Elastic Container Registry; A container registry on AWS Cloud. RDS Relational Database Service; A managed relational DB on AWS Cloud. IAM Identity and Access Management; A service to manage and access resources on AWS Cloud.","title":"Glossary"},{"location":"limitations/","text":"Limitations Objective The aim of this section is to create a similar pipeline for SuiteCRM, a different application than DVNA, to identify assumptions made whilst creating the pipeline for DVNA to provide a solution for 1st point of the problem statement under Task 5 . SuiteCRM The application that I chose for the validation of the previously built pipeline was SuiteCRM . It is a Customer Relationship Management tool which is the open-source forked version of SugarCRM . SuiteCRM adds a few additional features to its fork and is free to use. I chose the application because of of it being a bigger application than DVNA and it is also an application that is used in the real-world and is not just a dummy application. Another reason was that it is written in PHP and hence, the difference in stack from DVNA also would help identifying assumptions made in the previous pipeline created as the solution to the problem statements. Configuration Before I began setting up tools on the virtual machine, I forked the SuiteCRM GitHub repository on my own account. My fork of SuiteCRM can be found here . I also created a new pipeline, named suitecrm-aws-pipeline , following the steps from this report's earlier section on Setting Up Pipeline . Since SuiteCRM is written in PHP, I had to install a few things on the Jenkins EC2 instance. I followed this article to perform the necessary configurations for the application. I did, however, skipped all steps related to installing and configuring MariaDB as I already had MySQL Community installed on the system. I also skipped the 10th step as I cloned my fork of the SuiteCRM GitHub repository and did not use the zipped archive to get the source code of the applications. Lastly, I also skipped steps 19 and 20 because I did not need to set up the Cron job as it was not required in the context of the problem statement. Trying to view SuiteCRM from the browser (step 12 from the article) I faced an issue that the packages were not built. This was due to me cloning the repository instead of using the zip archive. Hence, I had to install Composer , the package manager for PHP. I followed the official documentation and performed to required steps to install Composer globally. After a successful installation, I ran composer install in the project's root directory to build the dependencies for SuiteCRM. Since SuiteCRM is a PHP application, I also needed to install a web-server to serve the application. I chose Apache for this as mentioned in the article I was using to set up SuiteCRM. Since, I had previously mapped port 80 to Jenkins' port 8080, I had to change the port Apache would listen on. To do this, I used the command - sudo nano /etc/apache2/ports.conf and made the following changes to make Apache listen on port 9090: #Listen 80 Listen 9090 <IfModule ssl_module> Listen 443 </IfModule> <IfModule mod_gnutls.c> Listen 443 </IfModule> SAST The first thing that came to light while creating the pipeline for SuiteCRM was the fact that since it was based on a different tech stack than DVNA, most of the SAST tools I used previously were incompatible with it. The only two tools that worked with both DVNA and SuiteCRM are mentioned below: Dependency Check Since it was already installed I did not need do a fresh installation for Dependency Check. I amended the command to point to the workspace directory of the new pipeline ( /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline ). I also renamed the output to be called suitecrm-dependency-check-report . The complete report generated by Dependency Check for SuiteCRM can be here . Snyk Like Dependency Checkm, since Snyk was already installed I just created a new script to run Synk from. I amended the script, like Dependency Check,to point to /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline and renamed the output report as suitecrm-snyk-report . The rest of the script remained as it is. The complete report generated by Snyk for SuiteCRM can be here . Since majority of the previously used SAST tools were not suitable to test a PHP application, I found two new tools built to perform SAST specifically on PHP applications. These applications are mentioned below: Symfony I followed the official documentation to download the Symfony CLI executable from this link mentioned in the documentation and moved it to /usr/local/bin to be accessible for all users. I used the binary executable as it was more convenient than using Composer to install it as a package. Symfony gave a non-zero status code on identifying issues so I wrote a bash script ( suitecrm-symfony.sh ) and placed it under tool_scripts/ directory. The complete report generated by Symfony for SuiteCRM can be here . The contents of the script ( suitecrm-symfony.sh ) are mentioned below: #!/bin/bash symfony security:check --dir /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline --format json > /{JENKINS HOME DIRECTORY}/reports/suitecrm-symfony-report echo $?> /dev/null PHP Malware Finder I followed the documentation present in the official GitHub repository for PHP Malware Finder (PMF). I first installed Yara with apt and then cloned the repository as instructed in the documentation in the tool_scripts directory. Since PMF did not exited with a non-zero code even after identifying a security issue, I added the scan command /{JENKINS HOME DIRECTORY}/tool_scripts/php-malware-finder/php-malware-finder/phpmalwarefinder $(pwd) > /{JENKINS HOME DIRECTORY}/reports/suitecrm-pmf-report as it is to a stage in the pipeline. The complete report generated by PHP Malware Finder for SuiteCRM can be here . DAST To perform DAST, I had to deploy SuiteCRM on the Jenkins EC2 instance for the tools to perform scans and attack the application. Another limitation that became evident was the fact that removing all application files and cloning a new copy from the source code repository required me to do the configuration for the application through the web-based console even if the database had the relevant entries persisted on the database. I looked around in the changes that happened after doing the initial configuration with the web-based console and found that SuiteCRM creates a new file, config.php , which stored the settings. The solution to this was to retain this configuration file while being able to fetch changes made to the source-code of the application. I thought of two solutions - store this config file locally and copy it after a fresh fetch of the source code or, the simpler solution, pull changes from the repository with a git pull . I chose the latter as it was the simpler solution. The next limitation surfaced when I had to use Apache as the web-server as SuiteCRM, unlike DVNA, does not ship with a development server. The problem was that starting and stopping Apache's service on the virtual machine required sudo privileges, that were not granted to the Jenkins system user. I found this answer on StackOverflow which explained how to grant passwordless sudo privileges for specific executables/scripts. I used this answer as reference to add the Jenkins system user to the sudoers file and grant it sudo privilege to start and stop the Apache web-server. I amended the sudoers file by adding the following line to it under User privilege specification : jenkins ALL= NOPASSWD: /etc/init.d/apache2 After granting the required privilege to Jenkins system user, I used the following commands to start and stop the Apache web-server: # To start the web-server sudo /etc/init.d/apache2 start # To stop the web-server sudo /etc/init.d/apache2 stop Since DAST is a black-box testing, where the tool interacts with the application being tested by performing actions like an attacker would, both the previously used tools (OWASP ZAP and W3AF) worked with SuiteCRM without any issues. I used the Jenkins Agent EC2 instance to run the tests like I did previously. The changes I made are mentioned below: OWASP ZAP I wrote an identical script as before and changed the output report's name to suitecrm-zap-report . I added a stage in the pipeline to use this new script to perform DAST on SuiteCRM. The complete report generated by OWASP ZAP for SuiteCRM can be here . W3AF I wrote an identical configuration script for W3AF as before and changed the values to reflect the names of 'username' and 'password' fields on the login page for SuiteCRM. I also renamed the output report's name to suitecrm-w3af-report . I added a stage in the pipeline to run the W3AF console executable with the new configuration script. The complete report generated by W3AF for SuiteCRM can be here . Code Quality Analysis Since the tools used for Code Quality Analysis are language-specific, I could not use the tools I used before for DVNA. Hence, I found a couple of tools that are meant for PHP applications. The tools I used are PHP Code Sniffer (PHPCS) and PHP Mess Detector (PHPMD). PHP Code Sniffer PHP Code Sniffer or PHPCS is a set of two PHP scripts - one that analyzes the code for violations of coding conventions and the other which can automatically fix the identified issues. For the purpose of the task in the problem statement, I was only concerned with identifying the linting issues and hence skipped the second script. To install PHPCS for Code Quality Analysis, I downloaded the phar executable for the scanner with wget (as instructed in the documentation ), made it executable with chmod and moved it to /usr/local/bin/ for it to be accessible to all system users. The commands I used to achieve these steps are mentioned below: wget https://squizlabs.github.io/PHP_CodeSniffer/phpcs.phar chmod +x phpcs.phar mv phpcs.phar /usr/local/bin/phpcs Next, I ran it PHPCS on the SuiteCRM project directory with the command mentioned below: phpcs /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline Note : Executing PHPCS on the whole project directory caused the virtual machine to run out of free memory which crashed PHPCS. So, I wrote a Python script to identify all PHP files present in the SuiteCRM project directory and ran PHPCS on the files individually and appended these individual results to suitecrm-phpcs-report . The contents of the script I wrote are mentioned below: #!/usr/bin/python3 import os import sys try: BASE_PATH = sys.argv[1] except IndexError: print('[-] Path not supplied...') sys.exit(1) paths = [BASE_PATH] php_files = [] print('[+] Scanning directory for PHP files...') while paths != []: base_path = paths.pop() try: with os.scandir(base_path) as entries: for entry in entries: if entry.is_file(): if entry.name.endswith('.php'): php_files.append(os.path.join(base_path, entry.name)) else: paths.append(os.path.join(base_path, entry.name)) except PermissionError: print(f'[-] Could not open {base_path} due to insufficient permission...') print('[+] Scan completed...') print('[+] Running linter on PHP files...') try: for php_file in php_files: print(f'[+] Scanning {php_file}') os.system(f'phpcs {php_file} >> /var/lib/jenkins/reports/suitecrm-phpcs-report') print('[+] All PHP files scanned...') print('[+] Code Quality Report generated...') except KeyboardInterrupt: print('[-] Exiting...') Lastly, I added the a stage in the pipeline to execute the Python script by supplying it the path of the project directory to scan. PHP Mess Detector PHP Mess Detector is another tool that finds Linting issues in PHP code. It is quite similar to PHPCS in terms of installation but has more customisability by allowing the user to write custom rules for the analysis as well as writing the output to a file in XML, HTML, Text and JSON formats. To start off, I downloaded the phar executable with the below mentioned command, as instructed in the official documentation : wget https://phpmd.org/static/latest/phpmd.phar Next, I made it executable with chmod and then moved the executable to /usr/local/bin to make it accessible to all system users: chmod +x phpmd.phar mv phpmd.phar /usr/local/bin/phpmd Then I tested the tool by executing it against the project directory for SuiteCRM with the below mentioned command, where the output report would be in XML format and the rule-set used would be cleancode that comes along with PHPMD: phpmd /path/to/source/ xml cleancode --reportfile /{JENKINS HOME DIRECTORY}/reports/suitecrm-phpmd-report Lastly, I added a stage to the pipeline to run the required command, generate the report and store it in the reports/ directory. Generating Software Bill of Materials Unlike DVNA, which is built on Nodejs, SuiteCRM is a PHP application and hence, I could not have used the CycloneDX's Nodejs binding. So, I had to switch to the PHP binding available from CycloneDX which can be found here . I had to first install cyclonedx-php-composer for which I used the command mentioned in the documentation present in the GitHub repository: composer require --dev cyclonedx/cyclone-php-composer Then to verify it was working, I ran the following command to generate the SBoM: composer make-bom After verifying the SBoM generated, I added a stage in the pipeline to install cyclonedx-php-composer as a dev dependency, run it to generate the SBoM and lastly, move the SBoM to the reports/ directory. The Software Bill of Materials generated by CycloneDX can be found here . Deploying SuiteCRM SuiteCRM posed another challenge when it came to deploying it to production. Since, it created a dynamic configuration file ( config.php ) after the initial installation and using the Docker image would require to go through the set up after each deployment, I chose to use a EC2 instance to deploy SuiteCRM. Also, now that I was using a full-fledged virtual machine, I set up the database for SuiteCRM also on the instance itself. The following are the steps I added to the stage to deploy SuiteCRM to the production EC2 instance: First, I copied the dependencies built locally in the Jenkins EC2 instance over to the production instance in a directory suitecrm/ . scp -r * ubuntu@<PRODUCTION VM IP>:/home/ubuntu/suitecrm Next, I stopped the Apache service running on the production instance. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo systemctl stop apache2\" I removed the existing files from /var/www/html/ for SuiteCRM's previous deployment. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo rm -r /var/www/html/*\" I copied over the new files from suitecrm/ to /var/www/html . ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo cp -r suitecrm/* /var/www/html\" I also copied the config.php file that contained all the configuration created from the first deployment of SuiteCRM on the production instance. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo cp config.php /var/www/html\" I changed the owner of the copied files to www-data with sudo chown -R www-data: /var/www/html . ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo chown -R www-data: /var/www/html\" Next, I restarted the Apache web-server. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo systemctl start apache2\" Lastly, I created a stage in the pipeline to add all the above mentioned steps for deployment.","title":"Limitations"},{"location":"limitations/#limitations","text":"","title":"Limitations"},{"location":"limitations/#objective","text":"The aim of this section is to create a similar pipeline for SuiteCRM, a different application than DVNA, to identify assumptions made whilst creating the pipeline for DVNA to provide a solution for 1st point of the problem statement under Task 5 .","title":"Objective"},{"location":"limitations/#suitecrm","text":"The application that I chose for the validation of the previously built pipeline was SuiteCRM . It is a Customer Relationship Management tool which is the open-source forked version of SugarCRM . SuiteCRM adds a few additional features to its fork and is free to use. I chose the application because of of it being a bigger application than DVNA and it is also an application that is used in the real-world and is not just a dummy application. Another reason was that it is written in PHP and hence, the difference in stack from DVNA also would help identifying assumptions made in the previous pipeline created as the solution to the problem statements.","title":"SuiteCRM"},{"location":"limitations/#configuration","text":"Before I began setting up tools on the virtual machine, I forked the SuiteCRM GitHub repository on my own account. My fork of SuiteCRM can be found here . I also created a new pipeline, named suitecrm-aws-pipeline , following the steps from this report's earlier section on Setting Up Pipeline . Since SuiteCRM is written in PHP, I had to install a few things on the Jenkins EC2 instance. I followed this article to perform the necessary configurations for the application. I did, however, skipped all steps related to installing and configuring MariaDB as I already had MySQL Community installed on the system. I also skipped the 10th step as I cloned my fork of the SuiteCRM GitHub repository and did not use the zipped archive to get the source code of the applications. Lastly, I also skipped steps 19 and 20 because I did not need to set up the Cron job as it was not required in the context of the problem statement. Trying to view SuiteCRM from the browser (step 12 from the article) I faced an issue that the packages were not built. This was due to me cloning the repository instead of using the zip archive. Hence, I had to install Composer , the package manager for PHP. I followed the official documentation and performed to required steps to install Composer globally. After a successful installation, I ran composer install in the project's root directory to build the dependencies for SuiteCRM. Since SuiteCRM is a PHP application, I also needed to install a web-server to serve the application. I chose Apache for this as mentioned in the article I was using to set up SuiteCRM. Since, I had previously mapped port 80 to Jenkins' port 8080, I had to change the port Apache would listen on. To do this, I used the command - sudo nano /etc/apache2/ports.conf and made the following changes to make Apache listen on port 9090: #Listen 80 Listen 9090 <IfModule ssl_module> Listen 443 </IfModule> <IfModule mod_gnutls.c> Listen 443 </IfModule>","title":"Configuration"},{"location":"limitations/#sast","text":"The first thing that came to light while creating the pipeline for SuiteCRM was the fact that since it was based on a different tech stack than DVNA, most of the SAST tools I used previously were incompatible with it. The only two tools that worked with both DVNA and SuiteCRM are mentioned below:","title":"SAST"},{"location":"limitations/#dependency-check","text":"Since it was already installed I did not need do a fresh installation for Dependency Check. I amended the command to point to the workspace directory of the new pipeline ( /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline ). I also renamed the output to be called suitecrm-dependency-check-report . The complete report generated by Dependency Check for SuiteCRM can be here .","title":"Dependency Check"},{"location":"limitations/#snyk","text":"Like Dependency Checkm, since Snyk was already installed I just created a new script to run Synk from. I amended the script, like Dependency Check,to point to /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline and renamed the output report as suitecrm-snyk-report . The rest of the script remained as it is. The complete report generated by Snyk for SuiteCRM can be here . Since majority of the previously used SAST tools were not suitable to test a PHP application, I found two new tools built to perform SAST specifically on PHP applications. These applications are mentioned below:","title":"Snyk"},{"location":"limitations/#symfony","text":"I followed the official documentation to download the Symfony CLI executable from this link mentioned in the documentation and moved it to /usr/local/bin to be accessible for all users. I used the binary executable as it was more convenient than using Composer to install it as a package. Symfony gave a non-zero status code on identifying issues so I wrote a bash script ( suitecrm-symfony.sh ) and placed it under tool_scripts/ directory. The complete report generated by Symfony for SuiteCRM can be here . The contents of the script ( suitecrm-symfony.sh ) are mentioned below: #!/bin/bash symfony security:check --dir /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline --format json > /{JENKINS HOME DIRECTORY}/reports/suitecrm-symfony-report echo $?> /dev/null","title":"Symfony"},{"location":"limitations/#php-malware-finder","text":"I followed the documentation present in the official GitHub repository for PHP Malware Finder (PMF). I first installed Yara with apt and then cloned the repository as instructed in the documentation in the tool_scripts directory. Since PMF did not exited with a non-zero code even after identifying a security issue, I added the scan command /{JENKINS HOME DIRECTORY}/tool_scripts/php-malware-finder/php-malware-finder/phpmalwarefinder $(pwd) > /{JENKINS HOME DIRECTORY}/reports/suitecrm-pmf-report as it is to a stage in the pipeline. The complete report generated by PHP Malware Finder for SuiteCRM can be here .","title":"PHP Malware Finder"},{"location":"limitations/#dast","text":"To perform DAST, I had to deploy SuiteCRM on the Jenkins EC2 instance for the tools to perform scans and attack the application. Another limitation that became evident was the fact that removing all application files and cloning a new copy from the source code repository required me to do the configuration for the application through the web-based console even if the database had the relevant entries persisted on the database. I looked around in the changes that happened after doing the initial configuration with the web-based console and found that SuiteCRM creates a new file, config.php , which stored the settings. The solution to this was to retain this configuration file while being able to fetch changes made to the source-code of the application. I thought of two solutions - store this config file locally and copy it after a fresh fetch of the source code or, the simpler solution, pull changes from the repository with a git pull . I chose the latter as it was the simpler solution. The next limitation surfaced when I had to use Apache as the web-server as SuiteCRM, unlike DVNA, does not ship with a development server. The problem was that starting and stopping Apache's service on the virtual machine required sudo privileges, that were not granted to the Jenkins system user. I found this answer on StackOverflow which explained how to grant passwordless sudo privileges for specific executables/scripts. I used this answer as reference to add the Jenkins system user to the sudoers file and grant it sudo privilege to start and stop the Apache web-server. I amended the sudoers file by adding the following line to it under User privilege specification : jenkins ALL= NOPASSWD: /etc/init.d/apache2 After granting the required privilege to Jenkins system user, I used the following commands to start and stop the Apache web-server: # To start the web-server sudo /etc/init.d/apache2 start # To stop the web-server sudo /etc/init.d/apache2 stop Since DAST is a black-box testing, where the tool interacts with the application being tested by performing actions like an attacker would, both the previously used tools (OWASP ZAP and W3AF) worked with SuiteCRM without any issues. I used the Jenkins Agent EC2 instance to run the tests like I did previously. The changes I made are mentioned below:","title":"DAST"},{"location":"limitations/#owasp-zap","text":"I wrote an identical script as before and changed the output report's name to suitecrm-zap-report . I added a stage in the pipeline to use this new script to perform DAST on SuiteCRM. The complete report generated by OWASP ZAP for SuiteCRM can be here .","title":"OWASP ZAP"},{"location":"limitations/#w3af","text":"I wrote an identical configuration script for W3AF as before and changed the values to reflect the names of 'username' and 'password' fields on the login page for SuiteCRM. I also renamed the output report's name to suitecrm-w3af-report . I added a stage in the pipeline to run the W3AF console executable with the new configuration script. The complete report generated by W3AF for SuiteCRM can be here .","title":"W3AF"},{"location":"limitations/#code-quality-analysis","text":"Since the tools used for Code Quality Analysis are language-specific, I could not use the tools I used before for DVNA. Hence, I found a couple of tools that are meant for PHP applications. The tools I used are PHP Code Sniffer (PHPCS) and PHP Mess Detector (PHPMD).","title":"Code Quality Analysis"},{"location":"limitations/#php-code-sniffer","text":"PHP Code Sniffer or PHPCS is a set of two PHP scripts - one that analyzes the code for violations of coding conventions and the other which can automatically fix the identified issues. For the purpose of the task in the problem statement, I was only concerned with identifying the linting issues and hence skipped the second script. To install PHPCS for Code Quality Analysis, I downloaded the phar executable for the scanner with wget (as instructed in the documentation ), made it executable with chmod and moved it to /usr/local/bin/ for it to be accessible to all system users. The commands I used to achieve these steps are mentioned below: wget https://squizlabs.github.io/PHP_CodeSniffer/phpcs.phar chmod +x phpcs.phar mv phpcs.phar /usr/local/bin/phpcs Next, I ran it PHPCS on the SuiteCRM project directory with the command mentioned below: phpcs /{JENKINS HOME DIRECTORY}/workspace/suitecrm-aws-pipeline Note : Executing PHPCS on the whole project directory caused the virtual machine to run out of free memory which crashed PHPCS. So, I wrote a Python script to identify all PHP files present in the SuiteCRM project directory and ran PHPCS on the files individually and appended these individual results to suitecrm-phpcs-report . The contents of the script I wrote are mentioned below: #!/usr/bin/python3 import os import sys try: BASE_PATH = sys.argv[1] except IndexError: print('[-] Path not supplied...') sys.exit(1) paths = [BASE_PATH] php_files = [] print('[+] Scanning directory for PHP files...') while paths != []: base_path = paths.pop() try: with os.scandir(base_path) as entries: for entry in entries: if entry.is_file(): if entry.name.endswith('.php'): php_files.append(os.path.join(base_path, entry.name)) else: paths.append(os.path.join(base_path, entry.name)) except PermissionError: print(f'[-] Could not open {base_path} due to insufficient permission...') print('[+] Scan completed...') print('[+] Running linter on PHP files...') try: for php_file in php_files: print(f'[+] Scanning {php_file}') os.system(f'phpcs {php_file} >> /var/lib/jenkins/reports/suitecrm-phpcs-report') print('[+] All PHP files scanned...') print('[+] Code Quality Report generated...') except KeyboardInterrupt: print('[-] Exiting...') Lastly, I added the a stage in the pipeline to execute the Python script by supplying it the path of the project directory to scan.","title":"PHP Code Sniffer"},{"location":"limitations/#php-mess-detector","text":"PHP Mess Detector is another tool that finds Linting issues in PHP code. It is quite similar to PHPCS in terms of installation but has more customisability by allowing the user to write custom rules for the analysis as well as writing the output to a file in XML, HTML, Text and JSON formats. To start off, I downloaded the phar executable with the below mentioned command, as instructed in the official documentation : wget https://phpmd.org/static/latest/phpmd.phar Next, I made it executable with chmod and then moved the executable to /usr/local/bin to make it accessible to all system users: chmod +x phpmd.phar mv phpmd.phar /usr/local/bin/phpmd Then I tested the tool by executing it against the project directory for SuiteCRM with the below mentioned command, where the output report would be in XML format and the rule-set used would be cleancode that comes along with PHPMD: phpmd /path/to/source/ xml cleancode --reportfile /{JENKINS HOME DIRECTORY}/reports/suitecrm-phpmd-report Lastly, I added a stage to the pipeline to run the required command, generate the report and store it in the reports/ directory.","title":"PHP Mess Detector"},{"location":"limitations/#generating-software-bill-of-materials","text":"Unlike DVNA, which is built on Nodejs, SuiteCRM is a PHP application and hence, I could not have used the CycloneDX's Nodejs binding. So, I had to switch to the PHP binding available from CycloneDX which can be found here . I had to first install cyclonedx-php-composer for which I used the command mentioned in the documentation present in the GitHub repository: composer require --dev cyclonedx/cyclone-php-composer Then to verify it was working, I ran the following command to generate the SBoM: composer make-bom After verifying the SBoM generated, I added a stage in the pipeline to install cyclonedx-php-composer as a dev dependency, run it to generate the SBoM and lastly, move the SBoM to the reports/ directory. The Software Bill of Materials generated by CycloneDX can be found here .","title":"Generating Software Bill of Materials"},{"location":"limitations/#deploying-suitecrm","text":"SuiteCRM posed another challenge when it came to deploying it to production. Since, it created a dynamic configuration file ( config.php ) after the initial installation and using the Docker image would require to go through the set up after each deployment, I chose to use a EC2 instance to deploy SuiteCRM. Also, now that I was using a full-fledged virtual machine, I set up the database for SuiteCRM also on the instance itself. The following are the steps I added to the stage to deploy SuiteCRM to the production EC2 instance: First, I copied the dependencies built locally in the Jenkins EC2 instance over to the production instance in a directory suitecrm/ . scp -r * ubuntu@<PRODUCTION VM IP>:/home/ubuntu/suitecrm Next, I stopped the Apache service running on the production instance. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo systemctl stop apache2\" I removed the existing files from /var/www/html/ for SuiteCRM's previous deployment. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo rm -r /var/www/html/*\" I copied over the new files from suitecrm/ to /var/www/html . ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo cp -r suitecrm/* /var/www/html\" I also copied the config.php file that contained all the configuration created from the first deployment of SuiteCRM on the production instance. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo cp config.php /var/www/html\" I changed the owner of the copied files to www-data with sudo chown -R www-data: /var/www/html . ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo chown -R www-data: /var/www/html\" Next, I restarted the Apache web-server. ssh -o StrictHostKeyChecking=no ubuntu@<PRODUCTION VM IP> \"sudo systemctl start apache2\" Lastly, I created a stage in the pipeline to add all the above mentioned steps for deployment.","title":"Deploying SuiteCRM"},{"location":"moving_setup_to_aws/","text":"Shifting Local Setup to AWS Objective The aim of this section is to shift the entire setup from the local machine to Amazon Web Services (AWS) Cloud to provide a solution to the 1st point of the problem statement under Task 4 . Configuring Jenkins with EC2 Instances Starting an EC2 instance To start shifting the entire setup that I had locally on my machine, firstly I brought up an EC2 instance to install and run Jenkins on. Below mentioned are the steps to spin up an EC2 instance: Firstly, I navigated to the EC2 page under 'Services'. Then I clicked on the 'Launch instance' button and selected the 'Launch instance' option from the drop-down menu instead of the 'Launch instance from template' option as I did not have any template configured. Under the 'Choose AMI' menu, I selected the 'Ubuntu Server 18.04 LTS (HVM), SSD Volume Type' option as I was using Ubuntu 18.04 as the OS on my local setup. Under the 'Choose an Instance Type' menu, I selected 't2.medium' type primarily because of the requirement of at least 4GB memory to run all the tools along with Jenkins on the instance. I left everything under the 'Configure Instance Details' page to their default values as no change was needed here. Under the 'Add Storage' page, I changed the storage size from 8GB to 10GB, again, to accommodate all the tools that will be installed on the system. I left all the other options to their defaults. Under the 'Add Tags' page I added a tag with the instance's name ('Jenkins [Master]'). Under 'Configure Security Group' page: I clicked on the \"Add Rule\" button to add a new 'Custom TCP Rule', gave '8080' as the 'Port Range' because that is where the Jenkins UI is accessible. Under the 'Source' column, I selected the 'My IP' option to allow access only from my current IP. I gave a brief description of both the rules I added for the instance. Lastly, I clicked on the 'Launch' button on the 'Review Instance Launch' page. Installing Jenkins on EC2 Instance After successfully starting the instance, I had to install Jenkins on it. I used the steps from the previous section that I wrote on the same. Starting Jenkins after the installation, I encountered an issue that I had not faced when I was running it on my machine locally. The URLs from where Jenkins fetches plugins had a few redirects which it was not able to handle on its own and failed to install any plugin. To rectify this issue, I ended up using Nginx (pronounced as 'Engine-X'), which is a reverse proxy and was able to handle the redirects successfully. To install Nginx, I followed this documentation . I, however, skipped step 5 on 'Setting Up Server Blocks' as it was not needed in the context of the problem statement. Lastly, as part of configuring Nginx, I wrote a config file, jenkins-config , whose contents are mentioned below: server { listen 80; server_name <EC2 PUBLIC IP>; location / { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Fix the \"It appears that your reverse proxy set up is broken\" error. proxy_pass http://127.0.0.1:8080; proxy_read_timeout 90; # proxy_redirect http://127.0.0.1:8080; # Required for new HTTP-based CLI proxy_http_version 1.1; proxy_request_buffering off; # workaround for https://issues.jenkins-ci.org/browse/JENKINS-45651 # add_header 'X-SSH-Endpoint' '<EC2 PUBLIC IP>:50022' always; } } I placed this script under the /etc/nginx/sites-available/ directory. Next, I created a symlink to the config file, jenkins-config , as follows: sudo ln -s /etc/nginx/sites-available/jenkins-config /etc/nginx/sites-enabled/ Lastly, I reloaded the Nginx service to have it run the latest configuration: sudo nginx -s reload After resolving the issue, I installed the plugins that Jenkins recommends. Configuring SAST Tools After Jenkins was set up on an EC2 instance on AWS, the next step was to add all the tools required to perform the various tests on the application. Below, I have mentioned how I went about doing the same on AWS with a note about any additional steps I took when I deviate from the setup instructions mentioned in Static Analysis . I, once again, made a reports/ directory inside Jenkins Home Directory to store all reports generated in a single location. This time, I also added a tool_scripts/ directory in the same location to house the various scripts I was required to use through the entire pipeline. SonarQube The installation for SonarQube is divided into two halves - setting up SonarQube Server and setting up the SonarQube Scanner. I was using docker previously to run the SonarQube Server on the local setup and hence, I had to use available services on AWS to run the SonarQube server as a container. For the second half, I re-used the steps from my documentation on installing SonarQube. To set up a container with SonarQube Server running in it, I followed this tutorial as it explained things in a simpler language as compared to other available articles. I, however, skipped steps 1 and 5 as I directly pulled the docker image from docker hub and I did not want to use a domain to point to the container. So, in essence, I started off by creating a cluster on ECS (which is a collective of services and tasks), created a new task definition (which is the details about the container) and lastly, created a service to run the task (container). There was one other thing where I deviated from the tutorial I used, in the security group configuration, I added rules to allow access to the container only from my current IP by selecting the 'My IP' option under the 'Source' field and another rule to allow the Jenkins EC2 instance to access the container via its public IP. Note : I had a doubt initially whether or not I could pull images from Docker hub directly. After searching for a bit, it turned that I could. I just had to specify the repository URL structure as docker.io/<docker-image-name>:version while creating the task definition. After setting up SonarQube Server, the rest of the setup was identical to the local setup. For configuring SonarQube Scanner, I followed all the steps (except the first one) as mentioned previously in the report here . NPM Audit For NPM Audit, I did not have to do any AWS specific configuration as it was installed on the Jenkins EC2 Instance itself and followed the steps from the documentation I wrote for the local setup. There were a few things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step. NodeJsScan For NodeJsScan, I again did not have to any additional step or deviate from the documentation I wrote in the static analysis section. Retire.js For Retire.js, I followed the documentation exactly as I had when I was setting it up on the local VM as there are no scripts or special requirements associated with this tool. OWASP Dependency Check For OWASP Dependency Check, I followed the documentation I wrote previously. The segments where I deviated are as follows: I placed the unzipped dependency-check/ directory in tool_scripts/ instead of Jenkins home directory. I amended the stage by changing the paths wherever necessary. Auditjs For Auditjs, I followed the steps from the documentation I wrote for the local setup. There were a few additional things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step. Snyk For Auditjs, I followed the steps from the documentation I wrote for the local setup. There were a few additional things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step. Configuring DAST Tools Both DAST tools, ZAP and W3AF, were set up on an EC2 instance different than the one running Jenkins Master node. This new EC2 instance was configured as a Jenkins Agent to run jobs making use of Jenkins' ability of distributed builds. To allow Jenkins Master EC2 instance to have access to the Jenkins Agent EC2, I configured SSH Keys and added a new node as I did previously with the local setup mentioned here . Note : Since DVNA had to be running for DAST, I used PM2 to run DVNA on the Jenkins Master EC2 instance. At one point, DVNA was trying to fetch view from a wrong directory even after restarting, stopping and starting again. This turned out to be because of the existence of caching when an app is run with PM2 as mentioned in this article . The rectification of this issue was also mentioned in the article. I had to delete the app from PM2's app list and then start it afterward. This solved the problem. OWASP ZAP I used ZAP with its docker image as I did in the local setup. I followed these instructions to first install Docker on the Agent EC2 instance. I then followed my own documentation to run the ZAP baseline scan with the official docker image. After pulling the image from Docker Hub, I skipped to running the baseline scan as I had already tested the usage mentioned in the report previously and hence, there was no need to repeat the process. W3AF I used W3AF exactly as before in the local setup, by installing it on the Agent EC2 instance. I configured W3AF exactly as mentioned in the documentation I wrote previously. Configuring Code Analysis Tools JsHint I followed the documentation I wrote for JsHint while setting it up locally exactly as mentioned. EsLint I followed the documentation I wrote for Eslint while setting it up locally exactly as mentioned. I also stored the required configurations JSON ( .eslintrc.json ) in the tool_scripts/ directory and copied it to the workspace directory during the execution of the lint analysis stage with EsLint. Generating Software Bill of Materials I followed the documentation I wrote previously to generate the Software Bill of Materials for DVNA with CycloneDX. I installed CycloneDX as I did before for the local setup. In the pipeline stage to generate the SBoM, I skipped the npm install step as here, I had already built the dependencies in a prior stage in the pipeline. Configuring Jenkins Pipeline Webhook for Jenkins on AWS To configure the webhook to trigger the execution of the pipeline on push and release events, I followed the steps exactly as mentioned in this report . I just replaced the payload URL to use the public IP of the EC2 instance running Jenkins. I also skipped the segment on Ngrok as in this case, the VM was accessible over the web via the public IP provided by AWS to the instance, hence, there was no need to use Ngrok . Deploying DVNA on AWS Since I was shifting everything to AWS I thought of changing the deployment strategy for DVNA from running it on a VM to running it as a Docker Container on AWS. I redid the deployment from scratch specifically for AWS utilizing features it offers. The steps I followed to deploy DVNA on AWS as a container are mentioned below: I decided to use the Jenkins Agent EC2 machine to build and deploy DVNA on AWS as a docker container. I installed AWS CLI to perform actions from the terminal itself and not the web console. I followed along with Amazon's official documentation for the same as it was well written and concise. I, however, skipped the section on 'Install Pip' as I already had pip installed on the machine. Next, I authenticated the AWS CLI by running aws configure and providing the prompt with my Access Key ID , Secret Access Key , Default Region and Output Format . Since, I wanted to use the docker images I build and not from the official docker images available on Docker Hub, I had to create a Registry on Amazon Container Registry (ECR) to store these custom images. For this, I required an additional policy, AmazonEC2ContainerRegistryPowerUser , to be attached to my IAM role. After the policy was attached to my IAM Role, I created a new registry, dvna-aws-registry , by executing aws ecr create-repository --repository-name dvna-aws-registry on the Jenkins Agent EC2 instance's terminal with help from this blog . The next step was to build the initial image and push to the registry I made in the previous step to be able to launch the initial deployment of DVNA on AWS ECS. To do this, I pulled DVNA from GitHub and ran docker build -t dvna_app . to create the image locally on the Jenkins Agent EC2 instance. Then I had to tag the image to be able to push it to the ECR registry I created previously. I did so by running the following command: docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest I, then, pushed the image to the registry on ECR with the following command: docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest I decided on another alteration to the deployment strategy I had for the local setup, I used AWS RDS to host a MySQL database for DVNA. I created a database instance, mysql-db , on RDS using the console, added the Master User as root, set the password for it and took note of the database URL. Next, I created a task definition ( deployDVNA ), as I did for SonarQube following this tutorial , to run DVNA by pulling the image I created from the ECR registry. Since the database was not another container, I just passed along the database connection configuration details as environment variables while configuring the container details. I created a new cluster from the console, deploymentCluster , added a new service ( dvnaDeployService ) and added a new task inside it from the task definition in the previous step and ran the task. Note : Running the task failed initially. Going through the logs, it turned out to be because the app was not able to connect with the database as there was no database named dvna as was needed. So, I created this database manually by connecting to the RDS database instance from the terminal by logging in to the mysql with mysql -u root -h <RDS DB URL> -p and then creating the database with create database dvna; and then exited. Re-running the task again resulted in successful deployment on DVNA with ECS on AWS. Now, to deploy the new image every time, I decided on stopping the currently running tasks under dvnaDeployService as then the service would automatically fetch the image with the tag 'latest' and run a new task. Since, every time I push a new image with the 'latest' tag, the previous one would get untagged, the service would always fetch the latest build. To do this I looked up how to list and stop tasks. I followed Amazon's official documentation to list tasks and to stop tasks . Since, I was now clear with the individual steps, I created a script to combine each action rather than running them individually from the pipeline. The script first authenticated the docker CLI to be able to push images, cloned the project repository, built the image, tagged it with 'latest' and pushed to ECR. It then fetched all active tasks running DVNA and stopped them and waited for them to brought back up with the latest image. Lastly, it again fetched the URLs for the latest deployment of DVNA. The script's contents are mentioned below: #!/bin/bash # Cloning the project repository to build the image git clone https://github.com/ayushpriya10/dvna.git cd dvna # Login to the ECR Registry to push docker images $(aws ecr get-login --no-include-email --region us-east-2) # Building the docker image, tagging it as 'latest' and pushing it to the registry docker build -t dvna_app . docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest # Fetching all active tasks running under the 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Stopping all tasks which are running the older docker image of DVNA for task in $task_arns do echo \"Stopping Task: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` aws ecs stop-task --cluster deploymentCluster --task $task_id > /dev/null done # Waiting for 'dvnaDeployService' to automatically run a new task echo \"Waiting for 1 minute for AWS to bring up new ECS Tasks...\" sleep 1m # Fetching all active tasks under 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Printing the URL where DVNA instance(s) were deployed for task in $task_arns do echo \"New Task ARN: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` task_attachments=`aws ecs describe-tasks --cluster deploymentCluster --tasks $task_id | jq '.tasks[0].attachments[0].details' | jq -c '.[]'` for attachment in $task_attachments do name=`echo $attachment | jq '.name'` if [ \"$name\" == \"\\\"networkInterfaceId\\\"\" ]; then interface_id=`echo $attachment | jq '.value'` interface_id=`echo $interface_id | tr -d \"\\\"\"` fi done public_ip=`aws ec2 describe-network-interfaces --network-interface-ids $interface_id | jq '.NetworkInterfaces[0].Association.PublicIp' | tr -d \"\\\"\"` echo \"DVNA is deployed at: http://$public_ip:9090\" done Next, I added a stage in the pipeline to execute this script on the Jenkins Agent EC2 instance. Note : Due to a issue in the $PATH variable, aws was not recognized as a command (even though it worked fine when I ran it on the machine directly over SSH). To solve this issue I added a symlink as follows: sudo ln -s /home/ubuntu/.local/bin/aws /usr/local/bin/aws Though the deployment setup was complete, I added an additional step to clear out all the older docker images (which got untagged) from the machine using the following command: docker rmi $(docker images | grep none | awk '{print $3}') Final Pipeline Structure After making all the amendments required to shift the entire setup from a local machine to AWS, the pipeline structure was altered a bit. I took this opportunity to rename a few stages and make small changes to the pipeline syntax to make it more uniform and clean. The updated pipeline script, though identical in function as the local version, with the amendments is mentioned below: pipeline { agent any stages { stage ('Fetching Code from Repository') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building Dependencies') { steps { sh 'npm install' } } stage ('SAST with SonarQube') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube Server') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('SAST with NPM Audit') { steps { sh '/var/lib/jenkins/tool_scripts/npm-audit.sh' } } stage ('SAST with NodeJsScan') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('SAST with Retire.js') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('SAST with Dependency Check') { steps { sh '/var/lib/jenkins/tool_scripts/dependency-check/bin/dependency-check.sh --scan /var/lib/jenkins/workspace/dvna-aws-pipeline --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('SAST with Auditjs') { steps { sh '/var/lib/jenkins/tool_scripts/auditjs.sh' } } stage ('SAST with Snyk') { steps { sh '/var/lib/jenkins/tool_scripts/snyk.sh' } } stage ('Starting DVNA for DAST') { steps { sh ''' source /var/lib/jenkins/tool_scripts/env.sh pm2 restart server.js ''' } } stage ('DAST with ZAP') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/zap_baseline.sh' sh 'scp baseline-report.html jenkins@3.14.249.80:/var/lib/jenkins/reports/zap-report' } } stage ('DAST with W3AF') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/w3af/w3af_console -s /home/ubuntu/w3af_config' sh 'scp output-w3af.txt jenkins@3.14.249.80:/var/lib/jenkins/reports/w3af-report' } } stage ('Stopping DVNA Instance') { steps { sh 'pm2 stop server.js' } } stage ('Code Quality Analysis with JsHint') { steps { sh '/var/lib/jenkins/tool_scripts/jshint.sh' } } stage ('Code Quality Analysis with EsLint') { steps { sh '/var/lib/jenkins/tool_scripts/eslint.sh' } } stage ('Generating Software Bill of Materials') { steps { sh 'cyclonedx-bom -o /var/lib/jenkins/reports/sbom.xml' } } stage ('Build and Deploy DVNA') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/task-manager.sh' sh 'rm -rf ./*' sh 'docker rmi $(docker images | grep none | awk \\'{print $3}\\')' } } } }","title":"Shifting Local Setup to AWS"},{"location":"moving_setup_to_aws/#shifting-local-setup-to-aws","text":"","title":"Shifting Local Setup to AWS"},{"location":"moving_setup_to_aws/#objective","text":"The aim of this section is to shift the entire setup from the local machine to Amazon Web Services (AWS) Cloud to provide a solution to the 1st point of the problem statement under Task 4 .","title":"Objective"},{"location":"moving_setup_to_aws/#configuring-jenkins-with-ec2-instances","text":"","title":"Configuring Jenkins with EC2 Instances"},{"location":"moving_setup_to_aws/#starting-an-ec2-instance","text":"To start shifting the entire setup that I had locally on my machine, firstly I brought up an EC2 instance to install and run Jenkins on. Below mentioned are the steps to spin up an EC2 instance: Firstly, I navigated to the EC2 page under 'Services'. Then I clicked on the 'Launch instance' button and selected the 'Launch instance' option from the drop-down menu instead of the 'Launch instance from template' option as I did not have any template configured. Under the 'Choose AMI' menu, I selected the 'Ubuntu Server 18.04 LTS (HVM), SSD Volume Type' option as I was using Ubuntu 18.04 as the OS on my local setup. Under the 'Choose an Instance Type' menu, I selected 't2.medium' type primarily because of the requirement of at least 4GB memory to run all the tools along with Jenkins on the instance. I left everything under the 'Configure Instance Details' page to their default values as no change was needed here. Under the 'Add Storage' page, I changed the storage size from 8GB to 10GB, again, to accommodate all the tools that will be installed on the system. I left all the other options to their defaults. Under the 'Add Tags' page I added a tag with the instance's name ('Jenkins [Master]'). Under 'Configure Security Group' page: I clicked on the \"Add Rule\" button to add a new 'Custom TCP Rule', gave '8080' as the 'Port Range' because that is where the Jenkins UI is accessible. Under the 'Source' column, I selected the 'My IP' option to allow access only from my current IP. I gave a brief description of both the rules I added for the instance. Lastly, I clicked on the 'Launch' button on the 'Review Instance Launch' page.","title":"Starting an EC2 instance"},{"location":"moving_setup_to_aws/#installing-jenkins-on-ec2-instance","text":"After successfully starting the instance, I had to install Jenkins on it. I used the steps from the previous section that I wrote on the same. Starting Jenkins after the installation, I encountered an issue that I had not faced when I was running it on my machine locally. The URLs from where Jenkins fetches plugins had a few redirects which it was not able to handle on its own and failed to install any plugin. To rectify this issue, I ended up using Nginx (pronounced as 'Engine-X'), which is a reverse proxy and was able to handle the redirects successfully. To install Nginx, I followed this documentation . I, however, skipped step 5 on 'Setting Up Server Blocks' as it was not needed in the context of the problem statement. Lastly, as part of configuring Nginx, I wrote a config file, jenkins-config , whose contents are mentioned below: server { listen 80; server_name <EC2 PUBLIC IP>; location / { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Fix the \"It appears that your reverse proxy set up is broken\" error. proxy_pass http://127.0.0.1:8080; proxy_read_timeout 90; # proxy_redirect http://127.0.0.1:8080; # Required for new HTTP-based CLI proxy_http_version 1.1; proxy_request_buffering off; # workaround for https://issues.jenkins-ci.org/browse/JENKINS-45651 # add_header 'X-SSH-Endpoint' '<EC2 PUBLIC IP>:50022' always; } } I placed this script under the /etc/nginx/sites-available/ directory. Next, I created a symlink to the config file, jenkins-config , as follows: sudo ln -s /etc/nginx/sites-available/jenkins-config /etc/nginx/sites-enabled/ Lastly, I reloaded the Nginx service to have it run the latest configuration: sudo nginx -s reload After resolving the issue, I installed the plugins that Jenkins recommends.","title":"Installing Jenkins on EC2 Instance"},{"location":"moving_setup_to_aws/#configuring-sast-tools","text":"After Jenkins was set up on an EC2 instance on AWS, the next step was to add all the tools required to perform the various tests on the application. Below, I have mentioned how I went about doing the same on AWS with a note about any additional steps I took when I deviate from the setup instructions mentioned in Static Analysis . I, once again, made a reports/ directory inside Jenkins Home Directory to store all reports generated in a single location. This time, I also added a tool_scripts/ directory in the same location to house the various scripts I was required to use through the entire pipeline.","title":"Configuring SAST Tools"},{"location":"moving_setup_to_aws/#sonarqube","text":"The installation for SonarQube is divided into two halves - setting up SonarQube Server and setting up the SonarQube Scanner. I was using docker previously to run the SonarQube Server on the local setup and hence, I had to use available services on AWS to run the SonarQube server as a container. For the second half, I re-used the steps from my documentation on installing SonarQube. To set up a container with SonarQube Server running in it, I followed this tutorial as it explained things in a simpler language as compared to other available articles. I, however, skipped steps 1 and 5 as I directly pulled the docker image from docker hub and I did not want to use a domain to point to the container. So, in essence, I started off by creating a cluster on ECS (which is a collective of services and tasks), created a new task definition (which is the details about the container) and lastly, created a service to run the task (container). There was one other thing where I deviated from the tutorial I used, in the security group configuration, I added rules to allow access to the container only from my current IP by selecting the 'My IP' option under the 'Source' field and another rule to allow the Jenkins EC2 instance to access the container via its public IP. Note : I had a doubt initially whether or not I could pull images from Docker hub directly. After searching for a bit, it turned that I could. I just had to specify the repository URL structure as docker.io/<docker-image-name>:version while creating the task definition. After setting up SonarQube Server, the rest of the setup was identical to the local setup. For configuring SonarQube Scanner, I followed all the steps (except the first one) as mentioned previously in the report here .","title":"SonarQube"},{"location":"moving_setup_to_aws/#npm-audit","text":"For NPM Audit, I did not have to do any AWS specific configuration as it was installed on the Jenkins EC2 Instance itself and followed the steps from the documentation I wrote for the local setup. There were a few things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step.","title":"NPM Audit"},{"location":"moving_setup_to_aws/#nodejsscan","text":"For NodeJsScan, I again did not have to any additional step or deviate from the documentation I wrote in the static analysis section.","title":"NodeJsScan"},{"location":"moving_setup_to_aws/#retirejs","text":"For Retire.js, I followed the documentation exactly as I had when I was setting it up on the local VM as there are no scripts or special requirements associated with this tool.","title":"Retire.js"},{"location":"moving_setup_to_aws/#owasp-dependency-check","text":"For OWASP Dependency Check, I followed the documentation I wrote previously. The segments where I deviated are as follows: I placed the unzipped dependency-check/ directory in tool_scripts/ instead of Jenkins home directory. I amended the stage by changing the paths wherever necessary.","title":"OWASP Dependency Check"},{"location":"moving_setup_to_aws/#auditjs","text":"For Auditjs, I followed the steps from the documentation I wrote for the local setup. There were a few additional things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step.","title":"Auditjs"},{"location":"moving_setup_to_aws/#snyk","text":"For Auditjs, I followed the steps from the documentation I wrote for the local setup. There were a few additional things that I did: I placed the bash script inside the tool_scripts/ directory mentioned at the start of this section. I amended the stage in the pipeline to execute the script from the new directory mentioned in the previous step.","title":"Snyk"},{"location":"moving_setup_to_aws/#configuring-dast-tools","text":"Both DAST tools, ZAP and W3AF, were set up on an EC2 instance different than the one running Jenkins Master node. This new EC2 instance was configured as a Jenkins Agent to run jobs making use of Jenkins' ability of distributed builds. To allow Jenkins Master EC2 instance to have access to the Jenkins Agent EC2, I configured SSH Keys and added a new node as I did previously with the local setup mentioned here . Note : Since DVNA had to be running for DAST, I used PM2 to run DVNA on the Jenkins Master EC2 instance. At one point, DVNA was trying to fetch view from a wrong directory even after restarting, stopping and starting again. This turned out to be because of the existence of caching when an app is run with PM2 as mentioned in this article . The rectification of this issue was also mentioned in the article. I had to delete the app from PM2's app list and then start it afterward. This solved the problem.","title":"Configuring DAST Tools"},{"location":"moving_setup_to_aws/#owasp-zap","text":"I used ZAP with its docker image as I did in the local setup. I followed these instructions to first install Docker on the Agent EC2 instance. I then followed my own documentation to run the ZAP baseline scan with the official docker image. After pulling the image from Docker Hub, I skipped to running the baseline scan as I had already tested the usage mentioned in the report previously and hence, there was no need to repeat the process.","title":"OWASP ZAP"},{"location":"moving_setup_to_aws/#w3af","text":"I used W3AF exactly as before in the local setup, by installing it on the Agent EC2 instance. I configured W3AF exactly as mentioned in the documentation I wrote previously.","title":"W3AF"},{"location":"moving_setup_to_aws/#configuring-code-analysis-tools","text":"","title":"Configuring Code Analysis Tools"},{"location":"moving_setup_to_aws/#jshint","text":"I followed the documentation I wrote for JsHint while setting it up locally exactly as mentioned.","title":"JsHint"},{"location":"moving_setup_to_aws/#eslint","text":"I followed the documentation I wrote for Eslint while setting it up locally exactly as mentioned. I also stored the required configurations JSON ( .eslintrc.json ) in the tool_scripts/ directory and copied it to the workspace directory during the execution of the lint analysis stage with EsLint.","title":"EsLint"},{"location":"moving_setup_to_aws/#generating-software-bill-of-materials","text":"I followed the documentation I wrote previously to generate the Software Bill of Materials for DVNA with CycloneDX. I installed CycloneDX as I did before for the local setup. In the pipeline stage to generate the SBoM, I skipped the npm install step as here, I had already built the dependencies in a prior stage in the pipeline.","title":"Generating Software Bill of Materials"},{"location":"moving_setup_to_aws/#configuring-jenkins-pipeline","text":"","title":"Configuring Jenkins Pipeline"},{"location":"moving_setup_to_aws/#webhook-for-jenkins-on-aws","text":"To configure the webhook to trigger the execution of the pipeline on push and release events, I followed the steps exactly as mentioned in this report . I just replaced the payload URL to use the public IP of the EC2 instance running Jenkins. I also skipped the segment on Ngrok as in this case, the VM was accessible over the web via the public IP provided by AWS to the instance, hence, there was no need to use Ngrok .","title":"Webhook for Jenkins on AWS"},{"location":"moving_setup_to_aws/#deploying-dvna-on-aws","text":"Since I was shifting everything to AWS I thought of changing the deployment strategy for DVNA from running it on a VM to running it as a Docker Container on AWS. I redid the deployment from scratch specifically for AWS utilizing features it offers. The steps I followed to deploy DVNA on AWS as a container are mentioned below: I decided to use the Jenkins Agent EC2 machine to build and deploy DVNA on AWS as a docker container. I installed AWS CLI to perform actions from the terminal itself and not the web console. I followed along with Amazon's official documentation for the same as it was well written and concise. I, however, skipped the section on 'Install Pip' as I already had pip installed on the machine. Next, I authenticated the AWS CLI by running aws configure and providing the prompt with my Access Key ID , Secret Access Key , Default Region and Output Format . Since, I wanted to use the docker images I build and not from the official docker images available on Docker Hub, I had to create a Registry on Amazon Container Registry (ECR) to store these custom images. For this, I required an additional policy, AmazonEC2ContainerRegistryPowerUser , to be attached to my IAM role. After the policy was attached to my IAM Role, I created a new registry, dvna-aws-registry , by executing aws ecr create-repository --repository-name dvna-aws-registry on the Jenkins Agent EC2 instance's terminal with help from this blog . The next step was to build the initial image and push to the registry I made in the previous step to be able to launch the initial deployment of DVNA on AWS ECS. To do this, I pulled DVNA from GitHub and ran docker build -t dvna_app . to create the image locally on the Jenkins Agent EC2 instance. Then I had to tag the image to be able to push it to the ECR registry I created previously. I did so by running the following command: docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest I, then, pushed the image to the registry on ECR with the following command: docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest I decided on another alteration to the deployment strategy I had for the local setup, I used AWS RDS to host a MySQL database for DVNA. I created a database instance, mysql-db , on RDS using the console, added the Master User as root, set the password for it and took note of the database URL. Next, I created a task definition ( deployDVNA ), as I did for SonarQube following this tutorial , to run DVNA by pulling the image I created from the ECR registry. Since the database was not another container, I just passed along the database connection configuration details as environment variables while configuring the container details. I created a new cluster from the console, deploymentCluster , added a new service ( dvnaDeployService ) and added a new task inside it from the task definition in the previous step and ran the task. Note : Running the task failed initially. Going through the logs, it turned out to be because the app was not able to connect with the database as there was no database named dvna as was needed. So, I created this database manually by connecting to the RDS database instance from the terminal by logging in to the mysql with mysql -u root -h <RDS DB URL> -p and then creating the database with create database dvna; and then exited. Re-running the task again resulted in successful deployment on DVNA with ECS on AWS. Now, to deploy the new image every time, I decided on stopping the currently running tasks under dvnaDeployService as then the service would automatically fetch the image with the tag 'latest' and run a new task. Since, every time I push a new image with the 'latest' tag, the previous one would get untagged, the service would always fetch the latest build. To do this I looked up how to list and stop tasks. I followed Amazon's official documentation to list tasks and to stop tasks . Since, I was now clear with the individual steps, I created a script to combine each action rather than running them individually from the pipeline. The script first authenticated the docker CLI to be able to push images, cloned the project repository, built the image, tagged it with 'latest' and pushed to ECR. It then fetched all active tasks running DVNA and stopped them and waited for them to brought back up with the latest image. Lastly, it again fetched the URLs for the latest deployment of DVNA. The script's contents are mentioned below: #!/bin/bash # Cloning the project repository to build the image git clone https://github.com/ayushpriya10/dvna.git cd dvna # Login to the ECR Registry to push docker images $(aws ecr get-login --no-include-email --region us-east-2) # Building the docker image, tagging it as 'latest' and pushing it to the registry docker build -t dvna_app . docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest # Fetching all active tasks running under the 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Stopping all tasks which are running the older docker image of DVNA for task in $task_arns do echo \"Stopping Task: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` aws ecs stop-task --cluster deploymentCluster --task $task_id > /dev/null done # Waiting for 'dvnaDeployService' to automatically run a new task echo \"Waiting for 1 minute for AWS to bring up new ECS Tasks...\" sleep 1m # Fetching all active tasks under 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Printing the URL where DVNA instance(s) were deployed for task in $task_arns do echo \"New Task ARN: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` task_attachments=`aws ecs describe-tasks --cluster deploymentCluster --tasks $task_id | jq '.tasks[0].attachments[0].details' | jq -c '.[]'` for attachment in $task_attachments do name=`echo $attachment | jq '.name'` if [ \"$name\" == \"\\\"networkInterfaceId\\\"\" ]; then interface_id=`echo $attachment | jq '.value'` interface_id=`echo $interface_id | tr -d \"\\\"\"` fi done public_ip=`aws ec2 describe-network-interfaces --network-interface-ids $interface_id | jq '.NetworkInterfaces[0].Association.PublicIp' | tr -d \"\\\"\"` echo \"DVNA is deployed at: http://$public_ip:9090\" done Next, I added a stage in the pipeline to execute this script on the Jenkins Agent EC2 instance. Note : Due to a issue in the $PATH variable, aws was not recognized as a command (even though it worked fine when I ran it on the machine directly over SSH). To solve this issue I added a symlink as follows: sudo ln -s /home/ubuntu/.local/bin/aws /usr/local/bin/aws Though the deployment setup was complete, I added an additional step to clear out all the older docker images (which got untagged) from the machine using the following command: docker rmi $(docker images | grep none | awk '{print $3}')","title":"Deploying DVNA on AWS"},{"location":"moving_setup_to_aws/#final-pipeline-structure","text":"After making all the amendments required to shift the entire setup from a local machine to AWS, the pipeline structure was altered a bit. I took this opportunity to rename a few stages and make small changes to the pipeline syntax to make it more uniform and clean. The updated pipeline script, though identical in function as the local version, with the amendments is mentioned below: pipeline { agent any stages { stage ('Fetching Code from Repository') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building Dependencies') { steps { sh 'npm install' } } stage ('SAST with SonarQube') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube Server') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('SAST with NPM Audit') { steps { sh '/var/lib/jenkins/tool_scripts/npm-audit.sh' } } stage ('SAST with NodeJsScan') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('SAST with Retire.js') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('SAST with Dependency Check') { steps { sh '/var/lib/jenkins/tool_scripts/dependency-check/bin/dependency-check.sh --scan /var/lib/jenkins/workspace/dvna-aws-pipeline --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('SAST with Auditjs') { steps { sh '/var/lib/jenkins/tool_scripts/auditjs.sh' } } stage ('SAST with Snyk') { steps { sh '/var/lib/jenkins/tool_scripts/snyk.sh' } } stage ('Starting DVNA for DAST') { steps { sh ''' source /var/lib/jenkins/tool_scripts/env.sh pm2 restart server.js ''' } } stage ('DAST with ZAP') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/zap_baseline.sh' sh 'scp baseline-report.html jenkins@3.14.249.80:/var/lib/jenkins/reports/zap-report' } } stage ('DAST with W3AF') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/w3af/w3af_console -s /home/ubuntu/w3af_config' sh 'scp output-w3af.txt jenkins@3.14.249.80:/var/lib/jenkins/reports/w3af-report' } } stage ('Stopping DVNA Instance') { steps { sh 'pm2 stop server.js' } } stage ('Code Quality Analysis with JsHint') { steps { sh '/var/lib/jenkins/tool_scripts/jshint.sh' } } stage ('Code Quality Analysis with EsLint') { steps { sh '/var/lib/jenkins/tool_scripts/eslint.sh' } } stage ('Generating Software Bill of Materials') { steps { sh 'cyclonedx-bom -o /var/lib/jenkins/reports/sbom.xml' } } stage ('Build and Deploy DVNA') { agent { label 'jenkins-agent-ec2' } steps { sh '/home/ubuntu/task-manager.sh' sh 'rm -rf ./*' sh 'docker rmi $(docker images | grep none | awk \\'{print $3}\\')' } } } }","title":"Final Pipeline Structure"},{"location":"problem_statement/","text":"Problem Statement Task 1 Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next, the DVNA should get deployed in a server. Setup the infrastructure, required for the task, on 2 virtual machines running locally on a laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparative report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub. Task 2 Add onto Task 1 by implementing DAST in the pipeline for DVNA with OWASP ZAP and W3AF . Make a report on the performance of the tools used to perform DAST by doing a comparative analysis of the results they provide. Implement and use Semantic Versioning to tag versions of the documentation created as part of the tasks. Append content for the task to the existing documentation from first task. Task 3 Perform commit-based checks on the source code for linting errors to improve code quality and generate quality report. Generate software bill of materials for all dependencies. Task 4 Move the setup from the local machine to Cloud (AWS). Store required secrets separately with a Secrets Management Service. Task 5 Identify assumptions made while creating the pipeline for DVNA by implementing a similar setup for another application (SuiteCRM).","title":"Problem Statement"},{"location":"problem_statement/#problem-statement","text":"","title":"Problem Statement"},{"location":"problem_statement/#task-1","text":"Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next, the DVNA should get deployed in a server. Setup the infrastructure, required for the task, on 2 virtual machines running locally on a laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparative report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Task 1"},{"location":"problem_statement/#task-2","text":"Add onto Task 1 by implementing DAST in the pipeline for DVNA with OWASP ZAP and W3AF . Make a report on the performance of the tools used to perform DAST by doing a comparative analysis of the results they provide. Implement and use Semantic Versioning to tag versions of the documentation created as part of the tasks. Append content for the task to the existing documentation from first task.","title":"Task 2"},{"location":"problem_statement/#task-3","text":"Perform commit-based checks on the source code for linting errors to improve code quality and generate quality report. Generate software bill of materials for all dependencies.","title":"Task 3"},{"location":"problem_statement/#task-4","text":"Move the setup from the local machine to Cloud (AWS). Store required secrets separately with a Secrets Management Service.","title":"Task 4"},{"location":"problem_statement/#task-5","text":"Identify assumptions made while creating the pipeline for DVNA by implementing a similar setup for another application (SuiteCRM).","title":"Task 5"},{"location":"resources/","text":"References These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"Resources"},{"location":"resources/#references","text":"These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"References"},{"location":"secrets_management/","text":"Secrets Management Objective The aim of this section is to set up a secrets management service on the cloud to segregate the app and the associated secrets required for it to function and provide a solution to the 2nd point of the problem statement under Task 4 . Secrets Management Secrets management refers to the tools and methods for managing digital authentication credentials (secrets), including passwords, keys, APIs, and tokens for use in applications, services, and privileged accounts. Secret Management services also provide additional services associated with managing various credentials such as rotation of secrets. AWS Secrets Manager The secrets in the context of DVNA are the values for database configuration that DVNA requires to access the database. These secrets are the database host, port, username, password, and database name. Up to this point, I was using bash scripts to source the required values and export them as environment variables. For the deployment on AWS with ECS, I added these values as environment variables while creating the task definition for DVNA deployment. But these values were in plaintext. To make use of secrets management, I decided to use Amazon's Secrets Manager, a secrets management service that comes along with AWS services. To utilize Secrets Manager and import secrets through it for the container's database configuration I needed the SecretsManagerReadWrite policy to be attached to my IAM role. After the policy was attached, I followed the following steps: The first thing I had to do was to create the secrets themselves. I used the AWS CLI to create the secrets with the names /db/dvna/username , /db/dvna/password , /db/dvna/host , /db/dvna/port and /db/dvna/database , provided a description and the secret values. I also took note of the Secret ARN received in the response for each secret created. The command I used is mentioned below: aws secretsmanager create-secret --name <SECRET NAME> --secret-string <SECRET VALUE> --description <DESCRIPTION> Note : Using the console to create the secrets was a bit confusing initially. Selecting 'Other type of secrets' option prompted for either a key-value or a plaintext type secret. The field for entering a plaintext secret had a JSON-like template structure present which turned out to be unnecessary. Removing the template JSON with just the required value worked as well to create secrets from the web console. Here's the AWS Secrets Manager web console after I added all the secrets: Below is an image depicting where to retrieve the Secret's ARN on the web console: Now, instead of creating a revision to the existing task definition, I created a new task definition following the steps I used earlier (skipping the first nine steps as they did not need to be redone) with the new task definition's name as deployDvnaSecretsManager . This time, however, instead of directly passing the values for the database configurations, I chose the ValueFrom option and passed the respective Secret ARN that I took note of in the first step as the value for each secret under the environment variables section while providing the container details. Next, I created a new service ( dvnaSSMDeployService ) under the previously created cluster named deploymentCluster and created a task from the task definition created in the previous step. This launched DVNA on a new ECS instance with the secrets retrieved from the Secrets Manager. I did not have to change anything in the pipeline or the script I wrote earlier to manage deployments through the pipeline as the script was programmed to stop all running tasks under the designated cluster and then display all new tasks that were spun by the services to fetch the public IPs of the DVNA instances deployed. HashiCorp Vault HashiCorp Vault is another secrets manager which is more generic in terms of a solution as it addresses the general problem of managing secrets and is not specific to a particular cloud vendor. It also more configurable than AWS Secrets Manager as it comes along with various secrets engines built-in that can be used as per the project requirements. Configuring HashiCorp Vault Since Vault is an application itself, it was needed to be installed to be used as a Secrets Manager. I used the Jenkins Agent EC2 instance to install and configure Vault with the steps mentioned below: I followed along with this article exactly as it also explained how to set up Consul which is a storage backend, also built by HashiCorp, which can be used in conjunction with Vault to store secrets. The tutorial also helped set up both Vault and Consul as system services which I felt was a better way to use them than explicitly running them manually. After completing all the steps mentioned in the article above, I took note of the root initialization token used to initialize Vault and the 5 keys required to unseal Vault to retrieve secrets. I added all the 5 keys and the root initialization token as Secret Text type credentials in Jenkins to use them in the pipeline as and when required. Integrating Vault with Pipeline After Vault was configured and was ready to be used, I went through the steps, mentioned below, to add the secrets and figure out how to retrieve them so I could use them to launch a new ECS task on AWS with the required parameters: I unsealed Vault with 3 (out of the 5) keys that were given during the installation of Vault with the command - vault operator unseal <KEY VALUE> . After Vault was unsealed, I used vault kv put dvna/db <NAME>=<VALUE> to add secrets to vault. Here, dvna/db path was chosen by me to represent the relevance of the secrets being stored in that particular path. Note : Initially put command was not working and I received an error saying - preflight capability check returned 403, please ensure client's policies grant access to path \"kv/dvna/\" . It turned out to be because of the path dvna/db not being initialized with the kv secrets engine. To solve this I used the command vault secrets enable -path=dvna kv according to the official documentation . To retrieve secrets I used the command - vault kv get dvna/db which printed all the secrets in that path (Database name, host, port, username, and password) in a tabular format. Now, I had to fetch just the secret out of the output, instead of formatting the output to suit my needs, Vault came with a utility to only print the value of the secret by specifying the name of the field I need. The command I used to retrieve the secrets' values was vault kv get -field=<SECRET NAME> dvna/db where SECRET NAME was one the values from - database, username, password, port, and host. Now, for the secrets to be retrieved in the pipeline, the vault needs to be unsealed first and then it should also be resealed after the secrets are fetched. To achieve this, firstly, I made use of the withCredentials section in the pipeline syntax to fetch secrets from Jenkins' credential manager to retrieve Vault Keys that we stored there earlier and used them with the unseal command in the pipeline stage. I then amended the script I used to stop the currently running ECS tasks and fetch new deployed ECS instances running the latest container images of DVNA and lastly, I sealed the vault again. Unsealing (and resealing) Vault with keys from Jenkins Secret Manager in the pipeline script: stage ('Build and Deploy DVNA') { agent { label 'jenkins-agent-ec2' } steps { withCredentials([string(credentialsId: 'vault-key-1', variable: 'key1'), string(credentialsId: 'vault-key-2', variable: 'key2'), string(credentialsId: 'vault-key-3', variable: 'key3')]) { sh ''' export VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal $key1 vault operator unseal $key2 vault operator unseal $key3 /home/ubuntu/task-manager.sh vault operator seal ''' } sh 'rm -rf ./*' sh 'docker rmi $(docker images | grep none | awk \\'{print $3}\\')' } } Amendments made to the previous deployment script, in the previous section , to launch ECS tasks with the updated DVNA docker image: #!/bin/bash # Cloning the project repository to build the image git clone https://github.com/ayushpriya10/dvna.git cd dvna # Login to the ECR Registry to push docker images $(aws ecr get-login --no-include-email --region us-east-2) # Building the docker image, tagging it as 'latest' and pushing it to the registry docker build -t dvna_app . docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest # Fetching all active tasks running under the 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Stopping all tasks which are running the older docker image of DVNA for task in $task_arns do echo \"Stopping Task: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` aws ecs stop-task --cluster deploymentCluster --task $task_id > /dev/null done # Retrieving secrets from Vault MYSQL_DATABASE=$(vault kv get -field=database dvna/db) MYSQL_HOST=$(vault kv get -field=host dvna/db) MYSQL_PORT=$(vault kv get -field=port dvna/db) MYSQL_USER=$(vault kv get -field=username dvna/db) MYSQL_PASSWORD=$(vault kv get -field=password dvna/db) # Creating JSON with container parameters to override for ECS Task OVERRIDES=\"{ \\\"containerOverrides\\\": [{ \\\"name\\\": \\\"dvna-latest-ssm\\\", \\\"environment\\\": [{ \\\"name\\\": \\\"MYSQL_DATABASE\\\", \\\"value\\\": \\\"$MYSQL_DATABASE\\\" }, { \\\"name\\\": \\\"MYSQL_HOST\\\", \\\"value\\\": \\\"$MYSQL_HOST\\\" }, { \\\"name\\\": \\\"MYSQL_PORT\\\", \\\"value\\\": \\\"$MYSQL_PORT\\\" }, { \\\"name\\\": \\\"MYSQL_PASSWORD\\\", \\\"value\\\": \\\"$MYSQL_PASSWORD\\\" }, { \\\"name\\\": \\\"MYSQL_USER\\\", \\\"value\\\": \\\"$MYSQL_USER\\\" }] }] }\" # Creating JSON with Network Configuration for the ECS Task NETWORK_CONFIG='{ \"awsvpcConfiguration\": { \"subnets\": [\"subnet-fb41fdb7\"], \"securityGroups\": [\"sg-0b3be5dc4044f6f52\"], \"assignPublicIp\": \"ENABLED\" } }' # Launching a new ECS Task, independent of existing services in the cluster, to run a Task with overriden parameters aws ecs run-task --overrides \"$OVERRIDES\" --cluster deploymentCluster --task-definition deployDvnaSecretsManager:6 --count 1 --launch-type FARGATE --network-configuration \"$NETWORK_CONFIG\" > /dev/null # Waiting for 'dvnaDeployService' to automatically run a new task echo \"Waiting for 1 minute for AWS to bring up new ECS Tasks...\" sleep 1m # Fetching all active tasks under 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Printing the URL where DVNA instance(s) were deployed for task in $task_arns do echo \"New Task ARN: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` task_attachments=`aws ecs describe-tasks --cluster deploymentCluster --tasks $task_id | jq '.tasks[0].attachments[0].details' | jq -c '.[]'` for attachment in $task_attachments do name=`echo $attachment | jq '.name'` if [ \"$name\" == \"\\\"networkInterfaceId\\\"\" ]; then interface_id=`echo $attachment | jq '.value'` interface_id=`echo $interface_id | tr -d \"\\\"\"` fi done public_ip=`aws ec2 describe-network-interfaces --network-interface-ids $interface_id | jq '.NetworkInterfaces[0].Association.PublicIp' | tr -d \"\\\"\"` echo \"DVNA is deployed at: http://$public_ip:9090\" done Comparing AWS Secrets Manager and HashiCorp Vault The tools I used for Secrets Management are a bit difficult to compare, unlike the other comparative observations I wrote as part of this report previously as both of these tools solve the same problem but have different use-cases. AWS Secrets Manager required minimal effort in terms of storing secrets, unlike HashiCorp Vault which required extensive setup and configuration. It also had the added ease of fetching secrets from it directly by using the ARN for the required in the container details while defining a new task. The biggest downside for AWS Secrets Manager was that it was an AWS-specific solution and that is exactly one of the strongest benefits of HashiCorp's Vault that it is independent of platform or cloud providers. Vault also is more customizable as compared to AWS Secret Manager. Some of these customizable features are - allowing the user to decide what path to store secrets on, what secrets engine to use to encrypt the secrets and which set of keys to use to unseal Vault. In the context of the problem statement that I had, AWS Secrets Manager is a better solution to use as I was specifically supposed to use AWS as the cloud provider and hence, it makes sense to use the tool/system which requires the least effort to operate. However, if a more general solution was needed or a scenario where the secrets are to be shared with services that do not integrate with AWS Secrets Manager, HashiCorp Vault would be the optimal choice because of its portability in terms on integration with external systems.","title":"Secrets Management"},{"location":"secrets_management/#secrets-management","text":"","title":"Secrets Management"},{"location":"secrets_management/#objective","text":"The aim of this section is to set up a secrets management service on the cloud to segregate the app and the associated secrets required for it to function and provide a solution to the 2nd point of the problem statement under Task 4 .","title":"Objective"},{"location":"secrets_management/#secrets-management_1","text":"Secrets management refers to the tools and methods for managing digital authentication credentials (secrets), including passwords, keys, APIs, and tokens for use in applications, services, and privileged accounts. Secret Management services also provide additional services associated with managing various credentials such as rotation of secrets.","title":"Secrets Management"},{"location":"secrets_management/#aws-secrets-manager","text":"The secrets in the context of DVNA are the values for database configuration that DVNA requires to access the database. These secrets are the database host, port, username, password, and database name. Up to this point, I was using bash scripts to source the required values and export them as environment variables. For the deployment on AWS with ECS, I added these values as environment variables while creating the task definition for DVNA deployment. But these values were in plaintext. To make use of secrets management, I decided to use Amazon's Secrets Manager, a secrets management service that comes along with AWS services. To utilize Secrets Manager and import secrets through it for the container's database configuration I needed the SecretsManagerReadWrite policy to be attached to my IAM role. After the policy was attached, I followed the following steps: The first thing I had to do was to create the secrets themselves. I used the AWS CLI to create the secrets with the names /db/dvna/username , /db/dvna/password , /db/dvna/host , /db/dvna/port and /db/dvna/database , provided a description and the secret values. I also took note of the Secret ARN received in the response for each secret created. The command I used is mentioned below: aws secretsmanager create-secret --name <SECRET NAME> --secret-string <SECRET VALUE> --description <DESCRIPTION> Note : Using the console to create the secrets was a bit confusing initially. Selecting 'Other type of secrets' option prompted for either a key-value or a plaintext type secret. The field for entering a plaintext secret had a JSON-like template structure present which turned out to be unnecessary. Removing the template JSON with just the required value worked as well to create secrets from the web console. Here's the AWS Secrets Manager web console after I added all the secrets: Below is an image depicting where to retrieve the Secret's ARN on the web console: Now, instead of creating a revision to the existing task definition, I created a new task definition following the steps I used earlier (skipping the first nine steps as they did not need to be redone) with the new task definition's name as deployDvnaSecretsManager . This time, however, instead of directly passing the values for the database configurations, I chose the ValueFrom option and passed the respective Secret ARN that I took note of in the first step as the value for each secret under the environment variables section while providing the container details. Next, I created a new service ( dvnaSSMDeployService ) under the previously created cluster named deploymentCluster and created a task from the task definition created in the previous step. This launched DVNA on a new ECS instance with the secrets retrieved from the Secrets Manager. I did not have to change anything in the pipeline or the script I wrote earlier to manage deployments through the pipeline as the script was programmed to stop all running tasks under the designated cluster and then display all new tasks that were spun by the services to fetch the public IPs of the DVNA instances deployed.","title":"AWS Secrets Manager"},{"location":"secrets_management/#hashicorp-vault","text":"HashiCorp Vault is another secrets manager which is more generic in terms of a solution as it addresses the general problem of managing secrets and is not specific to a particular cloud vendor. It also more configurable than AWS Secrets Manager as it comes along with various secrets engines built-in that can be used as per the project requirements.","title":"HashiCorp Vault"},{"location":"secrets_management/#configuring-hashicorp-vault","text":"Since Vault is an application itself, it was needed to be installed to be used as a Secrets Manager. I used the Jenkins Agent EC2 instance to install and configure Vault with the steps mentioned below: I followed along with this article exactly as it also explained how to set up Consul which is a storage backend, also built by HashiCorp, which can be used in conjunction with Vault to store secrets. The tutorial also helped set up both Vault and Consul as system services which I felt was a better way to use them than explicitly running them manually. After completing all the steps mentioned in the article above, I took note of the root initialization token used to initialize Vault and the 5 keys required to unseal Vault to retrieve secrets. I added all the 5 keys and the root initialization token as Secret Text type credentials in Jenkins to use them in the pipeline as and when required.","title":"Configuring HashiCorp Vault"},{"location":"secrets_management/#integrating-vault-with-pipeline","text":"After Vault was configured and was ready to be used, I went through the steps, mentioned below, to add the secrets and figure out how to retrieve them so I could use them to launch a new ECS task on AWS with the required parameters: I unsealed Vault with 3 (out of the 5) keys that were given during the installation of Vault with the command - vault operator unseal <KEY VALUE> . After Vault was unsealed, I used vault kv put dvna/db <NAME>=<VALUE> to add secrets to vault. Here, dvna/db path was chosen by me to represent the relevance of the secrets being stored in that particular path. Note : Initially put command was not working and I received an error saying - preflight capability check returned 403, please ensure client's policies grant access to path \"kv/dvna/\" . It turned out to be because of the path dvna/db not being initialized with the kv secrets engine. To solve this I used the command vault secrets enable -path=dvna kv according to the official documentation . To retrieve secrets I used the command - vault kv get dvna/db which printed all the secrets in that path (Database name, host, port, username, and password) in a tabular format. Now, I had to fetch just the secret out of the output, instead of formatting the output to suit my needs, Vault came with a utility to only print the value of the secret by specifying the name of the field I need. The command I used to retrieve the secrets' values was vault kv get -field=<SECRET NAME> dvna/db where SECRET NAME was one the values from - database, username, password, port, and host. Now, for the secrets to be retrieved in the pipeline, the vault needs to be unsealed first and then it should also be resealed after the secrets are fetched. To achieve this, firstly, I made use of the withCredentials section in the pipeline syntax to fetch secrets from Jenkins' credential manager to retrieve Vault Keys that we stored there earlier and used them with the unseal command in the pipeline stage. I then amended the script I used to stop the currently running ECS tasks and fetch new deployed ECS instances running the latest container images of DVNA and lastly, I sealed the vault again. Unsealing (and resealing) Vault with keys from Jenkins Secret Manager in the pipeline script: stage ('Build and Deploy DVNA') { agent { label 'jenkins-agent-ec2' } steps { withCredentials([string(credentialsId: 'vault-key-1', variable: 'key1'), string(credentialsId: 'vault-key-2', variable: 'key2'), string(credentialsId: 'vault-key-3', variable: 'key3')]) { sh ''' export VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal $key1 vault operator unseal $key2 vault operator unseal $key3 /home/ubuntu/task-manager.sh vault operator seal ''' } sh 'rm -rf ./*' sh 'docker rmi $(docker images | grep none | awk \\'{print $3}\\')' } } Amendments made to the previous deployment script, in the previous section , to launch ECS tasks with the updated DVNA docker image: #!/bin/bash # Cloning the project repository to build the image git clone https://github.com/ayushpriya10/dvna.git cd dvna # Login to the ECR Registry to push docker images $(aws ecr get-login --no-include-email --region us-east-2) # Building the docker image, tagging it as 'latest' and pushing it to the registry docker build -t dvna_app . docker tag dvna_app:latest <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest docker push <ID>.dkr.ecr.us-east-2.amazonaws.com/dvna-aws-registry:latest # Fetching all active tasks running under the 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Stopping all tasks which are running the older docker image of DVNA for task in $task_arns do echo \"Stopping Task: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` aws ecs stop-task --cluster deploymentCluster --task $task_id > /dev/null done # Retrieving secrets from Vault MYSQL_DATABASE=$(vault kv get -field=database dvna/db) MYSQL_HOST=$(vault kv get -field=host dvna/db) MYSQL_PORT=$(vault kv get -field=port dvna/db) MYSQL_USER=$(vault kv get -field=username dvna/db) MYSQL_PASSWORD=$(vault kv get -field=password dvna/db) # Creating JSON with container parameters to override for ECS Task OVERRIDES=\"{ \\\"containerOverrides\\\": [{ \\\"name\\\": \\\"dvna-latest-ssm\\\", \\\"environment\\\": [{ \\\"name\\\": \\\"MYSQL_DATABASE\\\", \\\"value\\\": \\\"$MYSQL_DATABASE\\\" }, { \\\"name\\\": \\\"MYSQL_HOST\\\", \\\"value\\\": \\\"$MYSQL_HOST\\\" }, { \\\"name\\\": \\\"MYSQL_PORT\\\", \\\"value\\\": \\\"$MYSQL_PORT\\\" }, { \\\"name\\\": \\\"MYSQL_PASSWORD\\\", \\\"value\\\": \\\"$MYSQL_PASSWORD\\\" }, { \\\"name\\\": \\\"MYSQL_USER\\\", \\\"value\\\": \\\"$MYSQL_USER\\\" }] }] }\" # Creating JSON with Network Configuration for the ECS Task NETWORK_CONFIG='{ \"awsvpcConfiguration\": { \"subnets\": [\"subnet-fb41fdb7\"], \"securityGroups\": [\"sg-0b3be5dc4044f6f52\"], \"assignPublicIp\": \"ENABLED\" } }' # Launching a new ECS Task, independent of existing services in the cluster, to run a Task with overriden parameters aws ecs run-task --overrides \"$OVERRIDES\" --cluster deploymentCluster --task-definition deployDvnaSecretsManager:6 --count 1 --launch-type FARGATE --network-configuration \"$NETWORK_CONFIG\" > /dev/null # Waiting for 'dvnaDeployService' to automatically run a new task echo \"Waiting for 1 minute for AWS to bring up new ECS Tasks...\" sleep 1m # Fetching all active tasks under 'deploymentCluster' task_arns=`aws ecs list-tasks --cluster deploymentCluster | jq '.taskArns' | jq -c '.[]'` # Printing the URL where DVNA instance(s) were deployed for task in $task_arns do echo \"New Task ARN: $task\" task_id=`echo $task | cut -d '/' -f 2 | cut -d '\"' -f 1` task_attachments=`aws ecs describe-tasks --cluster deploymentCluster --tasks $task_id | jq '.tasks[0].attachments[0].details' | jq -c '.[]'` for attachment in $task_attachments do name=`echo $attachment | jq '.name'` if [ \"$name\" == \"\\\"networkInterfaceId\\\"\" ]; then interface_id=`echo $attachment | jq '.value'` interface_id=`echo $interface_id | tr -d \"\\\"\"` fi done public_ip=`aws ec2 describe-network-interfaces --network-interface-ids $interface_id | jq '.NetworkInterfaces[0].Association.PublicIp' | tr -d \"\\\"\"` echo \"DVNA is deployed at: http://$public_ip:9090\" done","title":"Integrating Vault with Pipeline"},{"location":"secrets_management/#comparing-aws-secrets-manager-and-hashicorp-vault","text":"The tools I used for Secrets Management are a bit difficult to compare, unlike the other comparative observations I wrote as part of this report previously as both of these tools solve the same problem but have different use-cases. AWS Secrets Manager required minimal effort in terms of storing secrets, unlike HashiCorp Vault which required extensive setup and configuration. It also had the added ease of fetching secrets from it directly by using the ARN for the required in the container details while defining a new task. The biggest downside for AWS Secrets Manager was that it was an AWS-specific solution and that is exactly one of the strongest benefits of HashiCorp's Vault that it is independent of platform or cloud providers. Vault also is more customizable as compared to AWS Secret Manager. Some of these customizable features are - allowing the user to decide what path to store secrets on, what secrets engine to use to encrypt the secrets and which set of keys to use to unseal Vault. In the context of the problem statement that I had, AWS Secrets Manager is a better solution to use as I was specifically supposed to use AWS as the cloud provider and hence, it makes sense to use the tool/system which requires the least effort to operate. However, if a more general solution was needed or a scenario where the secrets are to be shared with services that do not integrate with AWS Secrets Manager, HashiCorp Vault would be the optimal choice because of its portability in terms on integration with external systems.","title":"Comparing AWS Secrets Manager and HashiCorp Vault"},{"location":"setting_up_pipeline/","text":"Setting Up Pipeline Objective The aim of this section is to set up a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement under Task 1 . Pipeline A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language. Jenkins Pipeline Project To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which leads me to a different page. I gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description of the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artifacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations. The Jenkinsfile Jenkins has a utility where the actions that are to be performed on the build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<MYSQL PASSWORD> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins. Stages I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose: Initialization This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline. Build In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependencies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets built with all of its dependencies only on the Jenkins VM. Static Analysis All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools . Deployment Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking the functioning of the instance of the application running on the production machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#setting-up-pipeline","text":"","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#objective","text":"The aim of this section is to set up a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement under Task 1 .","title":"Objective"},{"location":"setting_up_pipeline/#pipeline","text":"A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language.","title":"Pipeline"},{"location":"setting_up_pipeline/#jenkins-pipeline-project","text":"To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which leads me to a different page. I gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description of the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artifacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations.","title":"Jenkins Pipeline Project"},{"location":"setting_up_pipeline/#the-jenkinsfile","text":"Jenkins has a utility where the actions that are to be performed on the build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<MYSQL PASSWORD> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins.","title":"The Jenkinsfile"},{"location":"setting_up_pipeline/#stages","text":"I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose:","title":"Stages"},{"location":"setting_up_pipeline/#initialization","text":"This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline.","title":"Initialization"},{"location":"setting_up_pipeline/#build","text":"In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependencies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets built with all of its dependencies only on the Jenkins VM.","title":"Build"},{"location":"setting_up_pipeline/#static-analysis","text":"All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools .","title":"Static Analysis"},{"location":"setting_up_pipeline/#deployment","text":"Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking the functioning of the instance of the application running on the production machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Deployment"},{"location":"setting_up_vms/","text":"Setting up VMs Objective The aim of this section is to set up the required infrastructure to perform the task and solve the 6th point of the problem statement under Task 1 . System Configuration The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipeline. The release notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under the \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components would be installed. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete. Installing Jenkins Jenkins is a Continuos Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentation. For this documentation, I didn't skip any step. Choosing the Application The application that was to be analyzed and deployed, as required by the problem statement, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts. Configuring Production VM To serve DVNA , there were some prerequisites. The following steps conclude how to set up the prerequisites for Jenkins to be able to deploy the application through the pipeline. Setting up DVNA I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was installed on the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in the user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage. Configuring SSH Access For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , created a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh into the production VM. The credentials for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key. Additional Notes on VirtualBox While setting up the Virtual Machines on VirtualBox, I faced trouble scaling the Guest OS's window to fit the screen. The autoscaling feature did not work on it's own. The solution, which I found in this blog , was to select VBoxVGA under Machine Settings > Display > Graphics Controller , instead of VBoxSVGA (even though its the recommended one).","title":"Setting Up VMs"},{"location":"setting_up_vms/#setting-up-vms","text":"","title":"Setting up VMs"},{"location":"setting_up_vms/#objective","text":"The aim of this section is to set up the required infrastructure to perform the task and solve the 6th point of the problem statement under Task 1 .","title":"Objective"},{"location":"setting_up_vms/#system-configuration","text":"The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipeline. The release notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under the \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components would be installed. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete.","title":"System Configuration"},{"location":"setting_up_vms/#installing-jenkins","text":"Jenkins is a Continuos Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentation. For this documentation, I didn't skip any step.","title":"Installing Jenkins"},{"location":"setting_up_vms/#choosing-the-application","text":"The application that was to be analyzed and deployed, as required by the problem statement, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts.","title":"Choosing the Application"},{"location":"setting_up_vms/#configuring-production-vm","text":"To serve DVNA , there were some prerequisites. The following steps conclude how to set up the prerequisites for Jenkins to be able to deploy the application through the pipeline.","title":"Configuring Production VM"},{"location":"setting_up_vms/#setting-up-dvna","text":"I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was installed on the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in the user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage.","title":"Setting up DVNA"},{"location":"setting_up_vms/#configuring-ssh-access","text":"For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , created a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh into the production VM. The credentials for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Configuring SSH Access"},{"location":"setting_up_vms/#additional-notes-on-virtualbox","text":"While setting up the Virtual Machines on VirtualBox, I faced trouble scaling the Guest OS's window to fit the screen. The autoscaling feature did not work on it's own. The solution, which I found in this blog , was to select VBoxVGA under Machine Settings > Display > Graphics Controller , instead of VBoxSVGA (even though its the recommended one).","title":"Additional Notes on VirtualBox"},{"location":"static_analysis/","text":"Static Analysis Objective The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to the 2nd, 3rd and 4th points of the problem statement under Task 1 . SAST SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of the project's logic which might lead to a security vulnerability. DVNA's Tech Stack SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server-side language used, along with a SQL database. SAST Tools for Node.js Applications After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins: SonarQube SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilize SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at the pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article. NPM Audit NPM Audit is a built-in utility that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since NPM Audit comes along with npm itself, is not required to be installed separately. However, if one has an older version of npm on the system, the following command can be used to upgrade: sudo npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if run through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } NodeJsScan NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realized that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } Retire.js Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: sudo npm install -g retire Note : The -g flag specifies that the package needs to be installed globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } OWASP Dependency Check As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of its dependencies. OWASP Dependency Check comes as an executable for Linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinsfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology. Auditjs Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyze NodeJs applications. Like Retire.js, Auditjs is also available as an npm-package. So, I installed it with the following command: sudo npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirect STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } Snyk Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: sudo npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script, which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } SAST Analysis Reports I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file. Other Tools There were a few other tools available to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavailable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now deprecated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is a SAST tool that combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Static Analysis"},{"location":"static_analysis/#static-analysis","text":"","title":"Static Analysis"},{"location":"static_analysis/#objective","text":"The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to the 2nd, 3rd and 4th points of the problem statement under Task 1 .","title":"Objective"},{"location":"static_analysis/#sast","text":"SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of the project's logic which might lead to a security vulnerability.","title":"SAST"},{"location":"static_analysis/#dvnas-tech-stack","text":"SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server-side language used, along with a SQL database.","title":"DVNA's Tech Stack"},{"location":"static_analysis/#sast-tools-for-nodejs-applications","text":"After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins:","title":"SAST Tools for Node.js Applications"},{"location":"static_analysis/#sonarqube","text":"SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilize SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at the pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article.","title":"SonarQube"},{"location":"static_analysis/#npm-audit","text":"NPM Audit is a built-in utility that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since NPM Audit comes along with npm itself, is not required to be installed separately. However, if one has an older version of npm on the system, the following command can be used to upgrade: sudo npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if run through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } }","title":"NPM Audit"},{"location":"static_analysis/#nodejsscan","text":"NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realized that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } }","title":"NodeJsScan"},{"location":"static_analysis/#retirejs","text":"Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: sudo npm install -g retire Note : The -g flag specifies that the package needs to be installed globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } }","title":"Retire.js"},{"location":"static_analysis/#owasp-dependency-check","text":"As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of its dependencies. OWASP Dependency Check comes as an executable for Linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinsfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology.","title":"OWASP Dependency Check"},{"location":"static_analysis/#auditjs","text":"Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyze NodeJs applications. Like Retire.js, Auditjs is also available as an npm-package. So, I installed it with the following command: sudo npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirect STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } }","title":"Auditjs"},{"location":"static_analysis/#snyk","text":"Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: sudo npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script, which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } }","title":"Snyk"},{"location":"static_analysis/#sast-analysis-reports","text":"I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file.","title":"SAST Analysis Reports"},{"location":"static_analysis/#other-tools","text":"There were a few other tools available to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavailable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now deprecated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is a SAST tool that combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Other Tools"}]}