{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of contents The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Static Analysis Comparing SAST Tools Setting Up Pipeline Configuring Webhook Deploying the Report Resources This documentation can also be found online .","title":"Introduction"},{"location":"#table-of-contents","text":"The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Static Analysis Comparing SAST Tools Setting Up Pipeline Configuring Webhook Deploying the Report Resources This documentation can also be found online .","title":"Table of contents"},{"location":"comparing_sast_tools/","text":"Comparing SAST Tools Objective The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement . Vulnerability Reports The different tools found various vulnerabilities. Some found more vulnerabilities than the others. I went through all the reports generated to find relevant content found to the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found. SonarQube SonarQube states here that it utilises security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax based bugs. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here . NPM Audit According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here . NodeJsScan NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comaparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed a 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here . Retire.js Retire.js maintains a database of known vulnerabilities, which can be found listed here , in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here . OWASP Dependency Check According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project specific to the technology being used, and categorises them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards based vulnerability management data, to find maching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cybersecurity vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json from pprint import pprint file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here . Auditjs Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here . Snyk Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them gets collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below listed modules, with the type of vulnerability, number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitary Code Execution 3 High node-serialize Arbitary Code Execution 1 High typed-function Arbitary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitary Code Execution 2 Medium The full report generated by Snyk can be found here . Conclusion So, after reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as given below: Rank Tool No. of Vulnerabilities Found 1 NodeJsScan 34 Dependency-based + 5 Web-based 2 Auditjs 22 Dependency-based 3 Snyk 8 Dependency-based 4 Dependency Check 7 Dependency-based 5 NPM Audit 5 Dependency-based 6 Retire.js 3 Dependency-based 7 SonarQube 0 Dependency-based","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#comparing-sast-tools","text":"","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#objective","text":"The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement .","title":"Objective"},{"location":"comparing_sast_tools/#vulnerability-reports","text":"The different tools found various vulnerabilities. Some found more vulnerabilities than the others. I went through all the reports generated to find relevant content found to the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found.","title":"Vulnerability Reports"},{"location":"comparing_sast_tools/#sonarqube","text":"SonarQube states here that it utilises security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax based bugs. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here .","title":"SonarQube"},{"location":"comparing_sast_tools/#npm-audit","text":"According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here .","title":"NPM Audit"},{"location":"comparing_sast_tools/#nodejsscan","text":"NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comaparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed a 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here .","title":"NodeJsScan"},{"location":"comparing_sast_tools/#retirejs","text":"Retire.js maintains a database of known vulnerabilities, which can be found listed here , in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here .","title":"Retire.js"},{"location":"comparing_sast_tools/#owasp-dependency-check","text":"According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project specific to the technology being used, and categorises them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards based vulnerability management data, to find maching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cybersecurity vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json from pprint import pprint file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here .","title":"OWASP Dependency Check"},{"location":"comparing_sast_tools/#auditjs","text":"Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here .","title":"Auditjs"},{"location":"comparing_sast_tools/#snyk","text":"Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them gets collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below listed modules, with the type of vulnerability, number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitary Code Execution 3 High node-serialize Arbitary Code Execution 1 High typed-function Arbitary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitary Code Execution 2 Medium The full report generated by Snyk can be found here .","title":"Snyk"},{"location":"comparing_sast_tools/#conclusion","text":"So, after reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as given below: Rank Tool No. of Vulnerabilities Found 1 NodeJsScan 34 Dependency-based + 5 Web-based 2 Auditjs 22 Dependency-based 3 Snyk 8 Dependency-based 4 Dependency Check 7 Dependency-based 5 NPM Audit 5 Dependency-based 6 Retire.js 3 Dependency-based 7 SonarQube 0 Dependency-based","title":"Conclusion"},{"location":"configuring_webhook/","text":"Configuring Trigger with Webhook Objective The aim of this section is to create and configure a webhook to automate builds based on defined events occuring on the project repository in reference to 8th point's second section in the problem statement . Webhooks Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered on the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well. Configuring Jenkins Pipeline for Webhook For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked. Configuring GitHub for Webhook Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub doesn't send any request. I tried triggering the selected events, after unchecking the option, and it didn't send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL. Using ngrok to handle Webhook over Internet Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since, the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dyanamically generated URL which can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start a HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Configuring Webhook"},{"location":"configuring_webhook/#configuring-trigger-with-webhook","text":"","title":"Configuring Trigger with Webhook"},{"location":"configuring_webhook/#objective","text":"The aim of this section is to create and configure a webhook to automate builds based on defined events occuring on the project repository in reference to 8th point's second section in the problem statement .","title":"Objective"},{"location":"configuring_webhook/#webhooks","text":"Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered on the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well.","title":"Webhooks"},{"location":"configuring_webhook/#configuring-jenkins-pipeline-for-webhook","text":"For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked.","title":"Configuring Jenkins Pipeline for Webhook"},{"location":"configuring_webhook/#configuring-github-for-webhook","text":"Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub doesn't send any request. I tried triggering the selected events, after unchecking the option, and it didn't send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL.","title":"Configuring GitHub for Webhook"},{"location":"configuring_webhook/#using-ngrok-to-handle-webhook-over-internet","text":"Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since, the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dyanamically generated URL which can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start a HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Using ngrok to handle Webhook over Internet"},{"location":"deploying_report/","text":"Deploying Report with MkDocs Objective The aim of this section is to create a documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement . Format and Tools The report was written in Markdown as required by the problem statement. Markdown is a markup language which allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as reqiured by the problem statement, to build a static site with the report. MkDocs is a static site generator which creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file. Installing MkDocs I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as rest of the steps were not required in the context of the task/problem statement. Selecting a Theme MkDocs allows users to use various themes to customise the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section. Site Configuration To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site. Deploying Static Site To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying the Report"},{"location":"deploying_report/#deploying-report-with-mkdocs","text":"","title":"Deploying Report with MkDocs"},{"location":"deploying_report/#objective","text":"The aim of this section is to create a documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement .","title":"Objective"},{"location":"deploying_report/#format-and-tools","text":"The report was written in Markdown as required by the problem statement. Markdown is a markup language which allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as reqiured by the problem statement, to build a static site with the report. MkDocs is a static site generator which creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file.","title":"Format and Tools"},{"location":"deploying_report/#installing-mkdocs","text":"I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as rest of the steps were not required in the context of the task/problem statement.","title":"Installing MkDocs"},{"location":"deploying_report/#selecting-a-theme","text":"MkDocs allows users to use various themes to customise the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section.","title":"Selecting a Theme"},{"location":"deploying_report/#site-configuration","text":"To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site.","title":"Site Configuration"},{"location":"deploying_report/#deploying-static-site","text":"To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying Static Site"},{"location":"glossary/","text":"Glossary There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"glossary/#glossary","text":"There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"problem_statement/","text":"Problem Statement Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next the DVNA should get deployed in a server. To do all of the above just consider 2 virtual machines running in your laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparitive report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Problem Statement"},{"location":"problem_statement/#problem-statement","text":"Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next the DVNA should get deployed in a server. To do all of the above just consider 2 virtual machines running in your laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparitive report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Problem Statement"},{"location":"resources/","text":"References These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"Resources"},{"location":"resources/#references","text":"These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"References"},{"location":"setting_up_pipeline/","text":"Setting Up Pipeline Objective The aim of this section is to setup a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement . Pipeline A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language. Jenkins Pipeline Project To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which lead me to a different page. I gave gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst the all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description about the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artefacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations. The Jenkinsfile Jenkins has a utility where the actions that are to be performed on build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=ayushpriya10 export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins. Stages I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose: Initialization This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline. Build In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependecies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets build with all of its dependencies only on the Jenkins VM. Static Analysis All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools . Deployment Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the new updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking any installation of dependencies existing on the machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#setting-up-pipeline","text":"","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#objective","text":"The aim of this section is to setup a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement .","title":"Objective"},{"location":"setting_up_pipeline/#pipeline","text":"A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language.","title":"Pipeline"},{"location":"setting_up_pipeline/#jenkins-pipeline-project","text":"To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which lead me to a different page. I gave gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst the all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description about the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artefacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations.","title":"Jenkins Pipeline Project"},{"location":"setting_up_pipeline/#the-jenkinsfile","text":"Jenkins has a utility where the actions that are to be performed on build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=ayushpriya10 export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins.","title":"The Jenkinsfile"},{"location":"setting_up_pipeline/#stages","text":"I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose:","title":"Stages"},{"location":"setting_up_pipeline/#initialization","text":"This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline.","title":"Initialization"},{"location":"setting_up_pipeline/#build","text":"In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependecies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets build with all of its dependencies only on the Jenkins VM.","title":"Build"},{"location":"setting_up_pipeline/#static-analysis","text":"All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools .","title":"Static Analysis"},{"location":"setting_up_pipeline/#deployment","text":"Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the new updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking any installation of dependencies existing on the machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Deployment"},{"location":"setting_up_vms/","text":"Setting up VMs Objective The aim of this section is to setup the required infrastructure to perform the task and solve the 6th point of the problem statement . System Configuration The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipline. The realease notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete. Installing Jenkins Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentations. For this documentation, I didn't skip any step. Choosing the Application The application that was to be analysed and deployed, as required by the problem statment, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts. Configuring Production VM To serve DVNA , there were some prerequisites. The following steps conclude how to setup the prerequisites for Jenkins to be able to deploy the application through the pipeline. Setting up DVNA I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was insalled the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage. Configuring SSH Access For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , creates a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh in to the production VM. The credentails for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Setting Up VMs"},{"location":"setting_up_vms/#setting-up-vms","text":"","title":"Setting up VMs"},{"location":"setting_up_vms/#objective","text":"The aim of this section is to setup the required infrastructure to perform the task and solve the 6th point of the problem statement .","title":"Objective"},{"location":"setting_up_vms/#system-configuration","text":"The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipline. The realease notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete.","title":"System Configuration"},{"location":"setting_up_vms/#installing-jenkins","text":"Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentations. For this documentation, I didn't skip any step.","title":"Installing Jenkins"},{"location":"setting_up_vms/#choosing-the-application","text":"The application that was to be analysed and deployed, as required by the problem statment, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts.","title":"Choosing the Application"},{"location":"setting_up_vms/#configuring-production-vm","text":"To serve DVNA , there were some prerequisites. The following steps conclude how to setup the prerequisites for Jenkins to be able to deploy the application through the pipeline.","title":"Configuring Production VM"},{"location":"setting_up_vms/#setting-up-dvna","text":"I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was insalled the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage.","title":"Setting up DVNA"},{"location":"setting_up_vms/#configuring-ssh-access","text":"For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , creates a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh in to the production VM. The credentails for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Configuring SSH Access"},{"location":"static_analysis/","text":"Static Analysis Objective The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to 2nd, 3rd and 4th points of the problem statement . SAST SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of project's logic which might lead to a security vulnerability. DVNA's Tech Stack SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server side language used, along with a SQL database. SAST Tools for Node.js Applications After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins: SonarQube SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilise SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article. NPM Audit NPM Audit is a built-in utlitiy that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since, NPM Audit comes along with npm itself, is not required to be installed seprately. However, if one has an older version of npm on the system, the following command can be used to upgrade: npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if ran through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } NodeJsScan NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realised that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } Retire.js Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: npm install -g retire Note : The -g flag specifies that the package needs to be insalled globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } OWASP Dependency Check As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of it's dependencies. OWASP Dependency Check comes as an executable for linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology. Auditjs Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyse NodeJs applications. Like Retire.js, Auditjs is also available as a npm-package. So, I installed it with the following command: npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirct STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } Snyk Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script,which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } SAST Analysis Reports I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file. Other Tools There were a few other tools avaible to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavilable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now depracated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is an SAST tool which combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Static Analysis"},{"location":"static_analysis/#static-analysis","text":"","title":"Static Analysis"},{"location":"static_analysis/#objective","text":"The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to 2nd, 3rd and 4th points of the problem statement .","title":"Objective"},{"location":"static_analysis/#sast","text":"SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of project's logic which might lead to a security vulnerability.","title":"SAST"},{"location":"static_analysis/#dvnas-tech-stack","text":"SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server side language used, along with a SQL database.","title":"DVNA's Tech Stack"},{"location":"static_analysis/#sast-tools-for-nodejs-applications","text":"After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins:","title":"SAST Tools for Node.js Applications"},{"location":"static_analysis/#sonarqube","text":"SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilise SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article.","title":"SonarQube"},{"location":"static_analysis/#npm-audit","text":"NPM Audit is a built-in utlitiy that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since, NPM Audit comes along with npm itself, is not required to be installed seprately. However, if one has an older version of npm on the system, the following command can be used to upgrade: npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if ran through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } }","title":"NPM Audit"},{"location":"static_analysis/#nodejsscan","text":"NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realised that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } }","title":"NodeJsScan"},{"location":"static_analysis/#retirejs","text":"Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: npm install -g retire Note : The -g flag specifies that the package needs to be insalled globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } }","title":"Retire.js"},{"location":"static_analysis/#owasp-dependency-check","text":"As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of it's dependencies. OWASP Dependency Check comes as an executable for linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology.","title":"OWASP Dependency Check"},{"location":"static_analysis/#auditjs","text":"Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyse NodeJs applications. Like Retire.js, Auditjs is also available as a npm-package. So, I installed it with the following command: npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirct STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } }","title":"Auditjs"},{"location":"static_analysis/#snyk","text":"Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script,which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } }","title":"Snyk"},{"location":"static_analysis/#sast-analysis-reports","text":"I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file.","title":"SAST Analysis Reports"},{"location":"static_analysis/#other-tools","text":"There were a few other tools avaible to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavilable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now depracated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is an SAST tool which combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Other Tools"}]}