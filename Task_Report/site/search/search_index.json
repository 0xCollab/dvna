{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of contents The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Setting Up Pipeline Static Analysis Comparing SAST Tools Configuring Webhook Deploying the Report Dynamic Analysis Code Quality Analysis Generating Software Bill of Materials Resources","title":"Introduction"},{"location":"#table-of-contents","text":"The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Setting Up Pipeline Static Analysis Comparing SAST Tools Configuring Webhook Deploying the Report Dynamic Analysis Code Quality Analysis Generating Software Bill of Materials Resources","title":"Table of contents"},{"location":"code_quality_analysis/","text":"Source Code Quality Analysis Objective The aim of this section is to perform linting checks on the source code of DVNA and generate a code quality report to provide a solution to the 1st point of the problem statement under Task 3 . Source Code Linting Linting is a process in which a tool analyzes the source code of an application to identify the presence of programmatic and stylistic errors. Running a Linter (a tool which can perform linting analysis) can help a developer to adhere to standard coding conventions and best practices. Linting tools are language-specific and thus, the tool that can be used depends on the application being tested. These tools can also be used to generate reports about the code quality by either invoking a built-in functionality or writing a reporting wrapper around the tool to aid the ease of resolving identified issues. Linting tools for DVNA DVNA is a Nodejs application and hence, I used jshint as the linter. I primarily chose jshint as it is available as a command-line utility and hence, I could easily integrate it into my CI pipeline with Jenkins. I used the official documentation for using jshint that is available here . Along with jshint , I also found eslint and used it as well, again, by following the official documentation for the CLI interface for eslint which can be found here . Integrating Jshint with Jenkins Pipeline To start off, I installed Jshint with NPM with the following command: npm install -g jshint To try out jshint , I ran the a scan with the command below, as mentioned by the documentation: jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js Note : I wrote this one-liner: $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) to exclude the node_modules/ directory and also exclude any files which do not have a .js or .ejs extension. Since Jshint gave a non-zero status code, when it found issues, I had to write a bash script to run the scan in a sub-shell and prevent the build from failing. The contents of the script, jshint-script.sh , are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js > /{JENKINS HOME DIRECTORY}/reports/jshint-report echo $? > /dev/null Lastly, I added a stage in the pipeline to execute the script after making it executable with chmod +x . The stage structure I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } } Integrating Eslint with Jenkins Pipeline To install Eslint , I used NPM again to run the following command, following the official documentation : npm install -g eslint Now, Eslint requires the project being scanned to have a .eslintrc file which specifies what Eslint should expect in the project and a few other configurations. It can be made by running eslint --init in the project's root folder. When I ran this, it generated a file, .eslintrc.json . I took note that I'll need to place this file every time I'll run a scan on the project with Eslint so, it doesn't prompt for running initialization each time. The contents of .eslintrc.json are: { \"env\": { \"es6\": true, \"node\": true }, \"extends\": \"eslint:recommended\", \"globals\": { \"Atomics\": \"readonly\", \"SharedArrayBuffer\": \"readonly\" }, \"parserOptions\": { \"ecmaVersion\": 2018, \"sourceType\": \"module\" }, \"rules\": { } } After initializing Eslint, I ran the scan on DVNA with the following command, to scan all files within the current folder and its sub-folder with .ejs and .js extensions (because under the /views directory there were .ejs files) and lastly, write the report to a file in JSON format: eslint --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ Like Jshint, Eslint also gave a non-zero status code if it identified issues with linting. So, I wrapped the required command in a bash script, eslint-script.sh whose contents are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline eslint --no-color --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ echo $? > /dev/null Note : I added the --no-color flag to avoid color formatting (for Linux terminals) as otherwise, it would have appended additional syntactical text to provide formatting which made the report difficult to read. Finally, I added a stage in the pipeline to run the script after I made it executable with chmod +x . The stage that I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } } Code Quality Report This section contains a brief about the quality report that the tools used above generated based on the default linting rules they had. Jshint Jshint found a total of 247 linting issues with DVNA. The report mostly comprised of errors stating a 'Missing Semicolon' and a few other errors that included 'Missing Identifier', 'Expected an Assignment', etc. Some of the bugs were logical issues within the codebase of DVNA but the vast majority referred to stylistic issues. The complete report generated by Jshint can be found here . Eslint Eslint proved to be a better linting tool than Jshint as it found a total of 549 issues with DVNA. It identified a wider range of issues within DVNA's codebase. Along with stylistic issues, it also found logical errors such as 'Undefined Elements', 'Unused Variables', 'Empty Blocks', 'Using Prototype Builtins', 'Redeclaration of Elements', etc. The complete report generated by Eslint can be found here .","title":"Code Quality Analysis"},{"location":"code_quality_analysis/#source-code-quality-analysis","text":"","title":"Source Code Quality Analysis"},{"location":"code_quality_analysis/#objective","text":"The aim of this section is to perform linting checks on the source code of DVNA and generate a code quality report to provide a solution to the 1st point of the problem statement under Task 3 .","title":"Objective"},{"location":"code_quality_analysis/#source-code-linting","text":"Linting is a process in which a tool analyzes the source code of an application to identify the presence of programmatic and stylistic errors. Running a Linter (a tool which can perform linting analysis) can help a developer to adhere to standard coding conventions and best practices. Linting tools are language-specific and thus, the tool that can be used depends on the application being tested. These tools can also be used to generate reports about the code quality by either invoking a built-in functionality or writing a reporting wrapper around the tool to aid the ease of resolving identified issues.","title":"Source Code Linting"},{"location":"code_quality_analysis/#linting-tools-for-dvna","text":"DVNA is a Nodejs application and hence, I used jshint as the linter. I primarily chose jshint as it is available as a command-line utility and hence, I could easily integrate it into my CI pipeline with Jenkins. I used the official documentation for using jshint that is available here . Along with jshint , I also found eslint and used it as well, again, by following the official documentation for the CLI interface for eslint which can be found here .","title":"Linting tools for DVNA"},{"location":"code_quality_analysis/#integrating-jshint-with-jenkins-pipeline","text":"To start off, I installed Jshint with NPM with the following command: npm install -g jshint To try out jshint , I ran the a scan with the command below, as mentioned by the documentation: jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js Note : I wrote this one-liner: $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) to exclude the node_modules/ directory and also exclude any files which do not have a .js or .ejs extension. Since Jshint gave a non-zero status code, when it found issues, I had to write a bash script to run the scan in a sub-shell and prevent the build from failing. The contents of the script, jshint-script.sh , are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline jshint $(ls ./**/*.js ./**/*.ejs | grep -v node | grep js) *.js > /{JENKINS HOME DIRECTORY}/reports/jshint-report echo $? > /dev/null Lastly, I added a stage in the pipeline to execute the script after making it executable with chmod +x . The stage structure I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/jshint-script.sh' } }","title":"Integrating Jshint with Jenkins Pipeline"},{"location":"code_quality_analysis/#integrating-eslint-with-jenkins-pipeline","text":"To install Eslint , I used NPM again to run the following command, following the official documentation : npm install -g eslint Now, Eslint requires the project being scanned to have a .eslintrc file which specifies what Eslint should expect in the project and a few other configurations. It can be made by running eslint --init in the project's root folder. When I ran this, it generated a file, .eslintrc.json . I took note that I'll need to place this file every time I'll run a scan on the project with Eslint so, it doesn't prompt for running initialization each time. The contents of .eslintrc.json are: { \"env\": { \"es6\": true, \"node\": true }, \"extends\": \"eslint:recommended\", \"globals\": { \"Atomics\": \"readonly\", \"SharedArrayBuffer\": \"readonly\" }, \"parserOptions\": { \"ecmaVersion\": 2018, \"sourceType\": \"module\" }, \"rules\": { } } After initializing Eslint, I ran the scan on DVNA with the following command, to scan all files within the current folder and its sub-folder with .ejs and .js extensions (because under the /views directory there were .ejs files) and lastly, write the report to a file in JSON format: eslint --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ Like Jshint, Eslint also gave a non-zero status code if it identified issues with linting. So, I wrapped the required command in a bash script, eslint-script.sh whose contents are below: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-lint-pipeline eslint --no-color --format json --ext .ejs,.js --output-file /{JENKINS HOME DIRECTORY}/reports/eslint-report ./ echo $? > /dev/null Note : I added the --no-color flag to avoid color formatting (for Linux terminals) as otherwise, it would have appended additional syntactical text to provide formatting which made the report difficult to read. Finally, I added a stage in the pipeline to run the script after I made it executable with chmod +x . The stage that I added was as follows: stage ('Lint Analysis with Jshint') { steps { sh '/{PATH TO SCRIPT}/eslint-script.sh' } }","title":"Integrating Eslint with Jenkins Pipeline"},{"location":"code_quality_analysis/#code-quality-report","text":"This section contains a brief about the quality report that the tools used above generated based on the default linting rules they had.","title":"Code Quality Report"},{"location":"code_quality_analysis/#jshint","text":"Jshint found a total of 247 linting issues with DVNA. The report mostly comprised of errors stating a 'Missing Semicolon' and a few other errors that included 'Missing Identifier', 'Expected an Assignment', etc. Some of the bugs were logical issues within the codebase of DVNA but the vast majority referred to stylistic issues. The complete report generated by Jshint can be found here .","title":"Jshint"},{"location":"code_quality_analysis/#eslint","text":"Eslint proved to be a better linting tool than Jshint as it found a total of 549 issues with DVNA. It identified a wider range of issues within DVNA's codebase. Along with stylistic issues, it also found logical errors such as 'Undefined Elements', 'Unused Variables', 'Empty Blocks', 'Using Prototype Builtins', 'Redeclaration of Elements', etc. The complete report generated by Eslint can be found here .","title":"Eslint"},{"location":"comparing_sast_tools/","text":"Comparing SAST Tools Objective The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement under Task 1 . Vulnerability Reports The different tools found various vulnerabilities. Some found more vulnerabilities than others. I went through all the reports generated to find relevant content found in the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found. SonarQube SonarQube states here that it utilises security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax-based bugs. This is probably due to the fact that SonarQube has very few security rules for JavaScript. It works better with Java. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here . NPM Audit According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here . NodeJsScan NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi-Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here . Retire.js Retire.js maintains a database of known vulnerabilities, which can be found listed here under 'Vulnerabilities' sub-heading, in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here . OWASP Dependency Check According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project-specific to the technology being used, and categorizes them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards-based vulnerability management data, to find matching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cybersecurity vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. By default, Dependency Check assigns the lowest confidence to a module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here . Auditjs Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here . Snyk Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them get collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): import json file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below-listed modules, with the type of vulnerability, the number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitrary Code Execution 3 High node-serialize Arbitrary Code Execution 1 High typed-function Arbitrary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitrary Code Execution 2 Medium The full report generated by Snyk can be found here . Conclusion So, after reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as given below: Rank Tool No. of Vulnerabilities Found 1 NodeJsScan 34 Dependency-based + 5 Web-based 2 Auditjs 22 Dependency-based 3 Snyk 8 Dependency-based 4 Dependency Check 7 Dependency-based 5 NPM Audit 5 Dependency-based 6 Retire.js 3 Dependency-based 7 SonarQube 0 Dependency-based","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#comparing-sast-tools","text":"","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#objective","text":"The aim of this section is to compare the findings of the various SAST tools used in the previous section and rank them to provide a solution to the first sub-segment of the 8th point of the problem statement under Task 1 .","title":"Objective"},{"location":"comparing_sast_tools/#vulnerability-reports","text":"The different tools found various vulnerabilities. Some found more vulnerabilities than others. I went through all the reports generated to find relevant content found in the context of potential security vulnerabilities. I also went through the different methodologies the tools used to identify vulnerable dependencies. Listed below, is a summary of all the findings that I made by going through the reports, the complete reports generated by the tools, the methodology they used for identification of vulnerabilities and a concise list of vulnerabilities found.","title":"Vulnerability Reports"},{"location":"comparing_sast_tools/#sonarqube","text":"SonarQube states here that it utilises security rules based on three major sources: CWE (Common Weakness Enumeration) , SANS Top 25 and OWASP Top 10 . Even though SonarQube claims to have these security rules implemented but it failed to identify even a single vulnerability. It, instead, found linting and syntax-based bugs. This is probably due to the fact that SonarQube has very few security rules for JavaScript. It works better with Java. SonarQube's report can only be accessed via the web-based interface that the scanner has, yet the file generated as part of the scan can be found here .","title":"SonarQube"},{"location":"comparing_sast_tools/#npm-audit","text":"According to NPM's documentation , when one runs npm audit on a project, NPM sends a description of the dependencies, comprising the project, to the default registry (a database of JavaScript packages) for a report about known vulnerabilities for those modules. Based on this report received, NPM Audit lists which dependencies have a known vulnerability. The types of dependencies that NPM Audit checks are - Direct Dependencies , devDependencies , bundledDependencies and optionalDependencies . It does not check for peerDependencies . Running the NPM Audit on DVNA, there were a total of 5 security vulnerabilities found. The modules associated with those vulnerabilities are: Module Name No. of Vulnerabilities Severity mathjs 2 Critical node-serialize 1 Critical typed-function 1 High express-fileupload 1 Low The full report generated by NPM Audit can be found here .","title":"NPM Audit"},{"location":"comparing_sast_tools/#nodejsscan","text":"NodeJsScan comes with a set of security rules defined in a file named rules.xml which contains the various kinds of tags that identify different types of vulnerability as well as rules to match vulnerabilities in the project's codebase. The rules are segregated into six segments: String Comparison : The string comparison rules look for an exact match for the string specified in the rule. Regex Comparison : The regex comparison rules match a pattern of potentially vulnerable code as specified by the regex signature in the rule. Template Comparison : The template comparison rules look for vulnerable (potentially unsanitized) variables being used in the template. Multi-Match Regex Comparison : The multi-match regex rules are a two-staged regex match where, after the first signature matches with a potentially vulnerable entry-point for remote OS command execution, NodeJsScan looks if the second signature matches with the content within the code block for vulnerable parameters. Dynamic Regex Comparison : The dynamic regex rules have a two-part regex pattern where the first half is fixed and the second half is a dynamic signature. Missing Security Code : NodeJsScan also looks for some web-based vulnerabilities for things like missing headers and information disclosure. Scanning DVNA with NodeJsScan exposed 34 dependency-based vulnerabilities: Type of Vulnerability No. of Vulnerabilities Deserialization with Remote Code Execution 8 Open Redirect 1 SQL Injection 1 Secret Hardcoded 1 Server Side Injection 1 Unescaped Variables 12 Weak Hash Used 11 Additionally, NodeJsScan found 5 web-based vulnerabilities: Type of Vulnerability Description Missing Header Strict-Transport-Security (HSTS) Missing Header Public-Key-Pin (HPKP) Missing Header X-XSS-Protection Missing Header X-Download-Options Information Disclosure X-Powered-By The full report generated by NodeJsScan can be found here .","title":"NodeJsScan"},{"location":"comparing_sast_tools/#retirejs","text":"Retire.js maintains a database of known vulnerabilities, which can be found listed here under 'Vulnerabilities' sub-heading, in a JSON format in the tool's repository. Retire.js matches the dependencies mentioned in the target project being scanned against the existing entries present in the vulnerability database maintained by Retire.js' author. The modules that get matched, are added to the report with a severity rating associated based on the type of vulnerabilities listed for that particular module. Based on the scan, Retire.js identified 3 vulnerabilities within the following vulnerable modules: Module Name Version Severity node-serialize 0.0.4 High jquery 2.1.1 Medium jquery 3.2.1 Low The full report generated by Retire.js can be found here .","title":"Retire.js"},{"location":"comparing_sast_tools/#owasp-dependency-check","text":"According to Dependency Check's author's site , Dependency Check works by collecting information (called evidence) about the project associated files by Analyzers , which are programs that catalog information from the project-specific to the technology being used, and categorizes them into vendor , product , and version . Dependency Check then queries NVD (National Vulnerability Database) , the U.S. government's repository of standards-based vulnerability management data, to find matching CPEs (Common Platform Enumeration) . When a there's a match found, related CVEs (Common Vulnerabilities and Exposures) , a list of entries where each one contains an identification number, a description, and at least one public reference for a publicly known cybersecurity vulnerability, are added to the report generated by Dependency Check. The evidence that Dependency Check identifies, gets assigned a confidence level - low, medium, high or highest. It is a measure of how confident Dependency Check is about whether or not it has identified a module correctly by collating data about the same module from various sources within the project. Based on the confidence level of the source used to identify the module, the confidence level is assigned to the report for that particular module. By default, Dependency Check assigns the lowest confidence to a module. Note : Dependency Check mentions explicitly that because of the way it works, the report might contain both false-positives and false-negatives. The report generated by Dependency Check was quite huge, hence I ended up writing a small Python script to filter the relevant information for me. I wrapped the code I used into a function to do the filtering, which can be found below: def dependency_check_report(): import json file_handler = open('dependency-check-report') json_data = json.loads(file_handler.read()) file_handler.close() dependencies = json_data['dependencies'] for dependency in dependencies: if 'vulnerabilities' in dependency: print('\\n==============================================\\n') print(dependency['fileName'] + ' : ' + dependency['vulnerabilities'][0]['severity']) Dependency Check identified 7 vulnerabilities in total. The vulnerable modules identified are: Module Name Version Severity mathjs 3.10.1 Critical node-serialize 0.0.4 Critical sequelize 4.44.3 Critical typed-function 0.10.5 High jquery-2.1.1.min.js 2.1.1 Medium jquery-3.2.1.min.js 3.2.1 Medium express-fileupload 0.4.0 Low The full report generated by Dependency Check can be found here .","title":"OWASP Dependency Check"},{"location":"comparing_sast_tools/#auditjs","text":"Auditjs uses the REST API available for OSS Index , which is a public index of known vulnerabilities found in dependencies for various tech stacks, to identify known vulnerabilities and outdated package versions. Once a match is found, the modules are added to the report along with number of associated vulnerabilities found. Running Auditjs exposed 22 security vulnerabilities in the 5 vulnerable modules identified: Module Name Version No. of Vulnerabilities NodeJs 8.10.0 14 mathjs 3.10.1 3 typed-function 0.10.5 2 sequelize 4.44.3 2 express-fileupload 0.4.0 1 The full report generated by Auditjs can be found here .","title":"Auditjs"},{"location":"comparing_sast_tools/#snyk","text":"Snyk maintains a database of known vulnerabilities sourced from various origins like other Databases ( NVD ), issues and pull requests created on GitHub and manual research into finding previously unknown vulnerabilities. When Snyk scans a project, it queries this database to find matches. The matched modules along with the type of vulnerability associated with them get collated into a report. Like Dependency Check, I wrote a small script in Python to filter relevant information from the report generated. The code can be found as a function below: def snyk_report(): import json file_handler = open('snyk-report') json_data = json.loads(file_handler.read()) file_handler.close() for vuln in json_data['vulnerabilities']: print('\\n==============================================\\n') print(\"Module/Package Name: \" + vuln['moduleName']) print('Severity: ' + vuln['severity']) print('Title: ' + vuln['title']) Snyk exposed 8 security vulnerabilities in the below-listed modules, with the type of vulnerability, the number of vulnerabilities and severity identified: Module Name Type of Vulnerability No. of Vulnerabilities Severity mathjs Arbitrary Code Execution 3 High node-serialize Arbitrary Code Execution 1 High typed-function Arbitrary Code Execution 1 High express-fileupload Denial of Service 1 High mathjs Arbitrary Code Execution 2 Medium The full report generated by Snyk can be found here .","title":"Snyk"},{"location":"comparing_sast_tools/#conclusion","text":"So, after reading through the various reports generated by the different tools and consolidating the type of vulnerabilities found, coupled with any additional information that the tool provided, I ranked the tools as given below: Rank Tool No. of Vulnerabilities Found 1 NodeJsScan 34 Dependency-based + 5 Web-based 2 Auditjs 22 Dependency-based 3 Snyk 8 Dependency-based 4 Dependency Check 7 Dependency-based 5 NPM Audit 5 Dependency-based 6 Retire.js 3 Dependency-based 7 SonarQube 0 Dependency-based","title":"Conclusion"},{"location":"configuring_webhook/","text":"Configuring Trigger with Webhook Objective The aim of this section is to create and configure a webhook to automate builds based on defined events occurring on the project repository in reference to the second sub-segment of the 8th point in the problem statement under Task 1 . Webhooks Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered by the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurrence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well. Configuring Jenkins Pipeline for Webhook For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked. Configuring GitHub for Webhook Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub does not send any request. I tried triggering the selected events, after unchecking the option, and it did not send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL. Using ngrok to handle Webhook over Internet Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dynamically generated URL that can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start an HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Configuring Webhook"},{"location":"configuring_webhook/#configuring-trigger-with-webhook","text":"","title":"Configuring Trigger with Webhook"},{"location":"configuring_webhook/#objective","text":"The aim of this section is to create and configure a webhook to automate builds based on defined events occurring on the project repository in reference to the second sub-segment of the 8th point in the problem statement under Task 1 .","title":"Objective"},{"location":"configuring_webhook/#webhooks","text":"Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered by the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurrence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well.","title":"Webhooks"},{"location":"configuring_webhook/#configuring-jenkins-pipeline-for-webhook","text":"For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked.","title":"Configuring Jenkins Pipeline for Webhook"},{"location":"configuring_webhook/#configuring-github-for-webhook","text":"Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub does not send any request. I tried triggering the selected events, after unchecking the option, and it did not send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL.","title":"Configuring GitHub for Webhook"},{"location":"configuring_webhook/#using-ngrok-to-handle-webhook-over-internet","text":"Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dynamically generated URL that can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start an HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Using ngrok to handle Webhook over Internet"},{"location":"deploying_report/","text":"Deploying Report with MkDocs Objective The aim of this section is to create documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement under Task 1 . Format and Tools The report was written in Markdown as required by the problem statement. Markdown is a markup language that allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as required by the problem statement, to build a static site with the report. MkDocs is a static site generator that creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file. Installing MkDocs I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as the rest of the steps were not required in the context of the task/problem statement. Selecting a Theme MkDocs allows users to use various themes to customize the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section. Site Configuration To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site. Deploying Static Site To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with, in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying the Report"},{"location":"deploying_report/#deploying-report-with-mkdocs","text":"","title":"Deploying Report with MkDocs"},{"location":"deploying_report/#objective","text":"The aim of this section is to create documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement under Task 1 .","title":"Objective"},{"location":"deploying_report/#format-and-tools","text":"The report was written in Markdown as required by the problem statement. Markdown is a markup language that allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as required by the problem statement, to build a static site with the report. MkDocs is a static site generator that creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file.","title":"Format and Tools"},{"location":"deploying_report/#installing-mkdocs","text":"I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as the rest of the steps were not required in the context of the task/problem statement.","title":"Installing MkDocs"},{"location":"deploying_report/#selecting-a-theme","text":"MkDocs allows users to use various themes to customize the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section.","title":"Selecting a Theme"},{"location":"deploying_report/#site-configuration","text":"To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site.","title":"Site Configuration"},{"location":"deploying_report/#deploying-static-site","text":"To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with, in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying Static Site"},{"location":"dynamic_analysis/","text":"Dynamic Analysis Objective The aim of this section is to perform DAST for DVNA with OWASP ZAP and generate a report to provide a solution to the 1st point of the problem statement under Task 2 . DAST DAST or Dynamic Application Security Testing is a black-box testing technique in which the DAST tool interacts with the application being tested in its running state to imitate an attacker. Unlike static analysis, in DAST one does not have access to the source code of the application and the tool is completely reliant on the interactivity the application provides. In dynamic analysis, tools are used to automate attacks on the application ranging from SQL Injection, Input Validation, Cross-Site Scripting, and so forth. OWASP ZAP ZAP or Zed Attack Proxy is an open-source tool used to perform dynamic application security testing designed specifically for web applications. ZAP has a desktop interface, APIs for it to be used in an automated fashion and also a CLI. It imitates an actual user where it interacts with the application to perform various attacks. ZAP comes with a plethora of options to use, for which further details can be found here . ZAP also comes as a Docker image which is more convenient to use especially if one is using the CLI interface. Docker is a tool designed to provide ease in shipping applications across platforms. It packages the application, along with all its dependencies and other required libraries into an image . Then containers (running instances of the image) can be used to run the application on any machine. It is similar to a virtual machine but it differs greatly from them based on the fact that it does not require a full-fledged operating system to run the application. Instead, it runs the application on the system's kernel itself by just bringing along the required libraries with it which could be missing on machines other than the one the application was built on. This allows Docker-based applications to be portable i.e. they can be run on any machine that can run Docker containers. It allows for various versions of the same tool/library running on a host as different containers do not care about what is happening inside another container. Further information on docker can be found in the official documentation . Configuring OWASP ZAP with Docker To use ZAP with docker, I needed to pull the image from docker hub and start off. I used this documentation from Mozilla as it had a lot of errors demonstrated along with their rectification steps for starting out with ZAP. This was missing from all the other sources I found. Note : While running ZAP scans below, I explicitly ran DVNA, without Jenkins, for it to be tested. To start off with ZAP, I pulled the docker image by running: docker pull owasp/zap2docker-stable Then I tried to run the tool with its CLI interface. The CLI threw an error which was because of an encoding inconsistency between Python2 and Python3. To rectify this issue, I had to explicitly specify the encoding to be used with the help of some environment variables. These environment variables can be seen in the command below along with the -e flag for Docker to inject these environment variables in the container: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli --help Next I ran a quick-scan to have a look at how ZAP outputs information. Since ZAP is running inside a Docker container, I could not use localhost or 127.0.0.1 as then the container would take it to be its own host. So, to overcome this issue I used the IP assigned to docker0 , the network interface created for Docker. This IP can be found with this one-liner: $(ip -f inet -o addr show docker0 | awk '{print $4}' | cut -d '/' -f 1) .Also, as ZAP also has an API that can be used to interact with it programmatically, I had to use --start-options '-config api.disablekey=true' as otherwise, ZAP tried (and failed) to connect to the proxy as the API key was not specified. Also, the -l option specifies the severity level at which ZAP should log a vulnerability to console (or to a report). The --self-contained flag tells ZAP to turn off the daemon once the scan is complete and the --spider option tells ZAP to crawl the target to find other links present on the target site. The complete command is as mentioned below: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli quick-scan --start-options '-config api.disablekey=true' --self-contained --spider -l Low http://172.17.0.1:9090 Now, I tried the active-scan option. For this I first needed to start the daemon for zap to be accessed by the CLI, run open-url to add the target URL to the configuration in ZAP-CLI (without this, active-scan option will not start a scan on the target) and then run the scan against DVNA. This step was not required previously as while running a quick-scan , the daemon is automatically started to run the scan and stopped after the scan is finished. To do run the daemon with Docker, I ran the following command (with the -u zap segment to execute things with the zap user instead of Docker's default root user and I also appended the --rm flag to delete the container, automatically, when I stop it): docker run --rm -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 --name zap -u zap -p 8090:8080 -d owasp/zap2docker-stable zap.sh -daemon -port 8080 -host 0.0.0.0 -config api.disablekey=true docker exec <CONTAINER NAME/ID> zap-cli open-url <TARGET URL> docker exec <CONTAINER NAME/ID> zap-cli active-scan <TARGET URL> Note : When I ran the scan initially, it dropped an error from Python saying that the 'Connection was Refused' while sending requests to the target through the ZAP proxy. It turned out to be an issue with the command written in the blog that I was following. It exposed the wrong port and hence rectifying the port options in the command to -p 8090:8080 and -port 8080 solved the issue and the scan worked. Another thing is that I chose 8090 for the host port as I already had Jenkins running on 8080 . Now to be able to scan DVNA from the CLI with ZAP-CLI, I required a context which basically was a configuration written in XML to define how to perform a scan. This context also had a set of credentials in it that were recorded with a ZEST script, which is a JSON-based form used to record interactions between a website and a user specially created for security tools focused on web applications. This context file along with the ZEST script would allow zap-cli to authenticate with DVNA to be able to scan the portion of the application that lies behind the login screen. I used the browser-based GUI to generate the context and the Zest script and exported them to the machine. But importing them created various complications as zap-cli was unable to identify the embedded Zest script in the context provided to it. Due to the above complications, I decided to use Zap's baseline scan instead. To start off ZAP baseline scan with the docker image, I ran the following command, where the -t flag specified the target and -l flag defined the alert/severity level, by following this documentation : docker run -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -l INFO Now, to save the report on the Jenkins machine, I needed to mount a volume with Docker. I used the -v flag (as mentioned in the docker documentation ) to mount the present working directory of the host to the /zap/wrk directory of the container and also added the -r flag to save scan output to a HTML report on the Jenkins machine: docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS Integrating ZAP with Jenkins After understanding how to use the baseline-scan and its various options, I started integrating the scan as part of DAST in the Jenkins pipeline. But before I could scan DVNA with ZAP Baseline, I needed to build the dependencies and start an instance of DVNA. To do the same, I also had to fetch code from the repository on GitHub (explicitly, because I didn't use a Jenkinsfile for this task). Note : I chose to build a new pipeline just for DAST, as combining SAST and DAST in a single pipeline would have taken too much time during each execution which felt unnecessary at this point. To start off, I added a stage to fetch the code from the GitHub repository as follows: stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } Next, I built the dependencies for DVNA, as I did while performing SAST, and started an instance of DVNA, as mentioned in the stage mentioned below: stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } Note : I kept all the required environment variables in the env.sh file which can be seen in the above stage, and used it to export those variables for DVNA to be able to connect with the database. While trying to export the variables, the shell that comes along with Jenkins kept throwing an error saying it didn't recognize the command source . This turned out to be because that by default the sh shell in Jenkins points to /bin/dash which doesn't have the command source . To rectify this, I changed the Jenkins shell to point to /bin/bash instead with this command - sudo ln -sf /bin/bash /bin/sh , which I found in this blog . Now, that DVNA was up and running, ran the baseline scan on it with docker and Zap. But I had to wrap the command in a shell script to evade the non-zero status code that ZAP gives on finding issues. So, I wrote the script, baseline-scan.sh , mentioned below and made it executable with chmod +x : cd /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS echo $? > /dev/null Note : One thing to take note of is that Jenkins would require to use sudo when running docker commands as it's not part of the docker user group on the system. This is the preferred way, that the required docker setup is on another VM and Jenkins SSHs into the other VM with access to run docker commands as a sudo user. But for the purpose of this task, I did not set up another VM, instead, I added the jenkins user to the docker user group with - sudo usermod -aG docker jenkins , for it to be able to perform an operation without using sudo . This, however, is not recommended. I added a stage in the Jenkins Pipeline, to execute the shell script and generate the DAST report. The stage's content is mentioned below: stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } Note : Running the pipeline threw an access error, saying the report could not be written in the directory. This was because the zap user in the docker container did not have write permission for Jenkins' workspace. So, for the sake of the task, I modified the permissions of the node-dast-pipeline/ directory with chmod 777 node-dast-pipeline/ . This is also not recommended. If the permissions need to be changed, they should be specific and exact in terms of the access they grant which should not be more than that is required. Lastly, I added a stage to stop the instance of DVNA that was running as it was no longer needed and moved the report generated from the workspace directory to the reports directory that I've been using for the tasks: stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } The complete pipeline script is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } } } On a different note, I also tried running a docker agent in the Jenkins pipeline to execute zap-baseline scan but it kept throwing an access error (different than the one mentioned above). This issue is not rectified yet. The stage that I wrote (but was not working) is as follows: stage ('Run ZAP for DAST') { agent { docker { image 'owasp/zap2docker-stable' args '-v /{JENKINS HOME DIRECOTORY}/workspace/node-dast-pipeline/:/zap/wrk/' } } steps { sh 'zap-baseline.py -t http://172.17.0.1:9090 -r zap_baseline_report.html -l PASS' } } DAST Report generated by ZAP ZAP's baseline scan found a total of 10 issues with DVNA. The split-up of the issues found is listed in the table below: Sl. No. Description Severity 1. X-Frame-Options Header Not Set Medium 2. CSP Scanner: Wildcard Directive Medium 3. Cross-Domain JavaScript Source File Inclusion Low 4. Absence of Anti-CSRF Tokens Low 5. X-Content-Type-Options Header Missing Low 6. Cookie Without SameSite Attribute Low 7. Web Browser XSS Protection Not Enabled Low 8. Server Leaks Information via \"X-Powered-By\" HTTP Response Header Field(s) Low 9. Content Security Policy (CSP) Header Not Set Low 10. Information Disclosure - Suspicious Comments Informational The complete report generated by the ZAP baseline scan can be found here .","title":"Dynamic Analysis"},{"location":"dynamic_analysis/#dynamic-analysis","text":"","title":"Dynamic Analysis"},{"location":"dynamic_analysis/#objective","text":"The aim of this section is to perform DAST for DVNA with OWASP ZAP and generate a report to provide a solution to the 1st point of the problem statement under Task 2 .","title":"Objective"},{"location":"dynamic_analysis/#dast","text":"DAST or Dynamic Application Security Testing is a black-box testing technique in which the DAST tool interacts with the application being tested in its running state to imitate an attacker. Unlike static analysis, in DAST one does not have access to the source code of the application and the tool is completely reliant on the interactivity the application provides. In dynamic analysis, tools are used to automate attacks on the application ranging from SQL Injection, Input Validation, Cross-Site Scripting, and so forth.","title":"DAST"},{"location":"dynamic_analysis/#owasp-zap","text":"ZAP or Zed Attack Proxy is an open-source tool used to perform dynamic application security testing designed specifically for web applications. ZAP has a desktop interface, APIs for it to be used in an automated fashion and also a CLI. It imitates an actual user where it interacts with the application to perform various attacks. ZAP comes with a plethora of options to use, for which further details can be found here . ZAP also comes as a Docker image which is more convenient to use especially if one is using the CLI interface. Docker is a tool designed to provide ease in shipping applications across platforms. It packages the application, along with all its dependencies and other required libraries into an image . Then containers (running instances of the image) can be used to run the application on any machine. It is similar to a virtual machine but it differs greatly from them based on the fact that it does not require a full-fledged operating system to run the application. Instead, it runs the application on the system's kernel itself by just bringing along the required libraries with it which could be missing on machines other than the one the application was built on. This allows Docker-based applications to be portable i.e. they can be run on any machine that can run Docker containers. It allows for various versions of the same tool/library running on a host as different containers do not care about what is happening inside another container. Further information on docker can be found in the official documentation .","title":"OWASP ZAP"},{"location":"dynamic_analysis/#configuring-owasp-zap-with-docker","text":"To use ZAP with docker, I needed to pull the image from docker hub and start off. I used this documentation from Mozilla as it had a lot of errors demonstrated along with their rectification steps for starting out with ZAP. This was missing from all the other sources I found. Note : While running ZAP scans below, I explicitly ran DVNA, without Jenkins, for it to be tested. To start off with ZAP, I pulled the docker image by running: docker pull owasp/zap2docker-stable Then I tried to run the tool with its CLI interface. The CLI threw an error which was because of an encoding inconsistency between Python2 and Python3. To rectify this issue, I had to explicitly specify the encoding to be used with the help of some environment variables. These environment variables can be seen in the command below along with the -e flag for Docker to inject these environment variables in the container: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli --help Next I ran a quick-scan to have a look at how ZAP outputs information. Since ZAP is running inside a Docker container, I could not use localhost or 127.0.0.1 as then the container would take it to be its own host. So, to overcome this issue I used the IP assigned to docker0 , the network interface created for Docker. This IP can be found with this one-liner: $(ip -f inet -o addr show docker0 | awk '{print $4}' | cut -d '/' -f 1) .Also, as ZAP also has an API that can be used to interact with it programmatically, I had to use --start-options '-config api.disablekey=true' as otherwise, ZAP tried (and failed) to connect to the proxy as the API key was not specified. Also, the -l option specifies the severity level at which ZAP should log a vulnerability to console (or to a report). The --self-contained flag tells ZAP to turn off the daemon once the scan is complete and the --spider option tells ZAP to crawl the target to find other links present on the target site. The complete command is as mentioned below: docker run -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -i owasp/zap2docker-stable zap-cli quick-scan --start-options '-config api.disablekey=true' --self-contained --spider -l Low http://172.17.0.1:9090 Now, I tried the active-scan option. For this I first needed to start the daemon for zap to be accessed by the CLI, run open-url to add the target URL to the configuration in ZAP-CLI (without this, active-scan option will not start a scan on the target) and then run the scan against DVNA. This step was not required previously as while running a quick-scan , the daemon is automatically started to run the scan and stopped after the scan is finished. To do run the daemon with Docker, I ran the following command (with the -u zap segment to execute things with the zap user instead of Docker's default root user and I also appended the --rm flag to delete the container, automatically, when I stop it): docker run --rm -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 --name zap -u zap -p 8090:8080 -d owasp/zap2docker-stable zap.sh -daemon -port 8080 -host 0.0.0.0 -config api.disablekey=true docker exec <CONTAINER NAME/ID> zap-cli open-url <TARGET URL> docker exec <CONTAINER NAME/ID> zap-cli active-scan <TARGET URL> Note : When I ran the scan initially, it dropped an error from Python saying that the 'Connection was Refused' while sending requests to the target through the ZAP proxy. It turned out to be an issue with the command written in the blog that I was following. It exposed the wrong port and hence rectifying the port options in the command to -p 8090:8080 and -port 8080 solved the issue and the scan worked. Another thing is that I chose 8090 for the host port as I already had Jenkins running on 8080 . Now to be able to scan DVNA from the CLI with ZAP-CLI, I required a context which basically was a configuration written in XML to define how to perform a scan. This context also had a set of credentials in it that were recorded with a ZEST script, which is a JSON-based form used to record interactions between a website and a user specially created for security tools focused on web applications. This context file along with the ZEST script would allow zap-cli to authenticate with DVNA to be able to scan the portion of the application that lies behind the login screen. I used the browser-based GUI to generate the context and the Zest script and exported them to the machine. But importing them created various complications as zap-cli was unable to identify the embedded Zest script in the context provided to it. Due to the above complications, I decided to use Zap's baseline scan instead. To start off ZAP baseline scan with the docker image, I ran the following command, where the -t flag specified the target and -l flag defined the alert/severity level, by following this documentation : docker run -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -l INFO Now, to save the report on the Jenkins machine, I needed to mount a volume with Docker. I used the -v flag (as mentioned in the docker documentation ) to mount the present working directory of the host to the /zap/wrk directory of the container and also added the -r flag to save scan output to a HTML report on the Jenkins machine: docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS","title":"Configuring OWASP ZAP with Docker"},{"location":"dynamic_analysis/#integrating-zap-with-jenkins","text":"After understanding how to use the baseline-scan and its various options, I started integrating the scan as part of DAST in the Jenkins pipeline. But before I could scan DVNA with ZAP Baseline, I needed to build the dependencies and start an instance of DVNA. To do the same, I also had to fetch code from the repository on GitHub (explicitly, because I didn't use a Jenkinsfile for this task). Note : I chose to build a new pipeline just for DAST, as combining SAST and DAST in a single pipeline would have taken too much time during each execution which felt unnecessary at this point. To start off, I added a stage to fetch the code from the GitHub repository as follows: stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } Next, I built the dependencies for DVNA, as I did while performing SAST, and started an instance of DVNA, as mentioned in the stage mentioned below: stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } Note : I kept all the required environment variables in the env.sh file which can be seen in the above stage, and used it to export those variables for DVNA to be able to connect with the database. While trying to export the variables, the shell that comes along with Jenkins kept throwing an error saying it didn't recognize the command source . This turned out to be because that by default the sh shell in Jenkins points to /bin/dash which doesn't have the command source . To rectify this, I changed the Jenkins shell to point to /bin/bash instead with this command - sudo ln -sf /bin/bash /bin/sh , which I found in this blog . Now, that DVNA was up and running, ran the baseline scan on it with docker and Zap. But I had to wrap the command in a shell script to evade the non-zero status code that ZAP gives on finding issues. So, I wrote the script, baseline-scan.sh , mentioned below and made it executable with chmod +x : cd /{JENKINS HOME DIRECTORY}/workspace/node-dast-pipeline docker run -v $(pwd):/zap/wrk/ -i owasp/zap2docker zap-baseline.py -t \"http://172.17.0.1:9090\" -r baseline-report.html -l PASS echo $? > /dev/null Note : One thing to take note of is that Jenkins would require to use sudo when running docker commands as it's not part of the docker user group on the system. This is the preferred way, that the required docker setup is on another VM and Jenkins SSHs into the other VM with access to run docker commands as a sudo user. But for the purpose of this task, I did not set up another VM, instead, I added the jenkins user to the docker user group with - sudo usermod -aG docker jenkins , for it to be able to perform an operation without using sudo . This, however, is not recommended. I added a stage in the Jenkins Pipeline, to execute the shell script and generate the DAST report. The stage's content is mentioned below: stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } Note : Running the pipeline threw an access error, saying the report could not be written in the directory. This was because the zap user in the docker container did not have write permission for Jenkins' workspace. So, for the sake of the task, I modified the permissions of the node-dast-pipeline/ directory with chmod 777 node-dast-pipeline/ . This is also not recommended. If the permissions need to be changed, they should be specific and exact in terms of the access they grant which should not be more than that is required. Lastly, I added a stage to stop the instance of DVNA that was running as it was no longer needed and moved the report generated from the workspace directory to the reports directory that I've been using for the tasks: stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } The complete pipeline script is as follows: pipeline { agent any stages { stage ('Fetching Code') { steps { git url: 'https://github.com/ayushpriya10/dvna.git' } } stage ('Building DVNA') { steps { sh ''' npm install source /{PATH TO SCRIPT}/env.sh pm2 start server.js ''' } } stage ('Run ZAP for DAST') { steps { sh '/{PATH TO SCRIPT}/baseline-scan.sh' } } stage ('Take DVNA offline') { steps { sh 'pm2 stop server.js' sh 'mv baseline-report.html /{JENKINS HOME DIRECTORY}/reports/zap-report.html' } } } } On a different note, I also tried running a docker agent in the Jenkins pipeline to execute zap-baseline scan but it kept throwing an access error (different than the one mentioned above). This issue is not rectified yet. The stage that I wrote (but was not working) is as follows: stage ('Run ZAP for DAST') { agent { docker { image 'owasp/zap2docker-stable' args '-v /{JENKINS HOME DIRECOTORY}/workspace/node-dast-pipeline/:/zap/wrk/' } } steps { sh 'zap-baseline.py -t http://172.17.0.1:9090 -r zap_baseline_report.html -l PASS' } }","title":"Integrating ZAP with Jenkins"},{"location":"dynamic_analysis/#dast-report-generated-by-zap","text":"ZAP's baseline scan found a total of 10 issues with DVNA. The split-up of the issues found is listed in the table below: Sl. No. Description Severity 1. X-Frame-Options Header Not Set Medium 2. CSP Scanner: Wildcard Directive Medium 3. Cross-Domain JavaScript Source File Inclusion Low 4. Absence of Anti-CSRF Tokens Low 5. X-Content-Type-Options Header Missing Low 6. Cookie Without SameSite Attribute Low 7. Web Browser XSS Protection Not Enabled Low 8. Server Leaks Information via \"X-Powered-By\" HTTP Response Header Field(s) Low 9. Content Security Policy (CSP) Header Not Set Low 10. Information Disclosure - Suspicious Comments Informational The complete report generated by the ZAP baseline scan can be found here .","title":"DAST Report generated by ZAP"},{"location":"generating_sbom/","text":"Generating Software Bill of Materials Objective The aim of this section is to generate a Software Bill of Materials for DVNA and provide a solution to the 2nd point of the problem statement under Task 3 . Software Bill of Materials A Bill of Materials is a list of components used to assemble/create a product. It gives out a specification about how each component was used in the making of the end product. A Software Bill of Materials (SBoM) refers to a list of all software components, open-source or commercial, that was utilized to build a software solution. A more detailed description of software bill of materials can be found here . CycloneDX DVNA, like most other applications, is built with dependencies. To generate the SBoM for DVNA, I found a tool called CycloneDX . According to its documentation , it is a tool that creates the SBoM which contains the aggregate of all the dependencies for the project. CycloneDX is available to be used an NPM package that can generate SBoMs for Nodejs applications but also comes in a variety of implementations to serve projects using different stacks such as Python, Maven, .NET, etc. For my use case, I stuck with the NPM package as DVNA only utilizes Nodejs. Generating SBoM for DVNA To start off, I installed CycleDX's Node module with NPM by following the official documentation, and using the command: npm install -g @cyclonedx/bom Then I ran CycloneDX, with the command mentioned below, in the root directory of the project to gauge the output and figure out the structure of the SBoM generated: cyclonedx-bom Note : I initially ran the scan before building the modules as I used the project directory from the Lint Analysis pipeline which did not require me to build DVNA. So, I had to run npm install before I ran CycloneDX again as it required the /node_modules directory to look through and identify the dependencies. Lastly, I added a stage in the pipeline to run CycloneDX and store the SBoM ( sbom.xml ) in the local reports folder that I have been using through the entirety of the tasks: stage ('Generating Software Bill of Materials') { steps { //Building the dependencies to generate SBoM sh 'npm install' sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } } Software Bill of Material for DVNA CycloneDX generated a comprehensive SBoM for DVNA. It was in XML format. For each dependency, CycloneDX reported - Name of the Module, the version being used, its description, its hash checksum, the license the module uses, the package URL and external references (if any). The full Software Bill of Material generated by CycloneDX can be found here .","title":"Generating Software Bill of Materials"},{"location":"generating_sbom/#generating-software-bill-of-materials","text":"","title":"Generating Software Bill of Materials"},{"location":"generating_sbom/#objective","text":"The aim of this section is to generate a Software Bill of Materials for DVNA and provide a solution to the 2nd point of the problem statement under Task 3 .","title":"Objective"},{"location":"generating_sbom/#software-bill-of-materials","text":"A Bill of Materials is a list of components used to assemble/create a product. It gives out a specification about how each component was used in the making of the end product. A Software Bill of Materials (SBoM) refers to a list of all software components, open-source or commercial, that was utilized to build a software solution. A more detailed description of software bill of materials can be found here .","title":"Software Bill of Materials"},{"location":"generating_sbom/#cyclonedx","text":"DVNA, like most other applications, is built with dependencies. To generate the SBoM for DVNA, I found a tool called CycloneDX . According to its documentation , it is a tool that creates the SBoM which contains the aggregate of all the dependencies for the project. CycloneDX is available to be used an NPM package that can generate SBoMs for Nodejs applications but also comes in a variety of implementations to serve projects using different stacks such as Python, Maven, .NET, etc. For my use case, I stuck with the NPM package as DVNA only utilizes Nodejs.","title":"CycloneDX"},{"location":"generating_sbom/#generating-sbom-for-dvna","text":"To start off, I installed CycleDX's Node module with NPM by following the official documentation, and using the command: npm install -g @cyclonedx/bom Then I ran CycloneDX, with the command mentioned below, in the root directory of the project to gauge the output and figure out the structure of the SBoM generated: cyclonedx-bom Note : I initially ran the scan before building the modules as I used the project directory from the Lint Analysis pipeline which did not require me to build DVNA. So, I had to run npm install before I ran CycloneDX again as it required the /node_modules directory to look through and identify the dependencies. Lastly, I added a stage in the pipeline to run CycloneDX and store the SBoM ( sbom.xml ) in the local reports folder that I have been using through the entirety of the tasks: stage ('Generating Software Bill of Materials') { steps { //Building the dependencies to generate SBoM sh 'npm install' sh 'cyclonedx-bom -o /{JENKINS HOME DIRECTORY}/reports/sbom.xml' } }","title":"Generating SBoM for DVNA"},{"location":"generating_sbom/#software-bill-of-material-for-dvna","text":"CycloneDX generated a comprehensive SBoM for DVNA. It was in XML format. For each dependency, CycloneDX reported - Name of the Module, the version being used, its description, its hash checksum, the license the module uses, the package URL and external references (if any). The full Software Bill of Material generated by CycloneDX can be found here .","title":"Software Bill of Material for DVNA"},{"location":"glossary/","text":"Glossary There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"glossary/#glossary","text":"There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"problem_statement/","text":"Problem Statement Task 1 Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next, the DVNA should get deployed in a server. Setup the infrastructure, required for the task, on 2 virtual machines running locally on a laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparative report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub. Task 2 Add onto Task 1 by implementing DAST in the pipeline for DVNA with using OWASP ZAP . Implement and use Semantic Versioning to tag versions of the documentation created as part of the tasks. Append content for the task to the existing documentation from first task. Task 3 Perform commit-based checks on the source code for linting errors to improve code quality and generate quality report. Generate software bill of materials for all dependencies.","title":"Problem Statement"},{"location":"problem_statement/#problem-statement","text":"","title":"Problem Statement"},{"location":"problem_statement/#task-1","text":"Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next, the DVNA should get deployed in a server. Setup the infrastructure, required for the task, on 2 virtual machines running locally on a laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparative report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Task 1"},{"location":"problem_statement/#task-2","text":"Add onto Task 1 by implementing DAST in the pipeline for DVNA with using OWASP ZAP . Implement and use Semantic Versioning to tag versions of the documentation created as part of the tasks. Append content for the task to the existing documentation from first task.","title":"Task 2"},{"location":"problem_statement/#task-3","text":"Perform commit-based checks on the source code for linting errors to improve code quality and generate quality report. Generate software bill of materials for all dependencies.","title":"Task 3"},{"location":"resources/","text":"References These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"Resources"},{"location":"resources/#references","text":"These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"References"},{"location":"setting_up_pipeline/","text":"Setting Up Pipeline Objective The aim of this section is to set up a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement under Task 1 . Pipeline A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language. Jenkins Pipeline Project To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which leads me to a different page. I gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description of the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artifacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations. The Jenkinsfile Jenkins has a utility where the actions that are to be performed on the build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<MYSQL PASSWORD> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins. Stages I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose: Initialization This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline. Build In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependencies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets built with all of its dependencies only on the Jenkins VM. Static Analysis All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools . Deployment Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking the functioning of the instance of the application running on the production machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#setting-up-pipeline","text":"","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#objective","text":"The aim of this section is to set up a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement under Task 1 .","title":"Objective"},{"location":"setting_up_pipeline/#pipeline","text":"A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language.","title":"Pipeline"},{"location":"setting_up_pipeline/#jenkins-pipeline-project","text":"To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which leads me to a different page. I gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description of the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artifacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations.","title":"Jenkins Pipeline Project"},{"location":"setting_up_pipeline/#the-jenkinsfile","text":"Jenkins has a utility where the actions that are to be performed on the build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<MYSQL PASSWORD> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins.","title":"The Jenkinsfile"},{"location":"setting_up_pipeline/#stages","text":"I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose:","title":"Stages"},{"location":"setting_up_pipeline/#initialization","text":"This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline.","title":"Initialization"},{"location":"setting_up_pipeline/#build","text":"In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependencies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets built with all of its dependencies only on the Jenkins VM.","title":"Build"},{"location":"setting_up_pipeline/#static-analysis","text":"All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools .","title":"Static Analysis"},{"location":"setting_up_pipeline/#deployment","text":"Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking the functioning of the instance of the application running on the production machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Deployment"},{"location":"setting_up_vms/","text":"Setting up VMs Objective The aim of this section is to set up the required infrastructure to perform the task and solve the 6th point of the problem statement under Task 1 . System Configuration The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipeline. The release notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under the \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components would be installed. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete. Installing Jenkins Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentation. For this documentation, I didn't skip any step. Choosing the Application The application that was to be analyzed and deployed, as required by the problem statement, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts. Configuring Production VM To serve DVNA , there were some prerequisites. The following steps conclude how to set up the prerequisites for Jenkins to be able to deploy the application through the pipeline. Setting up DVNA I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was installed on the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in the user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage. Configuring SSH Access For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , created a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh into the production VM. The credentials for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Setting Up VMs"},{"location":"setting_up_vms/#setting-up-vms","text":"","title":"Setting up VMs"},{"location":"setting_up_vms/#objective","text":"The aim of this section is to set up the required infrastructure to perform the task and solve the 6th point of the problem statement under Task 1 .","title":"Objective"},{"location":"setting_up_vms/#system-configuration","text":"The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipeline. The release notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under the \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components would be installed. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete.","title":"System Configuration"},{"location":"setting_up_vms/#installing-jenkins","text":"Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentation. For this documentation, I didn't skip any step.","title":"Installing Jenkins"},{"location":"setting_up_vms/#choosing-the-application","text":"The application that was to be analyzed and deployed, as required by the problem statement, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts.","title":"Choosing the Application"},{"location":"setting_up_vms/#configuring-production-vm","text":"To serve DVNA , there were some prerequisites. The following steps conclude how to set up the prerequisites for Jenkins to be able to deploy the application through the pipeline.","title":"Configuring Production VM"},{"location":"setting_up_vms/#setting-up-dvna","text":"I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was installed on the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in the user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage.","title":"Setting up DVNA"},{"location":"setting_up_vms/#configuring-ssh-access","text":"For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , created a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh into the production VM. The credentials for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Configuring SSH Access"},{"location":"static_analysis/","text":"Static Analysis Objective The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to the 2nd, 3rd and 4th points of the problem statement under Task 1 . SAST SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of the project's logic which might lead to a security vulnerability. DVNA's Tech Stack SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server-side language used, along with a SQL database. SAST Tools for Node.js Applications After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins: SonarQube SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilize SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at the pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article. NPM Audit NPM Audit is a built-in utility that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since NPM Audit comes along with npm itself, is not required to be installed separately. However, if one has an older version of npm on the system, the following command can be used to upgrade: sudo npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if run through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } } NodeJsScan NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realized that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } } Retire.js Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: sudo npm install -g retire Note : The -g flag specifies that the package needs to be installed globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } } OWASP Dependency Check As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of its dependencies. OWASP Dependency Check comes as an executable for Linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology. Auditjs Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyze NodeJs applications. Like Retire.js, Auditjs is also available as an npm-package. So, I installed it with the following command: sudo npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirect STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } } Snyk Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: sudo npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script, which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } } SAST Analysis Reports I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file. Other Tools There were a few other tools available to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavailable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now deprecated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is a SAST tool that combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Static Analysis"},{"location":"static_analysis/#static-analysis","text":"","title":"Static Analysis"},{"location":"static_analysis/#objective","text":"The aim of this section is to understand the tech stack used for the project (DVNA), identify suitable tools to perform SAST and generate a report to provide a solution to the 2nd, 3rd and 4th points of the problem statement under Task 1 .","title":"Objective"},{"location":"static_analysis/#sast","text":"SAST or Static Application Security Testing is a process that analyses a project's source code, dependencies and related files for known security vulnerabilities. SAST could also help identify segments of the project's logic which might lead to a security vulnerability.","title":"SAST"},{"location":"static_analysis/#dvnas-tech-stack","text":"SAST is a targeted analysis configured based on the technologies being used in a project. Hence, for any meaningful SAST stage in a pipeline (or in general), the tools utilized should be concerned only with the technologies that the project uses. If need be, one could use multiple tools (as one should, in most cases) to cover different types of vulnerabilities and/or technologies present in the project. To perform static analysis on DVNA, the first step for me was to identify what all technologies comprise DVNA (which is quite obvious given the name is Damn Vulnerable NodeJs Application). So, I figured that NodeJs is the server-side language used, along with a SQL database.","title":"DVNA's Tech Stack"},{"location":"static_analysis/#sast-tools-for-nodejs-applications","text":"After figuring out the tech stack used, I focused on finding tools that perform static analysis specifically for Nodejs applications. The following are some tools that I found to perform SAST on Nodejs applications with steps to install it and configure them with Jenkins:","title":"SAST Tools for Node.js Applications"},{"location":"static_analysis/#sonarqube","text":"SonarQube is a commercial static analysis tool with a community version with restricted features. I used this Medium article to utilize SonarQube with Jenkins and Docker: To start the SonarQube server for analysis I used SonarQube's docker image, as it seemed more convenient than an installed setup unlike all the other tools I used which had a very simple installation procedure, and ran it with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Then for Jenkins to authenticate with the SonarQube server I created a new Access Token for Jenkins in SonarQube under Account > Security . I saved the above generated token in Jenkins, under Credentials > Add New Credentials as a Secret Text type credential so I could use it later with the credential identity created. I added the SonarQube plugin for Jenkins and then navigated to the SonarQube Server section under Manage Jenkins > Configure System . Here, I checked the Enable injection of SonarQube server configuration as build environment variables option to allow SonarQube to inject environment variables at the pipeline's runtime and be used in the Jenkinsfile. I provided the URL for SonarQube Server (in my case) localhost:9000 and added the previously saved SonarQube Credentials for authentication. Lastly, I added the following stage in the Jenkinsfile for DVNA's analysis by SonarQube, which injects the path to the SonarQube scanner, performs the scan and then saves the report locally: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /{JENKINS HOME DIRECTORY}/reports/sonarqube-report' } } } Note : There are two segments where I drifted away from the article I referred to: First is that in the article there is no mention of generating/storing a report. Secondly, I left out the timeout block in the pipeline stage given in the article.","title":"SonarQube"},{"location":"static_analysis/#npm-audit","text":"NPM Audit is a built-in utility that comes along with npm@6 which allows for auditing the dependencies being used in the project i.e. it analyses the dependencies against a database for known vulnerabilities. Since NPM Audit comes along with npm itself, is not required to be installed separately. However, if one has an older version of npm on the system, the following command can be used to upgrade: sudo npm install -g npm@latest Now, NPM Audit has a characteristic that gives a non-zero status code, if it finds any vulnerable dependencies. This is so if run through a pipeline, the build can fail thus, stopping the deployment of vulnerable code. Since, DVNA, quite obviously, has a lot of vulnerabilities, I had to run it through a script to avoid failure of the pipeline after analysis so the stages can still be executed. The script that I wrote, which runs npm-audit , formats the output in a JSON format, saves it to a file and finally just echoes the status code, is as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline npm audit --json > /{JENKINS HOME DIRECTORY}/reports/npm-audit-report echo $? > /dev/null Lastly, I added the following stage in the Jenkinsfile to execute the script I wrote: stage ('NPM Audit Analysis') { steps { sh '/{PATH TO SCRIPT}/npm-audit.sh' } }","title":"NPM Audit"},{"location":"static_analysis/#nodejsscan","text":"NodeJsScan is a static security code scanner for NodeJs applications written in python. It comes with a web-based interface, docker image, Python API as well as a CLI. Unlike SonarQube, installing NodeJsScan was just a single command so I went ahead and installed it. To install NodeJsScan , I used the following command: pip3 install nodejsscan Note : I noticed that the package was not available to all users, even though I installed it globally. So, to rectify this issue, I ran the following command: sudo -H pip3 install nodejsscan . Once NodeJsScan was installed, I ran the below command (taken from the official documentation ) to test the tool and observe its operation before I added it to the pipeline script: nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report After observing that NodeJsScan did not exit with a non-zero status code, even if vulnerabilities were found, I realized that the command to execute the scan can be directly added to the pipeline. So, I added the following stage in the Jenkinsfile to perform the scan, and store the report in JSON format on the Jenkins machine: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /{JENKINS HOME DIRECTORY}/reports/nodejsscan-report' } }","title":"NodeJsScan"},{"location":"static_analysis/#retirejs","text":"Retire.js is a tool that scans the project's dependencies to identify dependencies with versions that have known vulnerabilities. It comes as a plugin for various applications and as a CLI. Retire.js was also available to be installed as a package without too much hassle, so I installed it with the following command: sudo npm install -g retire Note : The -g flag specifies that the package needs to be installed globally. Then to look at how Retire.js functions, I ran it with the following command as mentioned in the official documentation to run the scan on DVNA, output the report in JSON format, save it locally on a file and then exit with a zero status-code even if vulnerabilities are found: retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0 After observing the output and since, I had the ability to alter the status code the program gave on exit, I used the command directly and added the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /{JENKINS HOME DIRECTORY}/reports/retirejs-report --exitwith 0' } }","title":"Retire.js"},{"location":"static_analysis/#owasp-dependency-check","text":"As mentioned on OWASP Dependency Check's official site, it is a software composition analysis tool, used to identify if the project has any known security vulnerabilities as part of its dependencies. OWASP Dependency Check comes as an executable for Linux and does not require any installation, so I decided to use the binary. I downloaded the executable from this archive . Next, I unzipped the archive and then placed its contents in /{JENKINS HOME DIRECTORY}/ : unzip dependency-check-5.2.4-release.zip As written in the official documentation , I ran the following command to start the scan with the executable and save the output to a file in JSON format: /{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint Since, Dependency-Check doesn't change the status code to a non-zero one, I added the command directly as a stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/{JENKINS HOME DIRECTORY}/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /{JENKINS HOME DIRECTORY}/reports/dependency-check-report --prettyPrint' } } Note : By using OWASP's Dependency Check, I happened to introduce a redundant use of a few tools, namely Retire.js and NPM Audit, as they are already a part of Dependency Check's scan methodology.","title":"OWASP Dependency Check"},{"location":"static_analysis/#auditjs","text":"Auditjs is a SAST tool which uses OSS Index , which is a service used to determine if a dependency being used has a known vulnerability, to analyze NodeJs applications. Like Retire.js, Auditjs is also available as an npm-package. So, I installed it with the following command: sudo npm install -g auditjs Next, I ran a scan to observe the output provided by Auditjs by running the following command, as mentioned in the documentation , while being inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 Note : As it appears, Auditjs prints the vulnerabilities found to STDERR and everything else to STDOUT. Hence, I couldn't write the vulnerabilities found to a file directly. So, I used 2>&1 to redirect STDERR output to STDOUT to be able to write everything to a file. Like some previous tools, Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid build failures with the pipeline. I wrote a script to overcome this issue, as done previously as well, to run the scan and save the report locally. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /{JENKINS HOME DIRECTORY}/reports/auditjs-report 2>&1 echo $? > /dev/null Lastly, I added the following stage to the Jenkinsfile to execute the script I wrote: stage ('Audit.js Analysis') { steps { sh '/{PATH TO SCRIPT}/auditjs.sh' } }","title":"Auditjs"},{"location":"static_analysis/#snyk","text":"Snyk is a platform that helps monitor (open source) projects present on GitHub, Bitbucket, etc. or locally to identify dependencies with known vulnerabilities. It is available as a CLI and as a docker image. Snyk can be installed with npm so, I used the following command to do so: sudo npm install -g snyk Snyk required that I authenticated Snyk CLI with an Authentication Token, that can be found on one's profile after signing up for Snyk, before scanning a project, which I did as follows: snyk auth <AUTH TOKEN> Then to perform a scan I ran the following command as mentioned in the official documentation : snyk test Snyk also gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script, which performs a scan and stores the report in a JSON format, to avoid build-failure with the pipeline. The contents of the script I wrote are as follows: #!/bin/bash cd /{JENKINS HOME DIRECTORY}/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /{JENKINS HOME DIRECTORY}/reports/snyk-report echo $? > /dev/null Finally, I added a stage to the pipeline which executes the script: stage ('Snyk Analysis') { steps { sh '/{PATH TO SCRIPT}/snyk.sh' } }","title":"Snyk"},{"location":"static_analysis/#sast-analysis-reports","text":"I stored the reports generated by the various tools in /reports/ directory inside Jenkins' home directory. Most of these reports were in JSON format, except for SonarQube and Auditjs. SonarQube's report was available on in the web interface and Auditjs' report was normal textual output being printed to console or, in my case, being redirected to a file.","title":"SAST Analysis Reports"},{"location":"static_analysis/#other-tools","text":"There were a few other tools available to perform SAST on NodeJs applications. They are listed below along with the reason why I chose not to use them for this task: NSP According to what NSP's (Node Security Project) official site said, it is now replaced with npm audit starting npm@6 and hence, is unavailable to new users but without any loss as it's functionality is available with NPM Audit. JSPrime JSPrime appeared to be a really nice tool from its documentation and a demonstration video from a talk in a security conference, but it lacked a CLI interface and hence, I couldn't integrate it into the CI Pipeline. ScanJS As stated by the official site, ScanJS is now deprecated and was throwing an exception when I tried running an available version via the CLI interface. So, I ended up excluding it from my implementation for the task. JSpwn (JSPrime + ScanJs) JSpwn is a SAST tool that combined both JSPrime and ScanJs and had a CLI interface as well but when I executed it in accordance with the official documentation, the CLI gave garbage output without throwing any error and ran without ever terminating. Hence, I chose not to use it in my solution for the task.","title":"Other Tools"}]}