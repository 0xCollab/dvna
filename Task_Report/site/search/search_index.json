{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of contents The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Static Analysis Comparing SAST Tools Setting Up Pipeline Configuring Webhook Deploying the Report Resources This documentation can also be found online .","title":"Introduction"},{"location":"#table-of-contents","text":"The following is the report/documentation for the problem statement stated below. The contents of the report are: Problem Statement Setting Up VMs Static Analysis Comparing SAST Tools Setting Up Pipeline Configuring Webhook Deploying the Report Resources This documentation can also be found online .","title":"Table of contents"},{"location":"comparing_sast_tools/","text":"Comparing SAST Tools A comparitive look at the various findings the various tools had. Vulnerability Reports Generated The following are the various vulnerabilities found by the tools used to perform SAST on DVNA: SonarQube SonarQube didn't find any security vulnerabilities. It, instead, found linting and syntax based bugs. NPM Audit NPM Audit found 5 security vulnerabilities: 3 Critical 1 High 1 Low Vulnereble modules identified: mathjs (2 Critical Vulnerabilities) node-serialize (1 Critical Vulnerability) typed-function (1 High Vulnerability) express-fileupload (1 Low Vulnerability) NodeJsScan NodeJsScan found a 34 dependency-based vulnerabilities: Deserialization with Remote Code Execution (8 Vulnerabilities) Open Redirect (1 Vulnerabilities) SQL Injection (1 Vulnerabilities) Secrete Hardcoded (1 Vulnerabilities) Server Side Injection (1 Vulnerabilities) Unescaped Variables (12 Vulnerabilities) Weak Hash Used (11 Vulnerabilities) Additionally, NodeJsScan found 5 web-based vulnerabilities: Missing Header Strict-Transport-Security (HSTS) Public-Key-Pin (HPKP) X-XSS-Protection X-Download-Options Information Disclosure X-Powered-By Retire.js Retire.js found 3 security vulnerabilities: 1 High 1 Medium 1 Low Vulnerable modules identified: node-serialize 0.0.4 (High) jquery 2.1.1 (Medium) jquery 3.2.1 (Low) OWASP Dependency Check Dependency-Check identified 7 security vulnerabilities: 3 Critical 1 High 2 Medium 1 Low Vulnerable modules identified: mathjs 3.10.1 (Critical) node-serialize 0.0.4 (Critical) sequelize 4.44.3 (Critical) typed-function 0.10.5 (High) jquery-2.1.1.min.js (Medium) jquery-3.2.1.min.js (Medium) express-fileupload 0.4.0 (Low) Auditjs Auditjs found 22 security vulnerabilities in the 5 vulnerable modules identified: Nodejs 8.10.0 (14 Vulnerabilities) mathjs 3.10.1 (3 Vulnerabilities) typed-function 0.10.5 (2 Vulnerabilities) sequelize 4.44.3 (2 Vulnerabilities) express-fileupload 0.4.0 (1 Vulnerability) Snyk Snyk indentifeid 8 security vulnerabilities: 6 High: express-fileupload (Denial of Service; 1 Vulnerability) typed-function (Arbitary Code Execution; 1 Vulnerability) mathjs (Arbitary Code Execution; 3 Vulnerabilities) node-serialize (Arbitary Code Execution; 1 Vulnerability) 2 Medium: mathjs (Arbitary Code Execution; 2 Vulnerabilities) Conclusion Based on the reports generated and the vulnerabilities found by the various scanner used to analyse DVNA, the ranking of these tools from best to worse (in my opinion) is as follows: NodeJsScan (34 dependency-based + 5 web-based) Auditjs (22) Snyk (8) OWASP Dependency Check (7) NPM Audit (5) Retire.js (3) SonarQube (0)","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#comparing-sast-tools","text":"A comparitive look at the various findings the various tools had.","title":"Comparing SAST Tools"},{"location":"comparing_sast_tools/#vulnerability-reports-generated","text":"The following are the various vulnerabilities found by the tools used to perform SAST on DVNA:","title":"Vulnerability Reports Generated"},{"location":"comparing_sast_tools/#sonarqube","text":"SonarQube didn't find any security vulnerabilities. It, instead, found linting and syntax based bugs.","title":"SonarQube"},{"location":"comparing_sast_tools/#npm-audit","text":"NPM Audit found 5 security vulnerabilities: 3 Critical 1 High 1 Low Vulnereble modules identified: mathjs (2 Critical Vulnerabilities) node-serialize (1 Critical Vulnerability) typed-function (1 High Vulnerability) express-fileupload (1 Low Vulnerability)","title":"NPM Audit"},{"location":"comparing_sast_tools/#nodejsscan","text":"NodeJsScan found a 34 dependency-based vulnerabilities: Deserialization with Remote Code Execution (8 Vulnerabilities) Open Redirect (1 Vulnerabilities) SQL Injection (1 Vulnerabilities) Secrete Hardcoded (1 Vulnerabilities) Server Side Injection (1 Vulnerabilities) Unescaped Variables (12 Vulnerabilities) Weak Hash Used (11 Vulnerabilities) Additionally, NodeJsScan found 5 web-based vulnerabilities: Missing Header Strict-Transport-Security (HSTS) Public-Key-Pin (HPKP) X-XSS-Protection X-Download-Options Information Disclosure X-Powered-By","title":"NodeJsScan"},{"location":"comparing_sast_tools/#retirejs","text":"Retire.js found 3 security vulnerabilities: 1 High 1 Medium 1 Low Vulnerable modules identified: node-serialize 0.0.4 (High) jquery 2.1.1 (Medium) jquery 3.2.1 (Low)","title":"Retire.js"},{"location":"comparing_sast_tools/#owasp-dependency-check","text":"Dependency-Check identified 7 security vulnerabilities: 3 Critical 1 High 2 Medium 1 Low Vulnerable modules identified: mathjs 3.10.1 (Critical) node-serialize 0.0.4 (Critical) sequelize 4.44.3 (Critical) typed-function 0.10.5 (High) jquery-2.1.1.min.js (Medium) jquery-3.2.1.min.js (Medium) express-fileupload 0.4.0 (Low)","title":"OWASP Dependency Check"},{"location":"comparing_sast_tools/#auditjs","text":"Auditjs found 22 security vulnerabilities in the 5 vulnerable modules identified: Nodejs 8.10.0 (14 Vulnerabilities) mathjs 3.10.1 (3 Vulnerabilities) typed-function 0.10.5 (2 Vulnerabilities) sequelize 4.44.3 (2 Vulnerabilities) express-fileupload 0.4.0 (1 Vulnerability)","title":"Auditjs"},{"location":"comparing_sast_tools/#snyk","text":"Snyk indentifeid 8 security vulnerabilities: 6 High: express-fileupload (Denial of Service; 1 Vulnerability) typed-function (Arbitary Code Execution; 1 Vulnerability) mathjs (Arbitary Code Execution; 3 Vulnerabilities) node-serialize (Arbitary Code Execution; 1 Vulnerability) 2 Medium: mathjs (Arbitary Code Execution; 2 Vulnerabilities)","title":"Snyk"},{"location":"comparing_sast_tools/#conclusion","text":"Based on the reports generated and the vulnerabilities found by the various scanner used to analyse DVNA, the ranking of these tools from best to worse (in my opinion) is as follows: NodeJsScan (34 dependency-based + 5 web-based) Auditjs (22) Snyk (8) OWASP Dependency Check (7) NPM Audit (5) Retire.js (3) SonarQube (0)","title":"Conclusion"},{"location":"configuring_webhook/","text":"Configuring Trigger with Webhook Objective The aim of this section is to create and configure a webhook to automate builds based on defined events occuring on the project repository in reference to 8th point's second section in the problem statement . Webhooks Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered on the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well. Configuring Jenkins Pipeline for Webhook For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked. Configuring GitHub for Webhook Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub doesn't send any request. I tried triggering the selected events, after unchecking the option, and it didn't send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL. Using ngrok to handle Webhook over Internet Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since, the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dyanamically generated URL which can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start a HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Configuring Webhook"},{"location":"configuring_webhook/#configuring-trigger-with-webhook","text":"","title":"Configuring Trigger with Webhook"},{"location":"configuring_webhook/#objective","text":"The aim of this section is to create and configure a webhook to automate builds based on defined events occuring on the project repository in reference to 8th point's second section in the problem statement .","title":"Objective"},{"location":"configuring_webhook/#webhooks","text":"Webhooks, sometimes referred to as Reverse APIs , are functions that are triggered on the occurrence of selected events. These functions, generally, are used to notify a different interface/endpoint about the occurence of the event. To build and deploy the application based on push events and new releases on the project repository on GitHub automatically, I needed a Jenkins Webhook to handle a trigger that GitHub will send when selected events occur. To create a webhook as part of the solution for the problem statement, I used this article as it also used ngrok as I was supposed to use in further sections. I, however, did not add the GitHub repository link under Configure Jenkins > GitHub Pull Requests mentioned in the article, as the webhook worked without it as well.","title":"Webhooks"},{"location":"configuring_webhook/#configuring-jenkins-pipeline-for-webhook","text":"For Jenkins, the configuration was straightforward and simple. All I had to do was to select the GitHub hook trigger for GITScm polling option under Build triggers for the Jenkins pipeline. This option allows Jenkins to listen for any requests sent to it via GitHub (in this case) and then trigger a new build for the project which has the option checked.","title":"Configuring Jenkins Pipeline for Webhook"},{"location":"configuring_webhook/#configuring-github-for-webhook","text":"Then I needed to add a webhook trigger on GitHub for the project repository. Following the documentation, mentioned above, I went to the Settings page for the repository and from there, I went to the Webhooks page. Then clicking on the Add New button, I got a page asking for specifics about the webhook that I wanted to create: The Payload URL is where I had to put my Jenkins Servers domain/IP which would be handling the webhook. As required by Jenkins, a valid Jenkins Webhook is a domain/IP appended with /github-webhook/ at the end. For example, http://{JENKINS_VM_IP}/github-webhook/ is a valid Jenkins webhook. I put the Content Type as application/json as Jenkins expects a JSON formatted request. Then I selected the required events that should trigger the webhook, and in turn, start the build via Jenkins. In this case, the events that were required by the problem statement were 'Pushes' and 'Releases'. Lastly, I checked the Active option and saved the Webhook. The active option is necessary to be set to checked else, GitHub doesn't send any request. I tried triggering the selected events, after unchecking the option, and it didn't send any request. Note : The webhook's payload URL should have exactly /github-webhook/ at the end. Missing the slash at the end will not be handled and thus, no build will be triggered if the exact route is not appended to the payload URL.","title":"Configuring GitHub for Webhook"},{"location":"configuring_webhook/#using-ngrok-to-handle-webhook-over-internet","text":"Now, GitHub needed a public IP/domain to send the event payload to when the webhook gets triggered over the internet. Since, the setup I had was on my local machine, I ended up using ngrok . Ngrok is a tool that helps to expose a machine to the internet by providing a dyanamically generated URL which can be used to tunnel traffic from the internet to our local machine and use it as needed. I found the basic documentation, which was enough for me to use it for the purpose of this task, on the downloads page itself. So, as specified in the instructions given at the site: I downloaded the ngrok executable, which was available for download here , on the Jenkins VM. Then I unzipped the downloaded file as follows: unzip /path/to/ngrok.zip Ngrok requires an authentication token to work. Hence, I signed up for an account to get an Authentication Token and then authenticated ngrok for initialization as follows: ./ngrok authtoken <AUTH_TOKEN> Finally, to run ngrok and start a HTTP Tunnel, I used the following command: ./ngrok http 8080 Note : I used port 8080, instead of the usual 80 used for HTTP traffic, as my instance of Jenkins was running on 8080. Lastly, I took the URL provided by ngrok , appended /github-webhook/ at the end, and used it as the PAYLOAD URL on GitHub for the Webhook as mentioned in the Configuring GitHub for Webhook section above. The final payload URL was like - http://{DYNAMIC_SUBDOMAIN}.ngrok.io/github-webhook/ where the dynamic subdomain was generated by ngrok to create a unique tunnel for us to use.","title":"Using ngrok to handle Webhook over Internet"},{"location":"deploying_report/","text":"Deploying Report with MkDocs Objective The aim of this section is to create a documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement . Format and Tools The report was written in Markdown as required by the problem statement. Markdown is a markup language which allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as reqiured by the problem statement, to build a static site with the report. MkDocs is a static site generator which creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file. Installing MkDocs I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as rest of the steps were not required in the context of the task/problem statement. Selecting a Theme MkDocs allows users to use various themes to customise the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section. Site Configuration To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site. Deploying Static Site To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying the Report"},{"location":"deploying_report/#deploying-report-with-mkdocs","text":"","title":"Deploying Report with MkDocs"},{"location":"deploying_report/#objective","text":"The aim of this section is to create a documentation in Markdown and use MkDocs to deploy the documentation generated as a static site in reference to the 7th point of the problem statement .","title":"Objective"},{"location":"deploying_report/#format-and-tools","text":"The report was written in Markdown as required by the problem statement. Markdown is a markup language which allows text formatting using a defined syntax. It is used extensively for documentation and can be converted to various other formats, including HTML, which allows many tools to build static sites with markdown documentation. MkDocs was the tool used, as reqiured by the problem statement, to build a static site with the report. MkDocs is a static site generator which creates sites with content written in Markdown and the site is configured with a YAML (YAML is a human-friendly data serialization standard and has various applications) file.","title":"Format and Tools"},{"location":"deploying_report/#installing-mkdocs","text":"I installed MkDocs with the command 'pip install mkdocs' as mentioned in the official documentation . I only referred to the 'Installing MkDocs' section under 'Manual Installation' as rest of the steps were not required in the context of the task/problem statement.","title":"Installing MkDocs"},{"location":"deploying_report/#selecting-a-theme","text":"MkDocs allows users to use various themes to customise the style and look of the site. For the report's site generated with MkDocs to look nice, I used the 'Material' theme as suggested during the preliminary review of the task. To use this theme with MkDocs, it is required to be installed with pip so, I installed Material theme using the command 'pip install mkdocs-material' as mentioned in the official documentation . Lastly, I specified the theme in MkDocs's configuration YAML file which can be seen in the next section.","title":"Selecting a Theme"},{"location":"deploying_report/#site-configuration","text":"To build the static site, MkDocs needs a YAML file, called mkdocs.yml , be present in the root directory of the project that configures the site structure, site title, pages, themes, etc. It is used to define properties for the site. The YAML file that I wrote for the report is below: site_name: 'Jenkins Pipeline' pages: - Introduction: 'index.md' - Problem Statement: 'problem_statement.md' - Glossary: 'glossary.md' - Setting Up VMs: 'setting_up_vms.md' - Setting Up Pipeline: 'setting_up_pipeline.md' - Static Analysis: 'static_analysis.md' - Comparing SAST Tools: 'comparing_sast_tools.md' - Configuring Webhook: 'configuring_webhook.md' - Deploying the Report: 'deploying_report.md' - Resources: 'resources.md' theme: 'material' site_name defines the title for the site generated by MkDocs. pages defines the various pages that the site will consist of. Within this section, the title for the different pages, along with the Markdown file they are to be associated with, are also declared in a key-value pair structure. theme defines which theme MkDocs should use while generating/serving the static site.","title":"Site Configuration"},{"location":"deploying_report/#deploying-static-site","text":"To generate the static site, in the root directory of the report, I ran the command 'mkdocs build' as mentioned in the documentation. This creates a /site directory containing all the required files to deploy the site. Note : To just preview how the site looks, I used 'mkdocs serve' in the terminal to serve the site locally on my machine. Now, to serve the site I needed a web server for which I installed Apache , following Digital Ocean's documentation , as the server. I chose Apache as it is popular, has great support and is easy to work with in my opinion. The documentation was concise and complete in the context of the task and hence, I went with it. I, however, skipped step 4, 'Setting Up Virtual Hosts', as I was only going to host one site and thus, did not require to configure virtual hosts which are used to serve multiple websites on the same server. Lastly, I copied the contents from static site directory ( /site ) generated with MkDocs to the web root directory ( /var/www/html ) to serve the report as a static site.","title":"Deploying Static Site"},{"location":"glossary/","text":"Glossary There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"glossary/#glossary","text":"There are some terms used in this report which might not be common and/or might have a different meaning in general. This glossary contains a list of such terms along with their intended meaning in the report: Term Description DVNA Damn Vulnerable Node Application; An intentionally vulnerably Node.js application. VM Virtual Machine used to complete the task. Production VM/Server The Virtual Machine used to deploy and Host DVNA. Jenkins Machine/VM The VM that has Jenkins installed on it to execute the pipeline for the task. Jenkins User The system user created on the VM after installing Jenkins. Infrastructure The environment with the two VMs, the Jenkins VM and the Production VM.","title":"Glossary"},{"location":"problem_statement/","text":"Problem Statement Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next the DVNA should get deployed in a server. To do all of the above just consider 2 virtual machines running in your laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparitive report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Problem Statement"},{"location":"problem_statement/#problem-statement","text":"Setup a basic pipeline (use Jenkins ) for generating a security report for DVNA . The DVNA code should be downloaded from Github and then undergo static analysis. As part of the project understand what is the tech stack for DVNA hence what potential static analysis tools can be applied here. Once a static analysis is done the report should be saved. Next the DVNA should get deployed in a server. To do all of the above just consider 2 virtual machines running in your laptop. One VM contains the Jenkins and related infrastructure, and the second VM is for deploying the DVNA using the pipeline. Do document extensively in markdown and deploy the documentation in a MkDocs website on the second VM. Additionally, there was some inferred task to address in the problem statement: To create a comparitive report about how various SAST tools performed. To create a webhook to trigger the build when a push event occurs on the project repository on GitHub.","title":"Problem Statement"},{"location":"resources/","text":"References These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"Resources"},{"location":"resources/#references","text":"These are some references I used along with the ones mentioned implicitly in the report: https://hub.docker.com/_/sonarqube/ https://medium.com/@rosaniline/setup-sonarqube-with-jenkins-declarative-pipeline-75bccdc9075f https://codebabel.com/sonarqube-with-jenkins/amp/ https://github.com/xseignard/sonar-js https://discuss.bitrise.io/t/sonarqube-authorization-problem/4229/2 https://www.sonarqube.org/ https://docs.npmjs.com/cli/audit https://github.com/ajinabraham/NodeJsScan https://retirejs.github.io/retire.js/ https://www.owasp.org/index.php/OWASP_Dependency_Check https://github.com/sonatype-nexus-community/auditjs https://github.com/snyk/snyk#cli https://github.com/nodesecurity/nsp https://github.com/dvolvox/JSpwn https://github.com/dpnishant/jsprime https://github.com/mozilla/scanjs","title":"References"},{"location":"setting_up_pipeline/","text":"Setting Up Pipeline Objective The aim of this section is to setup a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement . Pipeline A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language. Jenkins Pipeline Project To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which lead me to a different page. I gave gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst the all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description about the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artefacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations. The Jenkinsfile Jenkins has a utility where the actions that are to be performed on build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=ayushpriya10 export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins. Stages I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose: Initialization This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline. Build In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependecies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets build with all of its dependencies only on the Jenkins VM. Static Analysis All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools . Deployment Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the new updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking any installation of dependencies existing on the machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#setting-up-pipeline","text":"","title":"Setting Up Pipeline"},{"location":"setting_up_pipeline/#objective","text":"The aim of this section is to setup a basic pipeline in Jenkins to provide a solution to the 1st, 2nd, 4th and 5th points of the problem statement .","title":"Objective"},{"location":"setting_up_pipeline/#pipeline","text":"A Continuous Integration (CI) pipeline is a set of automated actions defined to be performed for delivering software applications. The pipeline helps in automating building the application, testing it and deploying it to production thus, reducing the time it requires for a software update to reach the production stage. A more detailed explanation can be found in this article by Atlassian. I liked this article's explanation because it was written in an easy-to-understand language.","title":"Pipeline"},{"location":"setting_up_pipeline/#jenkins-pipeline-project","text":"To start off with the task of building a pipeline, I setup Jenkins as mentioned in the previous section, logged on to the Jenkins Web Interface and then followed these steps to create a new pipeline under Jenkins for DVNA: I clicked on New Item from the main dashboard which lead me to a different page. I gave gave node-app-pipeline as the project's name and chose Pipeline as the project type amongst the all the options present. Few articles on the web also demonstrated the usage of Freestyle Project but I liked the syntactical format and hence, went with Pipeline . Next came the project configurations page. Here: Under General section: I gave a brief description about the application being deployed and the purpose of this pipeline. I checked the Discard Old Builds option as I felt there was no need of keeping artefacts from previous builds. I also checked the GitHub Project option and provided the GitHub URL for the project's repository. This option allowed Jenkins to know where to fetch the project from. Under Build Triggers section: I checked the GitHub hook trigger for GITScm Polling option to allow automated builds based on webhook triggers on GitHub for selected events. The need for this option is explained in more detail in the upcoming section, Configuring Webhook . Under Pipeline section: For Definition , I chose Pipeline Script from SCM option as I planned on adding the Jenkinsfile directly to the project repository. For Script Path , I just provided Jenkinsfile as it was situated at the project's root directory. Lastly, I clicked on save to save the configurations.","title":"Jenkins Pipeline Project"},{"location":"setting_up_pipeline/#the-jenkinsfile","text":"Jenkins has a utility where the actions that are to be performed on build can be written in a syntactical format in a file called Jenkinsfile . I used this format to define the pipeline as I found it programmatically intuitive and easy to understand. I followed this article because it was the official documentation from Jenkins and it was very thoroughly written in a simple format with examples. I wrote and added a Jenkinsfile to the root folder of the project repository. The following are the contents of the Jenkinsfile which executes the CI pipeline: pipeline { agent any stages { stage ('Initialization') { steps { sh 'echo \"Starting the build\"' } } stage ('Build') { steps { sh ''' export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=ayushpriya10 export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 npm install ''' } } stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } stage ('Deploy to App Server') { steps { sh 'echo \"Deploying App to Server\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"cd dvna && pm2 stop server.js\"' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"rm -rf dvna/ && mkdir dvna\"' sh 'scp -r * chaos@10.0.2.20:~/dvna' sh 'ssh -o StrictHostKeyChecking=no chaos@10.0.2.20 \"source ./env.sh && ./env.sh && cd dvna && pm2 start server.js\"' } } } } The pipeline block constitutes the entire definition of the pipeline. The agent keyword is used to choose the way the Jenkins instance(s) are used to run the pipeline. The any keyword defines that Jenkins should allocate any available agent (an instance of Jenkins/a slave/the master instance) to execute the pipeline. A more thorough explanation can be found here . The stages block houses all the stages that will comprise the various operations to be performed during the execution of the pipeline. The stage block defines a set of steps to be executed as part of that particular stage. The steps block defines the actions that are to be performed within a particular stage. Lastly, sh keyword is used to execute shell commands through Jenkins.","title":"The Jenkinsfile"},{"location":"setting_up_pipeline/#stages","text":"I divided the pipeline into various stages based on the operations being performed. The following are the stages I chose:","title":"Stages"},{"location":"setting_up_pipeline/#initialization","text":"This is just a dummy stage, nothing happens here. I wrote this to test out and practice the syntax for writing the pipeline.","title":"Initialization"},{"location":"setting_up_pipeline/#build","text":"In the build stage, I built the app with npm install on the Jenkins VM. This loads all the dependecies that the app (DVNA) requires so static analysis can be performed on the app in later stages. Note : The app gets build with all of its dependencies only on the Jenkins VM.","title":"Build"},{"location":"setting_up_pipeline/#static-analysis","text":"All the stages that follow the Build Stage, except for the last (deployment) stage, consist of performing static analysis on DVNA with various tools. These stages are used to generate a report of their analysis and are stored locally on the Jenkins VM. The individual stages under Static Analysis are explained in detail in the upcoming sections Static Analysis and Comparing SAST Tools .","title":"Static Analysis"},{"location":"setting_up_pipeline/#deployment","text":"Finally, in the stage titled 'Deploy to App Server', operations are performed on the App VM over SSH which I configured previously as mentioned in Setting up VMs . Firstly, I stop the instance of the app running on the App VM. Then I purge all project associated files present on the App VM. All files from the Jenkins machine, including the dependencies built earlier, are copied over to the production VM. Finally, I restart the application with the new updates reflecting changes made to the project. Note : The app is not built on the Production VM to avoid breaking any installation of dependencies existing on the machine. All the dependencies are always built on the Jenkins machine and then copied over to the production server.","title":"Deployment"},{"location":"setting_up_vms/","text":"Setting up VMs Objective The aim of this section is to setup the required infrastructure to perform the task and solve the 6th point of the problem statement . System Configuration The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipline. The realease notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete. Installing Jenkins Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentations. For this documentation, I didn't skip any step. Choosing the Application The application that was to be analysed and deployed, as required by the problem statment, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts. Configuring Production VM To serve DVNA , there were some prerequisites. The following steps conclude how to setup the prerequisites for Jenkins to be able to deploy the application through the pipeline. Setting up DVNA I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was insalled the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage. Configuring SSH Access For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , creates a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh in to the production VM. The credentails for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Setting Up VMs"},{"location":"setting_up_vms/#setting-up-vms","text":"","title":"Setting up VMs"},{"location":"setting_up_vms/#objective","text":"The aim of this section is to setup the required infrastructure to perform the task and solve the 6th point of the problem statement .","title":"Objective"},{"location":"setting_up_vms/#system-configuration","text":"The lab setup is of two VMs running Ubuntu 18.04 on VirtualBox as it is an LTS (Long Term Support) version which is a desirable feature for a CI pipline. The realease notes for Ubuntu 18.04 can be found here for additional details. One VM has the Jenkins Infrastructure and the other is used as a Production Server to deploy the application (DVNA) on the server via the Jenkins Pipeline. I installed Ubuntu on both VirtualBox VMs following this documentation . I decided to go with this documentation as it was concise. I, however, chose the 'Normal Installation' under \"Minimal Install Option and Third Party Software\" segment instead of the ones specified in the instructions as otherwise only the essential Ubuntu core components. Additionally, I left out the optional third step \"Managing installation media\" as I did not need the boot media after the installation was complete.","title":"System Configuration"},{"location":"setting_up_vms/#installing-jenkins","text":"Jenkins is a Continous Integration (CI) Tool used to automate actions for CI operations for building and deploying applications. Jenkins was used as the tool to build the application deployment pipeline as it was a requisite of the problem statement given. Installed Jenkins following Digital Ocean's documentation . I went along with this particular documentation as it seemed the easiest to follow with clear steps, and I like the style of Digital Ocean's documentations. For this documentation, I didn't skip any step.","title":"Installing Jenkins"},{"location":"setting_up_vms/#choosing-the-application","text":"The application that was to be analysed and deployed, as required by the problem statment, was DVNA (or Damn Vulnerable Node Application). It is an intentionally vulnerable application written with Node.js and has various security issues designed to illustrate different security concepts.","title":"Choosing the Application"},{"location":"setting_up_vms/#configuring-production-vm","text":"To serve DVNA , there were some prerequisites. The following steps conclude how to setup the prerequisites for Jenkins to be able to deploy the application through the pipeline.","title":"Configuring Production VM"},{"location":"setting_up_vms/#setting-up-dvna","text":"I forked the DVNA repository onto my GitHub account to be able to add files and edit project structure. Then I added a Jenkinsfile to the project repository to configure pipeline stages and execute it. DVNA's documentation specifies MySQL as the database needed so, to to install MySQL for DVNA I again used Digital Ocean's documentation . Under step 3, I skipped the section about provisionally MySQL access for a dedicated user. I created the 'root' user as mentioned in the documentation previously and then went straight to step 4 so as there was no need for an additional user after root . MySQL was insalled the Production VM for a successful deployment of the application. The Jenkins VM need not have MySQL installed as DVNA was only getting built on this machine and not deployed. To not leak the MySQL Server configuration details for the production server, I used a shell script, named env.sh , and placed it in the Production VM in user's home directory ( /home/<username> ). The script gets executed from the pipeline to export the Environment Variables for the application to deploy. The contents of the script (env.sh) to setup environment variables on Production VM are: #!/bin/bash export MYSQL_USER=root export MYSQL_DATABASE=dvna export MYSQL_PASSWORD=<mysql_password> export MYSQL_HOST=127.0.0.1 export MYSQL_PORT=3306 This script is executed through the pipeline in the 'Deploy to App Server' stage.","title":"Setting up DVNA"},{"location":"setting_up_vms/#configuring-ssh-access","text":"For Jenkins to be able to perform operations and copy application files onto the on the Production VM, ssh configuration was required to allow the Jenkins User to log on to the Production VM. For the same, I switched to the Jenkins User , creates a pair of SSH keys (an extensive article about how user authentication works in SSH with public keys can be found here ) and placed the public key in the Production VM: Switching to Jenkins User sudo su - jenkins Generating new SSH Keys for the Jenkins User ssh-keygen -t ed25519 -C \"<email>\" The public key generated above was added to ~/.ssh/authorized_keys on the Production VM. Note : One could also use the ssh-agent plugin in Jenkins to use a different user to ssh in to the production VM. The credentails for that user will have to be added under Jenkins Credentials Section. Note : ed25519 is used instead of the rsa option as it provides a smaller key while providing the equivalent security of an RSA key.","title":"Configuring SSH Access"},{"location":"static_analysis/","text":"Static Analysis SAST Tools for Node.js Applications The following are some tools that I found to perform SAST on Nodejs Applications: SonarQube Used SonarQube's docker image to run the application with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Created a new Access Token for Jenkins in SonarQube under Account > Security . In Jenkins, under Credentials > Add New Credentials the token is saved as a Secret Text type credential. The SonarQube Server section under Manage Jenkins > Configure System , check the Enable injection of SonarQube server configuration as build environment variables option. Provide the URL for SonarQube Server (in our case, localhost:9000) and add the previously saved SonarQube Credentials. Add the following stage in the Jenkinsfile: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } } NPM Audit NPM Audit comes along with npm@6 and is not required to be installed seprately. To upgrade npm, if needed, run the following command: npm install -g npm@latest NPM Audit gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline npm audit --json > /var/lib/jenkins/reports/npm-audit-report echo $? > /dev/null Add the following stage in the Jenkinsfile: stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } } NodeJsScan To install NodeJsScan , use the following command: pip3 install nodejsscan Note : If the package is not getting installed globally, as it will be run by the Jenkins User, run the following command: sudo -H pip3 install nodejsscan . To analyse the Nodejs project, the following command is used: nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report Add the following stage in the Jenkinsfile: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } } Retire.js To install Retire.js use the following command: npm install -g retire To analyse the project with Retire.js run the following command: retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0 Add the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } } OWASP Dependency Checker OWASP Dependency Checker comes as an executable for linux. To get the executable, download the archive . Unzip the archive: unzip dependency-check-5.2.4-release.zip To execute the scan, run the following command: /var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan /var/lib/jenkins/workspace/node-app-pipeline --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint Note : Copy the unzipped archive to /var/lib/jenkins/ before scanning. Add the following stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } } Auditjs To install Audit.js, use the following command: npm install auditjs -g To perform a scan, run the following command while inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /var/lib/jenkins/reports/auditjs-report 2>&1 Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /var/lib/jenkins/reports/auditjs-report 2>&1 echo $? > /dev/null Note : We use 2>&1 to redirct STDERR output to STDOUT otherwise the Vulnerabilities found will not be written to the report but instead will be printed to console. Add the following stage to the Jenkinsfile: stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } } Snyk To install Snyk, use the following command: npm install -g snyk Before scanning a project, we need to authenticate Snyk CLI which can be done as follows: snyk auth <AUTH TOKEN> To perform a scan: snyk test Snyk gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /var/lib/jenkins/reports/snyk-report echo $? > /dev/null Add the following stage to the pipeline: stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } } Other Tools NSP NSP or Node Security Project is now replaced with npm audit starting npm@6 and hence, is unavilable to new users. JSPrime JSPrime lacks a CLI interface and hence, couldn't be integrated into the CI Pipeline. ScanJS (Deprecated) ScanJS is depracated and was throwing an exception while being run via the CLI interface. JSpwn (JSPrime + ScanJs) JSpwn combines both JSPrime and JsScan and has a CLI interface as well. The CLI gave garbage output when ran.","title":"Static Analysis"},{"location":"static_analysis/#static-analysis","text":"","title":"Static Analysis"},{"location":"static_analysis/#sast-tools-for-nodejs-applications","text":"The following are some tools that I found to perform SAST on Nodejs Applications:","title":"SAST Tools for Node.js Applications"},{"location":"static_analysis/#sonarqube","text":"Used SonarQube's docker image to run the application with the following command: docker run -d -p 9000:9000 -p 9092:9092 --name sonarqube sonarqube Created a new Access Token for Jenkins in SonarQube under Account > Security . In Jenkins, under Credentials > Add New Credentials the token is saved as a Secret Text type credential. The SonarQube Server section under Manage Jenkins > Configure System , check the Enable injection of SonarQube server configuration as build environment variables option. Provide the URL for SonarQube Server (in our case, localhost:9000) and add the previously saved SonarQube Credentials. Add the following stage in the Jenkinsfile: stage ('SonarQube Analysis') { environment { scannerHome = tool 'SonarQube Scanner' } steps { withSonarQubeEnv ('SonarQube') { sh '${scannerHome}/bin/sonar-scanner' sh 'cat .scannerwork/report-task.txt > /var/lib/jenkins/reports/sonarqube-report' } } }","title":"SonarQube"},{"location":"static_analysis/#npm-audit","text":"NPM Audit comes along with npm@6 and is not required to be installed seprately. To upgrade npm, if needed, run the following command: npm install -g npm@latest NPM Audit gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline npm audit --json > /var/lib/jenkins/reports/npm-audit-report echo $? > /dev/null Add the following stage in the Jenkinsfile: stage ('NPM Audit Analysis') { steps { sh '/home/chaos/npm-audit.sh' } }","title":"NPM Audit"},{"location":"static_analysis/#nodejsscan","text":"To install NodeJsScan , use the following command: pip3 install nodejsscan Note : If the package is not getting installed globally, as it will be run by the Jenkins User, run the following command: sudo -H pip3 install nodejsscan . To analyse the Nodejs project, the following command is used: nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report Add the following stage in the Jenkinsfile: stage ('NodeJsScan Analysis') { steps { sh 'nodejsscan --directory `pwd` --output /var/lib/jenkins/reports/nodejsscan-report' } }","title":"NodeJsScan"},{"location":"static_analysis/#retirejs","text":"To install Retire.js use the following command: npm install -g retire To analyse the project with Retire.js run the following command: retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0 Add the following stage in the Jenkinsfile: stage ('Retire.js Analysis') { steps { sh 'retire --path `pwd` --outputformat json --outputpath /var/lib/jenkins/reports/retirejs-report --exitwith 0' } }","title":"Retire.js"},{"location":"static_analysis/#owasp-dependency-checker","text":"OWASP Dependency Checker comes as an executable for linux. To get the executable, download the archive . Unzip the archive: unzip dependency-check-5.2.4-release.zip To execute the scan, run the following command: /var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan /var/lib/jenkins/workspace/node-app-pipeline --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint Note : Copy the unzipped archive to /var/lib/jenkins/ before scanning. Add the following stage in the Jenkinfile: stage ('Dependency-Check Analysis') { steps { sh '/var/lib/jenkins/dependency-check/bin/dependency-check.sh --scan `pwd` --format JSON --out /var/lib/jenkins/reports/dependency-check-report --prettyPrint' } }","title":"OWASP Dependency Checker"},{"location":"static_analysis/#auditjs","text":"To install Audit.js, use the following command: npm install auditjs -g To perform a scan, run the following command while inside the project directory: auditjs --username ayushpriya10@gmail.com --token <auth_token> /var/lib/jenkins/reports/auditjs-report 2>&1 Auditjs gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline auditjs --username ayushpriya10@gmail.com --token <auth_token> /var/lib/jenkins/reports/auditjs-report 2>&1 echo $? > /dev/null Note : We use 2>&1 to redirct STDERR output to STDOUT otherwise the Vulnerabilities found will not be written to the report but instead will be printed to console. Add the following stage to the Jenkinsfile: stage ('Audit.js Analysis') { steps { sh '/home/chaos/auditjs.sh' } }","title":"Auditjs"},{"location":"static_analysis/#snyk","text":"To install Snyk, use the following command: npm install -g snyk Before scanning a project, we need to authenticate Snyk CLI which can be done as follows: snyk auth <AUTH TOKEN> To perform a scan: snyk test Snyk gives a non-zero status code, if it finds any vulnerable dependencies, hence, I ran it through a script to avoid failure of the pipeline. The script is as follows: #!/bin/bash cd /var/lib/jenkins/workspace/node-app-pipeline snyk auth <auth_token> snyk test --json > /var/lib/jenkins/reports/snyk-report echo $? > /dev/null Add the following stage to the pipeline: stage ('Snyk Analysis') { steps { sh '/home/chaos/snyk.sh' } }","title":"Snyk"},{"location":"static_analysis/#other-tools","text":"NSP NSP or Node Security Project is now replaced with npm audit starting npm@6 and hence, is unavilable to new users. JSPrime JSPrime lacks a CLI interface and hence, couldn't be integrated into the CI Pipeline. ScanJS (Deprecated) ScanJS is depracated and was throwing an exception while being run via the CLI interface. JSpwn (JSPrime + ScanJs) JSpwn combines both JSPrime and JsScan and has a CLI interface as well. The CLI gave garbage output when ran.","title":"Other Tools"}]}